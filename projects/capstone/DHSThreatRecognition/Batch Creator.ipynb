{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import HelperFuncs as hfuncs\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.data_utils import Sequence\n",
    "import h5py\n",
    "import os\n",
    "import pickle\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(filename='Upload.log',filemode=\"w\",level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to first try training a CNN on the individual images.\n",
    "We will be using binary cross entropy across the 17 regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "FINAL_WIDTH = 400\n",
    "FINAL_HEIGHT = 600\n",
    "CHANNELS = 1\n",
    "ZONES = 17\n",
    "BUCKET_NAME = 'miscdatastorage'\n",
    "DATA_DIR = 'DHSData/'\n",
    "TEMP_DIR = 'temp'\n",
    "LABELS_DIR = r'stage1_labels.csv'\n",
    "EXTENSION = '.a3daps'\n",
    "np.random.seed(0)\n",
    "\n",
    "#Define a generator function\n",
    "class myGenerator:\n",
    "    #AWS and Directory information \n",
    "    bucketName = BUCKET_NAME\n",
    "    dataDir = DATA_DIR\n",
    "    temp_dir = TEMP_DIR\n",
    "    labels_dir = LABELS_DIR\n",
    "    #Connect to AWS\n",
    "    key_id, secret_key = hfuncs.GetAWSCredentials()\n",
    "    client = hfuncs.GetAWSClient(key_id,secret_key)\n",
    "    bucket = client.Bucket(bucketName)\n",
    "    extension = EXTENSION\n",
    "    #labels and keys\n",
    "    labels_dict = hfuncs.GetLabelsDict(labels_dir)\n",
    "    key_ary = None\n",
    "    #Batch information\n",
    "    n_samples = 0\n",
    "    batch_size = 0\n",
    "    #Requester\n",
    "    batch_requester = None\n",
    "    #Initialize required parameters\n",
    "    def __init__(self,keys,n_samples,batch_size=BATCH_SIZE):\n",
    "        self.key_ary = keys\n",
    "        self.n_samples = n_samples\n",
    "        self.batch_size = batch_size\n",
    "        #Initialize AWS Batch Requester\n",
    "        self.batchrequester = hfuncs.BatchRequester(self.bucket,self.key_ary,self.labels_dict,self.dataDir,self.temp_dir,self.extension)\n",
    "    def GenerateSamples(self):\n",
    "        '''Returns generator that retireves n_sample scans at a time,\n",
    "        mixes each scan-slice image into a meta-batch, and returns mini-batches of \n",
    "        BATCH_SIZE'''\n",
    "        #While there is data left, yield batch\n",
    "        while self.batchrequester.DoItemsRemain():\n",
    "            #Request data\n",
    "            print(\"Retrieving data..\")\n",
    "            pointer = self.batchrequester.key_pointer\n",
    "            logging.info(\"Last succesful key {} at pointer {}\".format(self.batchrequester.keys[pointer],pointer))\n",
    "            X,y = self.batchrequester.NextBatch(self.n_samples)\n",
    "            n_angles = X.shape[3] #num angles (64)\n",
    "\n",
    "            #Create efficient mapping for mixing and indexing batch data\n",
    "            indexing_dict = {}\n",
    "            order = np.arange(X.shape[0]*n_angles)\n",
    "            np.random.shuffle(order)\n",
    "            k = 0\n",
    "            for s in range(X.shape[0]):\n",
    "                for a in range(n_angles):\n",
    "                    indexing_dict[order[k]]=[s,a]\n",
    "                    k+=1\n",
    "\n",
    "            print(\"Data retrieved and indexing computed.\")\n",
    "\n",
    "            #Initialize output arrays\n",
    "            print(\"Initializing arrays...\")\n",
    "            X_train = np.zeros((X.shape[0]*n_angles,FINAL_WIDTH,FINAL_HEIGHT,CHANNELS))\n",
    "            y_train = np.zeros((X.shape[0]*n_angles,ZONES))\n",
    "            print(\"Arrays initialized\")\n",
    "\n",
    "            #Set counter to 0, channel to 1\n",
    "            chan = 0 #No need to iterate here\n",
    "            i = 0\n",
    "            #Clean each image and store it in output\n",
    "            while i < X.shape[0] * n_angles:\n",
    "                j = i\n",
    "                while j < i+BATCH_SIZE:\n",
    "                    s,a = indexing_dict[j]                    \n",
    "                    X_train[j,:,:,chan] = hfuncs.CropCleanResize(X[s,:,:,a],FINAL_WIDTH,FINAL_HEIGHT)\n",
    "                    y_train[j,:] = y[s,:]\n",
    "                    j += 1\n",
    "                yield X_train[i:i+BATCH_SIZE,:,:,:],y_train[i:i+BATCH_SIZE]\n",
    "                i += BATCH_SIZE\n",
    "\n",
    "\n",
    "def CleanKeyAry(key_ary,labels_dict,dataDir,extension):\n",
    "    '''Taken from the BatchRequester class'''\n",
    "    key_ary_new=[]\n",
    "    for key in key_ary:\n",
    "        img_id = key.strip().replace(dataDir,'').replace(extension,'')\n",
    "        if img_id in labels_dict.keys():\n",
    "            key_ary_new.append(key)\n",
    "        else:\n",
    "            continue\n",
    "    return key_ary_new\n",
    "\n",
    "def getTrainTestValData(labels_dir=LABELS_DIR,extension=EXTENSION,dataDir=DATA_DIR,bucketName=BUCKET_NAME):\n",
    "    '''Retrieves all samples that have corresponding labels \n",
    "    and splits data into a train, test, val set. '''\n",
    "    #Labels        \n",
    "    labels_dict = hfuncs.GetLabelsDict(labels_dir)\n",
    "    \n",
    "    #AWS Bucket\n",
    "    key_id, secret_key = hfuncs.GetAWSCredentials()\n",
    "    client = hfuncs.GetAWSClient(key_id,secret_key)\n",
    "    bucket = client.Bucket(bucketName)\n",
    "    \n",
    "    #Get shuffled keys and separate into train,test,and validation\n",
    "    key_ary = hfuncs.GetShuffledKeys(bucket)\n",
    "    key_ary = CleanKeyAry(key_ary,labels_dict,dataDir,extension)\n",
    "    K_train,K_test = train_test_split(key_ary,test_size=0.20,random_state=0)\n",
    "    K_train,K_val = train_test_split(K_train,test_size=0.25,random_state=0) #0.80*0.25 = 0.20 validation \n",
    "    \n",
    "    return K_train,K_test,K_val     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate data and save\n",
    "K_train,K_test,K_val = getTrainTestValData()\n",
    "\n",
    "filename = \"data_separated.pickle\"\n",
    "save = {'K_train':K_train,'K_test':K_train,'K_val':K_val}\n",
    "with open(filename,\"wb\") as f:\n",
    "    pickle.dump(save,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving data..\n",
      "Data retrieved and indexing computed.\n",
      "Initializing arrays...\n",
      "Arrays initialized\n",
      "Completed batch 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-df8ddee38778>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtrainGen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainGen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerateSamples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEMP_DIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"batch_{}.hdf5\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"train/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"batch_{}.hdf5\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-1f4f56f0f09a>\u001b[0m in \u001b[0;36mGenerateSamples\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                     \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexing_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                     \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchan\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhfuncs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCropCleanResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mFINAL_WIDTH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mFINAL_HEIGHT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m                     \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                     \u001b[0mj\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/machine-learning/projects/capstone/DHSThreatRecognition/HelperFuncs.py\u001b[0m in \u001b[0;36mCropCleanResize\u001b[0;34m(x, new_i, new_j)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mCropCleanResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew_i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew_j\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;34m'''Crops and returns 2d image with specified uniform dimensions'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0mmin_i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFindCropDimensions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0mx_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCropImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_j\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew_i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew_j\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0mReduceNoise_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mReduceNoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/machine-learning/projects/capstone/DHSThreatRecognition/HelperFuncs.py\u001b[0m in \u001b[0;36mFindCropDimensions\u001b[0;34m(x, thresh)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mnum_consec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mthresh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m                 \u001b[0mnum_relevant\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0mnum_consec\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Connect to aws s3\n",
    "UPLOAD_BUCKET = 'cleandhsdata'\n",
    "key_id, secret_key = hfuncs.GetAWSCredentials()\n",
    "client = hfuncs.GetAWSClient(key_id,secret_key)\n",
    "bucket = client.Bucket(UPLOAD_BUCKET)\n",
    "\n",
    "#Clean and upload\n",
    "logging.info(\"Starting train upload\")\n",
    "trainGen = myGenerator(K_train,5)\n",
    "i = 0\n",
    "for X, y in trainGen.GenerateSamples():\n",
    "    filename = os.path.join(TEMP_DIR,\"batch_{}.hdf5\".format(i))\n",
    "    key = \"train/{}\".format(\"batch_{}.hdf5\".format(i))\n",
    "    with h5py.File(filename,\"w\") as f:\n",
    "        dset = f.create_dataset('image',data=X)\n",
    "        dset2 = f.create_dataset('labels',data=y)\n",
    "    bucket.upload_file(Filename=filename,Key=key)\n",
    "    os.remove(filename)\n",
    "    i += 1\n",
    "    logging.info(\"Completed batch {}\".format(i))\n",
    "    print(\"Completed batch {}\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Clean and upload\n",
    "logging.info(\"Starting val upload\")\n",
    "trainGen = myGenerator(K_val,25)\n",
    "i = 0\n",
    "for X, y in trainGen.GenerateSamples():\n",
    "    filename = os.path.join(TEMP_DIR,\"batch_{}.hdf5\".format(i))\n",
    "    key = \"val/{}\".format(\"batch_{}.hdf5\".format(i))\n",
    "    with h5py.File(filename,\"w\") as f:\n",
    "        dset = f.create_dataset('image',data=X)\n",
    "        dset2 = f.create_dataset('labels',data=y)\n",
    "    bucket.upload_file(Filename=filename,Key=key)\n",
    "    os.remove(filename)\n",
    "    i += 1\n",
    "    print(\"Completed batch {}\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Clean and upload\n",
    "logging.info(\"Starting test upload\")\n",
    "trainGen = myGenerator(K_test,25)\n",
    "i = 0\n",
    "for X, y in trainGen.GenerateSamples():\n",
    "    filename = os.path.join(TEMP_DIR,\"batch_{}.hdf5\".format(i))\n",
    "    key = \"test/{}\".format(\"batch_{}.hdf5\".format(i))\n",
    "    with h5py.File(filename,\"w\") as f:\n",
    "        dset = f.create_dataset('image',data=X)\n",
    "        dset2 = f.create_dataset('labels',data=y)\n",
    "    bucket.upload_file(Filename=filename,Key=key)\n",
    "    os.remove(filename)\n",
    "    i += 1\n",
    "    print(\"Completed batch {}\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (DSHenv_3.5)",
   "language": "python",
   "name": "dhsenv_3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
