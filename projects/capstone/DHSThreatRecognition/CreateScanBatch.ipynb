{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving data..\n",
      "Something went wrong. Skipping 158c596536e90a3edb3c84dbecc29420\n",
      "Data retrieved\n",
      "Initializing arrays...\n",
      "Arrays initialized\n",
      "Completed batch 1\n",
      "Completed batch 2\n",
      "Completed batch 3\n",
      "Completed batch 4\n",
      "Completed batch 5\n",
      "Completed batch 6\n",
      "Completed batch 7\n",
      "Completed batch 8\n",
      "Completed batch 9\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-688e93e75bad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0mtrainGen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyScanGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainGen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerateSamples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEMP_DIR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"batch_{}.hdf5\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_root\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"batch_{}.hdf5\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-688e93e75bad>\u001b[0m in \u001b[0;36mGenerateSamples\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_angles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                     \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchan\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhfuncs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCropCleanResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mFINAL_WIDTH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mFINAL_HEIGHT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m                     \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/machine-learning/projects/capstone/DHSThreatRecognition/HelperFuncs.py\u001b[0m in \u001b[0;36mCropCleanResize\u001b[0;34m(x, new_i, new_j)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0mx_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCropImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_j\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew_i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew_j\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0mReduceNoise_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mReduceNoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m     \u001b[0mx_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReduceNoise_v\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DHSenv_3.5/lib/python3.5/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2732\u001b[0m             \u001b[0mvargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_n\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_n\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2734\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vectorize_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2736\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_ufunc_and_otypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DHSenv_3.5/lib/python3.5/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_vectorize_call\u001b[0;34m(self, func, args)\u001b[0m\n\u001b[1;32m   2808\u001b[0m                       for a in args]\n\u001b[1;32m   2809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2810\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2812\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/machine-learning/projects/capstone/DHSThreatRecognition/HelperFuncs.py\u001b[0m in \u001b[0;36mReduceNoise\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mReduceNoise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mNOISE_THRESHOLD\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNOISE_THRESHOLD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import HelperFuncs as hfuncs\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.data_utils import Sequence\n",
    "import h5py\n",
    "import os\n",
    "import pickle\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(filename='UploadScans.log',level=logging.INFO)\n",
    "\n",
    "BATCH_SIZE = 20\n",
    "FINAL_WIDTH = 400\n",
    "FINAL_HEIGHT = 600\n",
    "CHANNELS = 1\n",
    "ZONES = 17\n",
    "BUCKET_NAME = 'miscdatastorage'\n",
    "DATA_DIR = 'DHSData/'\n",
    "TEMP_DIR = 'temp'\n",
    "LABELS_DIR = r'stage1_labels.csv'\n",
    "EXTENSION = '.a3daps'\n",
    "np.random.seed(0)\n",
    "\n",
    "#Define a generator function\n",
    "class myScanGenerator:\n",
    "    #AWS and Directory information \n",
    "    bucketName = BUCKET_NAME\n",
    "    dataDir = DATA_DIR\n",
    "    temp_dir = TEMP_DIR\n",
    "    labels_dir = LABELS_DIR\n",
    "    #Connect to AWS\n",
    "    key_id, secret_key = hfuncs.GetAWSCredentials()\n",
    "    client = hfuncs.GetAWSClient(key_id,secret_key)\n",
    "    bucket = client.Bucket(bucketName)\n",
    "    extension = EXTENSION\n",
    "    #labels and keys\n",
    "    labels_dict = hfuncs.GetLabelsDict(labels_dir)\n",
    "    key_ary = None\n",
    "    #Batch information\n",
    "    n_samples = 0\n",
    "    batch_size = 0\n",
    "    #Requester\n",
    "    batch_requester = None\n",
    "    #Initialize required parameters\n",
    "    def __init__(self,keys,n_samples):\n",
    "        #Keys of samples to process\n",
    "        self.key_ary = keys\n",
    "        #Samples to load at a time\n",
    "        self.n_samples = n_samples\n",
    "        #Initialize AWS Batch Requester\n",
    "        self.batchrequester = hfuncs.BatchRequester(self.bucket,self.key_ary,self.labels_dict,self.dataDir,self.temp_dir,self.extension)\n",
    "    def GenerateSamples(self):\n",
    "        '''Returns generator that retireves n_sample scans at a time,\n",
    "        mixes each scan-slice image into a meta-batch, and returns mini-batches of \n",
    "        BATCH_SIZE'''\n",
    "        #While there is data left, yield batch\n",
    "        while self.batchrequester.DoItemsRemain():\n",
    "            #Request data\n",
    "            print(\"Retrieving data..\")\n",
    "            pointer = self.batchrequester.key_pointer\n",
    "            logging.info(\"Last succesful key {} at pointer {}\".format(self.batchrequester.keys[pointer],pointer))\n",
    "            X,y = self.batchrequester.NextBatch(self.n_samples)\n",
    "            n_angles = X.shape[3] #num angles (64)\n",
    "\n",
    "            print(\"Data retrieved\")\n",
    "\n",
    "            #Initialize output arrays\n",
    "            print(\"Initializing arrays...\")\n",
    "            X_train = np.zeros((X.shape[0],n_angles,FINAL_WIDTH,FINAL_HEIGHT,CHANNELS))\n",
    "            y_train = np.zeros((X.shape[0],ZONES))\n",
    "            print(\"Arrays initialized\")\n",
    "\n",
    "            #Set counter to 0, channel to 1\n",
    "            chan = 0 #No need to iterate here\n",
    "            i = 0\n",
    "            #Clean each image and store it in output\n",
    "            for i in range(X.shape[0]):\n",
    "                for j in range(n_angles):\n",
    "                    X_train[i,j,:,:,chan] = hfuncs.CropCleanResize(X[i,:,:,j],FINAL_WIDTH,FINAL_HEIGHT)\n",
    "                    y_train[i,:] = y[i,:]\n",
    "                    \n",
    "                yield X_train[i,:,:,:,:],y_train[i]\n",
    "\n",
    "def CleanKeyAry(key_ary,labels_dict,dataDir,extension):\n",
    "    '''Taken from the BatchRequester class'''\n",
    "    key_ary_new=[]\n",
    "    for key in key_ary:\n",
    "        img_id = key.strip().replace(dataDir,'').replace(extension,'')\n",
    "        if img_id in labels_dict.keys():\n",
    "            key_ary_new.append(key)\n",
    "        else:\n",
    "            continue\n",
    "    return key_ary_new\n",
    "\n",
    "def getTrainTestValData(labels_dir=LABELS_DIR,extension=EXTENSION,dataDir=DATA_DIR,bucketName=BUCKET_NAME):\n",
    "    '''Retrieves all samples that have corresponding labels \n",
    "    and splits data into a train, test, val set. '''\n",
    "    #Labels        \n",
    "    labels_dict = hfuncs.GetLabelsDict(labels_dir)\n",
    "    \n",
    "    #AWS Bucket\n",
    "    key_id, secret_key = hfuncs.GetAWSCredentials()\n",
    "    client = hfuncs.GetAWSClient(key_id,secret_key)\n",
    "    bucket = client.Bucket(bucketName)\n",
    "    \n",
    "    #Get shuffled keys and separate into train,test,and validation\n",
    "    key_ary = hfuncs.GetShuffledKeys(bucket)\n",
    "    key_ary = CleanKeyAry(key_ary,labels_dict,dataDir,extension)\n",
    "    K_train,K_test = train_test_split(key_ary,test_size=0.20,random_state=0)\n",
    "    K_train,K_val = train_test_split(K_train,test_size=0.25,random_state=0) #0.80*0.25 = 0.20 validation \n",
    "    \n",
    "    return K_train,K_test,K_val     \n",
    "\n",
    "#Separate data and save\n",
    "K_train,K_test,K_val = getTrainTestValData()\n",
    "\n",
    "#Connect to aws s3\n",
    "UPLOAD_BUCKET = 'cleandhsdata'\n",
    "key_id, secret_key = hfuncs.GetAWSCredentials()\n",
    "client = hfuncs.GetAWSClient(key_id,secret_key)\n",
    "bucket = client.Bucket(UPLOAD_BUCKET)\n",
    "\n",
    "#Clean and upload\n",
    "logging.info(\"Starting train upload\")\n",
    "key_root = \"train_scan\"\n",
    "trainGen = myScanGenerator(K_train,25)\n",
    "i = 0\n",
    "for X, y in trainGen.GenerateSamples():\n",
    "    filename = os.path.join(TEMP_DIR,\"batch_{}.hdf5\".format(i))\n",
    "    key = \"{}/{}\".format(key_root,\"batch_{}.hdf5\".format(i))\n",
    "    with h5py.File(filename,\"w\") as f:\n",
    "        dset = f.create_dataset('image',data=X)\n",
    "        dset2 = f.create_dataset('labels',data=y)\n",
    "    bucket.upload_file(Filename=filename,Key=key)\n",
    "    os.remove(filename)\n",
    "    i += 1\n",
    "    logging.info(\"Completed batch {}\".format(i))\n",
    "    print(\"Completed batch {}\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import HelperFuncs as hfuncs\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.data_utils import Sequence\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "BATCH_SIZE = 20\n",
    "FINAL_WIDTH = 400\n",
    "FINAL_HEIGHT = 600\n",
    "CHANNELS = 1\n",
    "ZONES = 17\n",
    "\n",
    "class Sequencer(Sequence):\n",
    "    idx_dict={}\n",
    "    \n",
    "    def __init__(self,num_batches,bucket_name,mode=\"train\"):\n",
    "        self.num_batches = num_batches\n",
    "        self.bucket_name = bucket_name\n",
    "        self.mode = mode\n",
    "        self.key_id, self.secret_key = hfuncs.GetAWSCredentials()\n",
    "        self.mode = mode\n",
    "        \n",
    "        order = np.arange(self.num_batches)\n",
    "        np.random.seed(0)\n",
    "        np.random.shuffle(order)\n",
    "        k = 0\n",
    "        for k in range(len(order)):\n",
    "            self.idx_dict[k]=order[k]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "    def __getitem__(self,idx):\n",
    "        #Get Client\n",
    "        client = hfuncs.GetAWSClient(self.key_id,self.secret_key)\n",
    "        bucket = client.Bucket(self.bucket_name)\n",
    "        \n",
    "        idx = self.idx_dict[idx] #Mix up order of the batches \n",
    "        \n",
    "        path = \"temp/{}/batch_{}.hdf5\".format(self.mode,idx)\n",
    "        key = \"{}/batch_{}.hdf5\".format(self.mode,idx)\n",
    "        bucket.download_file(Key=key,Filename=path)\n",
    "        \n",
    "        f = h5py.File(path,\"r\")\n",
    "        try:\n",
    "            return f['/image'].value, f['/labels'].value\n",
    "        finally:\n",
    "            f.close()\n",
    "            os.remove(path)        \n",
    "\n",
    "class ScanSequencer(Sequence):\n",
    "    idx_dict={}\n",
    "    \n",
    "    def __init__(self,num_batches,bucket_name,mode=\"train_scan\"):\n",
    "        self.num_batches = num_batches\n",
    "        self.bucket_name = bucket_name\n",
    "        self.mode = mode\n",
    "        self.key_id, self.secret_key = hfuncs.GetAWSCredentials()\n",
    "        self.mode = mode\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "    def __getitem__(self,idx):\n",
    "        #Get Client\n",
    "        client = hfuncs.GetAWSClient(self.key_id,self.secret_key)\n",
    "        bucket = client.Bucket(self.bucket_name)\n",
    "        \n",
    "        #Download batch at index\n",
    "        path = \"temp/{}/batch_{}.hdf5\".format(self.mode,idx)\n",
    "        key = \"{}/batch_{}.hdf5\".format(self.mode,idx)\n",
    "        bucket.download_file(Key=key,Filename=path)\n",
    "        \n",
    "        f = h5py.File(path,\"r\")\n",
    "        try:\n",
    "            return f['/image'].value, f['/labels'].value\n",
    "        finally:\n",
    "            f.close()\n",
    "            os.remove(path) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 400, 600, 1) (17,)\n"
     ]
    }
   ],
   "source": [
    "#Bucket with clean data\n",
    "UPLOAD_BUCKET = 'cleandhsdata' #bucket where clean data was stored\n",
    "key_id, secret_key = hfuncs.GetAWSCredentials()\n",
    "client = hfuncs.GetAWSClient(key_id,secret_key)\n",
    "bucket = client.Bucket(UPLOAD_BUCKET)\n",
    "\n",
    "mode =\"train_scan\"\n",
    "num_batches = sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1 #train,test,val root directories have their own keys\n",
    "seq = ScanSequencer(num_batches=num_batches,bucket_name=UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "X,y = seq.__getitem__(1)\n",
    "\n",
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (DSHenv_3.5)",
   "language": "python",
   "name": "dhsenv_3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
