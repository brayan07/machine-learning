{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import HelperFuncs as hfuncs\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to first try training a CNN on the individual images.\n",
    "We will be using binary cross entropy across the 17 regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "FINAL_WIDTH = 400\n",
    "FINAL_HEIGHT = 600\n",
    "CHANNELS = 1\n",
    "ZONES = 17\n",
    "\n",
    "#Define a generator function\n",
    "def myGenerator():\n",
    "    #AWS and Directory information \n",
    "    bucketName = 'miscdatastorage'\n",
    "    dataDir = 'DHSData/'\n",
    "    temp_dir = 'temp'\n",
    "    labels_dir = r'stage1_labels.csv'\n",
    "    #Connect to AWS\n",
    "    key_id, secret_key = hfuncs.GetAWSCredentials()\n",
    "    client = hfuncs.GetAWSClient(key_id,secret_key)\n",
    "    bucket = client.Bucket(bucketName)\n",
    "    #Initialize required parameters\n",
    "    key_ary = hfuncs.GetShuffledKeys(bucket)\n",
    "    labels_dict = hfuncs.GetLabelsDict(labels_dir)\n",
    "    extension = '.a3daps'\n",
    "    \n",
    "    #Initialize AWS Batch Requester\n",
    "    batchrequester = hfuncs.BatchRequester(bucket,key_ary,labels_dict,dataDir,temp_dir,extension)\n",
    "    \n",
    "    #Preprocessing parameters\n",
    "    n_samples = 10 #Distinct samples (x64 images each) to retrieve iteratively\n",
    "    angles = 64\n",
    "    \n",
    "    \n",
    "    #While there is data left, yield batch\n",
    "    while batchrequester.DoItemsRemain():\n",
    "        X,y = batchrequester.NextBatch(n_samples)\n",
    "        \n",
    "      #  if X.shape[0] < n_samples:\n",
    "      #     return\n",
    "        #Set counter to 0, channel to 1, and initialize output arrays\n",
    "        i = 0\n",
    "        chan = 0 #No need to iterate here\n",
    "        X_train = np.zeros((n_samples*angles,FINAL_WIDTH,FINAL_HEIGHT,CHANNELS))\n",
    "        y_train = np.zeros((n_samples*angles,ZONES))\n",
    "        \n",
    "        #Clean each image and store it in output array\n",
    "        for s in range(X.shape[0]):\n",
    "            for a in range(X.shape[3]):\n",
    "                X_train[i,:,:,chan] = hfuncs.CropCleanResize(X[s,:,:,a],FINAL_WIDTH,FINAL_HEIGHT)\n",
    "                y_train[i,:] =  y[s,:]\n",
    "        i = 0\n",
    "        while i < n_samples * angles:\n",
    "            yield X_train[i:i+BATCH_SIZE,:,:,:],y_train[i:i+BATCH_SIZE]\n",
    "            i += BATCH_SIZE\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D , AveragePooling2D,Flatten\n",
    "from keras.models import Model\n",
    "from keras.layers.core import Dropout\n",
    "import keras\n",
    "\n",
    "#Build Basic model\n",
    "\n",
    "input_img = Input(shape=(FINAL_WIDTH,FINAL_HEIGHT,CHANNELS))\n",
    "\n",
    "pooling_1 = MaxPooling2D((2,2),padding='same')(input_img)\n",
    "\n",
    "tower_1 = Conv2D(64, (1, 1), padding='same', activation='relu')(pooling_1)\n",
    "tower_1 = Conv2D(64, (3, 3), padding='same', activation='relu')(tower_1)\n",
    "\n",
    "tower_2 = Conv2D(64, (1, 1), padding='same', activation='relu')(pooling_1)\n",
    "tower_2 = Conv2D(64, (5, 5), padding='same', activation='relu')(tower_2)\n",
    "\n",
    "tower_3 = MaxPooling2D((2, 2), strides=(1, 1), padding='same')(pooling_1)\n",
    "tower_3 = Conv2D(64, (1, 1), padding='same', activation='relu')(tower_3)\n",
    "\n",
    "output_inception = keras.layers.concatenate([tower_1, tower_2, tower_3], axis=1)\n",
    "\n",
    "pooling_2 = MaxPooling2D((3,2),padding='same')(output_inception)\n",
    "pooling_2 = Dropout(0.10)(pooling_2)\n",
    "\n",
    "tower_1_2 = Conv2D(128, (1, 1), padding='same', activation='relu')(pooling_2)\n",
    "tower_1_2 = Conv2D(128, (3, 3), padding='same', activation='relu')(tower_1_2)\n",
    "\n",
    "tower_2_2 = Conv2D(128, (1, 1), padding='same', activation='relu')(pooling_2)\n",
    "tower_2_2 = Conv2D(128, (5, 5), padding='same', activation='relu')(tower_2_2)\n",
    "\n",
    "tower_3_2 = MaxPooling2D((2, 2), strides=(1, 1), padding='same')(pooling_2)\n",
    "tower_3_2 = Conv2D(128, (1, 1), padding='same', activation='relu')(tower_3_2)\n",
    "\n",
    "output_inception_2 = keras.layers.concatenate([tower_1_2, tower_2_2, tower_3_2], axis=1)\n",
    "\n",
    "output_inception_2 = Dropout(0.10)(output_inception_2)\n",
    "output_inception_2 = MaxPooling2D((2, 1),strides=(2,1), padding='same')(output_inception_2)\n",
    "\n",
    "conv_3 = Conv2D(256, (1, 1), padding='same', activation='relu')(output_inception_2)\n",
    "last = Flatten()(conv_3)\n",
    "\n",
    "#List of independent guesses for each zone\n",
    "output_nodes = []\n",
    "for i in range(ZONES):\n",
    "    output_nodes.append(Dense(1,activation='sigmoid')(last))\n",
    "\n",
    "out = keras.layers.concatenate(output_nodes)\n",
    "\n",
    "multi_label_model = Model(input_img, out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.optimizers import SGD\n",
    "from keras import metrics\n",
    "\n",
    "x = datetime.today()\n",
    "stamp = \"{}-{}-{}_{}:{}:{}\".format(x.year,x.month,x.day,x.hour,x.minute,x.second)\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(stamp))\n",
    "\n",
    "\n",
    "multi_label_model.compile(optimizer='SGD',\n",
    "                          metrics=[metrics.binary_accuracy,metrics.binary_crossentropy],\n",
    "                         loss= 'binary_crossentropy')\n",
    "gen = myGenerator()\n",
    "#multi_label_model.fit_generator(gen,steps_per_epoch=10,epochs=5,callbacks=[tensorboard])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Something went wrong. Skipping 4c62a5d32ab40d1d55499dfd0dfc1e7a\n",
      "(20, 400, 600, 1)\n"
     ]
    }
   ],
   "source": [
    "gen = myGenerator()\n",
    "X,y = gen.__next__()\n",
    "print (X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (DSHenv_3.5)",
   "language": "python",
   "name": "dhsenv_3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
