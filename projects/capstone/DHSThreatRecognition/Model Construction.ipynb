{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import HelperFuncs as hfuncs\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to first try training a CNN on the individual images.\n",
    "We will be using binary cross entropy across the 17 regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import HelperFuncs as hfuncs\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.data_utils import Sequence\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "FINAL_WIDTH = 400\n",
    "FINAL_HEIGHT = 600\n",
    "CHANNELS = 1\n",
    "ZONES = 17\n",
    "ANGLES = 16\n",
    "#Create directories for sequencer function if they don't exist\n",
    "for d in ['temp/train_scan/','temp/test_scan/','temp/val_scan/']:\n",
    "    if not os.path.isdir(d):\n",
    "        print(\"Created directory: {}\".format(d))\n",
    "        os.makedirs(d)\n",
    "        \n",
    "class ScanSequencer(Sequence):\n",
    "    idx_dict={}\n",
    "    \n",
    "    def __init__(self,num_batches,bucket_name,mode=\"train_scan\"):\n",
    "        self.num_batches = num_batches\n",
    "        self.bucket_name = bucket_name\n",
    "        self.mode = mode\n",
    "        self.key_id, self.secret_key = hfuncs.GetAWSCredentials()\n",
    "        self.mode = mode\n",
    "        self.angles = np.arange(0,64,64//ANGLES)\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "    def on_epoch_end(self):\n",
    "        pass\n",
    "    def __getitem__(self,idx):\n",
    "        #Get Client\n",
    "        client = hfuncs.GetAWSClient(self.key_id,self.secret_key)\n",
    "        bucket = client.Bucket(self.bucket_name)\n",
    "        \n",
    "        #Initialize vectors\n",
    "        X_train = np.zeros((BATCH_SIZE,ANGLES,FINAL_WIDTH,FINAL_HEIGHT,CHANNELS))\n",
    "        y_train = np.zeros((BATCH_SIZE,ZONES))\n",
    "        \n",
    "        j=0\n",
    "        for i in range(idx*BATCH_SIZE,(idx+1)*BATCH_SIZE):\n",
    "            #Download batch at index\n",
    "            path = \"temp/{}/batch_{}.hdf5\".format(self.mode,i)\n",
    "            key = \"{}/batch_{}.hdf5\".format(self.mode,i)\n",
    "            bucket.download_file(Key=key,Filename=path)\n",
    "        \n",
    "            f = h5py.File(path,\"r\")\n",
    "            try:\n",
    "                X_train[j,:,:,:,:] = f['/image'].value[self.angles,:,:,:]\n",
    "                y_train[j,:] = f['/labels'].value\n",
    "                j += 1\n",
    "            finally:\n",
    "                f.close()\n",
    "                os.remove(path) \n",
    "        return X_train, y_train\n",
    "class SingleScanSequencer(Sequence):\n",
    "    idx_dict={}\n",
    "    \n",
    "    def __init__(self,num_batches,bucket_name,mode=\"train_scan\",target_zone = 0):\n",
    "        self.num_batches = num_batches\n",
    "        self.bucket_name = bucket_name\n",
    "        self.mode = mode\n",
    "        self.key_id, self.secret_key = hfuncs.GetAWSCredentials()\n",
    "        self.mode = mode\n",
    "        self.target_zone = target_zone\n",
    "        self.angles = np.arange(0,64,64//ANGLES)\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "    def on_epoch_end(self):\n",
    "        pass\n",
    "    def __getitem__(self,idx):\n",
    "        #Get Client\n",
    "        client = hfuncs.GetAWSClient(self.key_id,self.secret_key)\n",
    "        bucket = client.Bucket(self.bucket_name)\n",
    "        \n",
    "        #Initialize vectors\n",
    "        X_train = np.zeros((BATCH_SIZE,ANGLES,FINAL_WIDTH,FINAL_HEIGHT,CHANNELS))\n",
    "        y_train = np.zeros((BATCH_SIZE,1))\n",
    "        \n",
    "        j=0\n",
    "        for i in range(idx*BATCH_SIZE,(idx+1)*BATCH_SIZE):\n",
    "            #Download batch at index\n",
    "            path = \"temp/{}/batch_{}.hdf5\".format(self.mode,i)\n",
    "            key = \"{}/batch_{}.hdf5\".format(self.mode,i)\n",
    "            bucket.download_file(Key=key,Filename=path)\n",
    "        \n",
    "            f = h5py.File(path,\"r\")\n",
    "            try:\n",
    "                X_train[j,:,:,:,:] = f['/image'].value[self.angles,:,:,:]\n",
    "                y_train[j,:] = f['/labels'].value[self.target_zone]\n",
    "                j += 1\n",
    "            finally:\n",
    "                f.close()\n",
    "                os.remove(path) \n",
    "        return X_train, y_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from twilio.rest import Client\n",
    "import configparser\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "class SMSNotifier(Callback):\n",
    "    def on_epoch_end(self,epoch,logs=None):\n",
    "        #Execute every other epoch\n",
    "        if epoch % 2 == 0:\n",
    "            #Get config credentials\n",
    "            config = configparser.ConfigParser()\n",
    "            config.read('twilio.conf')\n",
    "            account_sid = config['DEFAULT']['AccountID']\n",
    "            auth_token = config['DEFAULT']['AuthToken']\n",
    "            #Get client\n",
    "            client = Client(account_sid, auth_token)\n",
    "            #Create message\n",
    "            if logs is not None:\n",
    "                message = \"Epoch {} complete. Loss: {} Val_loss: {} \".format(epoch,\n",
    "                                                                             logs.get('loss'),\n",
    "                                                                             logs.get('val_loss'))\n",
    "            else:\n",
    "                message = \"Epoch {} complete. No loss data available.\".format(epoch)\n",
    "            #Sendmessage\n",
    "            message = client.messages.create(\n",
    "                to=\"+16178884129\", \n",
    "                from_=\"+18572142288\",\n",
    "                body=message)\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def weighted_binary_crossentropy(y_true, y_pred):\n",
    "    x = 0.10 #Approximate percentage of positives in each of the 17 zones\n",
    "    POS_ADJ = 0.5/x\n",
    "    NEG_ADJ = 0.5/(1-x)\n",
    "    n_values = BATCH_SIZE\n",
    "    elems = (tf.unstack(y_true,num=n_values,axis=0)) \n",
    "    adj = tf.map_fn(lambda x:tuple([tf.cond(tf.equal(x[i],1.),lambda:POS_ADJ,lambda: NEG_ADJ) for i in range(n_values)]),\n",
    "                    elems, \n",
    "                    dtype = tuple([tf.float32 for i in range(n_values)]) )\n",
    "    adj = tf.stack(adj,axis=0)\n",
    "    return K.mean(tf.multiply(adj,K.binary_crossentropy(y_true,y_pred)),axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Build pre-trained V2 model\n",
    "import numpy as np\n",
    "from keras.layers import Input,Flatten,Dense,Concatenate,Dropout,concatenate,GlobalMaxPool2D\n",
    "from keras.models import Model\n",
    "from datetime import datetime\n",
    "from keras.callbacks import TensorBoard,EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import binary_accuracy\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.applications import inception_resnet_v2 as resv2\n",
    "from keras.layers.core import Lambda\n",
    "import tensorflow as tf\n",
    "\n",
    "def ToRGB(x):\n",
    "    max_v = tf.reduce_max(x)\n",
    "    min_v = tf.reduce_min(x)\n",
    "    max_rgb = tf.constant(255,dtype=x.dtype)\n",
    "    x = tf.floordiv(tf.multiply(tf.subtract(x,min_v),max_rgb),tf.subtract(max_v,min_v))\n",
    "    return x\n",
    "\n",
    "def ToNewShape(x):\n",
    "    ndim = len(x.shape)\n",
    "    if ndim == 5:\n",
    "        return tf.reverse(tf.transpose(x,[0,1,3,2,4]),[-3])\n",
    "    elif ndim == 4:\n",
    "        return tf.reverse(tf.transpose(x,[0,2,1,3]),[-3])\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected number of dims!\")\n",
    "\n",
    "def getModel(lstm_dim = 500):\n",
    "    #Single model image\n",
    "    input_img = Input(shape=(FINAL_WIDTH,FINAL_HEIGHT,CHANNELS))\n",
    "    \n",
    "    #preprocess and extract channels\n",
    "    input_img_pp = Lambda(ToRGB)(input_img)\n",
    "    input_img_pp = Lambda(resv2.preprocess_input)(input_img_pp)\n",
    "    input_img_pp = Lambda(ToNewShape)(input_img_pp)\n",
    "    \n",
    "    #Extract 3 channels in order to use pre-trained weights\n",
    "    extract_channels = resv2.conv2d_bn(input_img_pp,\n",
    "                  filters=3,\n",
    "                  kernel_size=1,\n",
    "                  strides=(3,2),\n",
    "                  padding='same',\n",
    "                  activation='relu',\n",
    "                  use_bias=False,\n",
    "                  name=None)\n",
    "    #Load resnet\n",
    "    resnet = resv2.InceptionResNetV2(include_top=False,\n",
    "                          weights='imagenet',\n",
    "                          input_tensor=None,\n",
    "                          input_shape=(FINAL_HEIGHT,FINAL_WIDTH,3),\n",
    "                          pooling='max')\n",
    "    for l in resnet.layers:\n",
    "        l.trainable=False\n",
    "\n",
    "    #Take off top\n",
    "    reduced_resnet = Model(resnet.input,resnet.get_layer('mixed_6a').output)\n",
    "    output = reduced_resnet(extract_channels)\n",
    "    output = GlobalMaxPool2D()(output)\n",
    "    print(\"Max pool output {}\".format(output.shape))\n",
    "    intermediate_model = Model(input_img,output)\n",
    "\n",
    "    #Time distributed model\n",
    "    input_scan = Input(shape=(ANGLES,FINAL_HEIGHT,FINAL_WIDTH,CHANNELS))  \n",
    "    sequenced_model = TimeDistributed(intermediate_model)(input_scan)\n",
    "    #model = Model(input_scan,sequenced_model)\n",
    "    #print(model.input_shape,model.output_shape)\n",
    "    #One lstm layer for now\n",
    "\n",
    "    lstm = LSTM(lstm_dim,recurrent_dropout=0.20)(sequenced_model)\n",
    "\n",
    "    #Finally, 17 dense layers connected to the output\n",
    "    output_nodes = []\n",
    "    for i in range(ZONES):\n",
    "        output_nodes.append(Dense(1,activation='sigmoid')(lstm))\n",
    "\n",
    "    out = concatenate(output_nodes)\n",
    "    del resnet\n",
    "    #complete model\n",
    "    try:\n",
    "        return Model(input_scan, out)\n",
    "    finally:\n",
    "        del intermediate_model,reduced_resnet\n",
    "def getSingleModel(lstm_dim = 250):\n",
    "    #Single model image\n",
    "    input_img = Input(shape=(FINAL_WIDTH,FINAL_HEIGHT,CHANNELS))\n",
    "    \n",
    "    #preprocess and extract channels\n",
    "    input_img_pp = Lambda(ToRGB)(input_img)\n",
    "    input_img_pp = Lambda(resv2.preprocess_input)(input_img_pp)\n",
    "    input_img_pp = Lambda(ToNewShape)(input_img_pp)\n",
    "    \n",
    "    #Extract 3 channels in order to use pre-trained weights\n",
    "    extract_channels = resv2.conv2d_bn(input_img_pp,\n",
    "                  filters=3,\n",
    "                  kernel_size=1,\n",
    "                  strides=(3,2),\n",
    "                  padding='same',\n",
    "                  activation='relu',\n",
    "                  use_bias=False,\n",
    "                  name=None)\n",
    "    #Load resnet\n",
    "    resnet = resv2.InceptionResNetV2(include_top=False,\n",
    "                          weights='imagenet',\n",
    "                          input_tensor=None,\n",
    "                          input_shape=(FINAL_HEIGHT,FINAL_WIDTH,3),\n",
    "                          pooling='max')\n",
    "    for l in resnet.layers:\n",
    "        l.trainable=False\n",
    "\n",
    "    #Take off top\n",
    "    reduced_resnet = Model(resnet.input,resnet.get_layer('mixed_6a').output)\n",
    "    output = reduced_resnet(extract_channels)\n",
    "    output = GlobalMaxPool2D()(output)\n",
    "    print(\"Max pool output {}\".format(output.shape))\n",
    "    intermediate_model = Model(input_img,output)\n",
    "\n",
    "    #Time distributed model\n",
    "    input_scan = Input(shape=(ANGLES,FINAL_WIDTH,FINAL_HEIGHT,CHANNELS))  \n",
    "    sequenced_model = TimeDistributed(intermediate_model)(input_scan)\n",
    "    #model = Model(input_scan,sequenced_model)\n",
    "    #print(model.input_shape,model.output_shape)\n",
    "    #One lstm layer for now\n",
    "\n",
    "    lstm = LSTM(lstm_dim,recurrent_dropout=0.20)(sequenced_model)\n",
    "\n",
    "    #Finally, 1 dense layers\n",
    "    out = Dense(1,activation='sigmoid')(lstm)\n",
    "    del resnet\n",
    "    #complete model\n",
    "    try:\n",
    "        return Model(input_scan, out)\n",
    "    finally:\n",
    "        del intermediate_model,reduced_resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max pool output (?, 1088)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "#optimizer\n",
    "lr = 0.001\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "description = \"Pretrained model\"\n",
    "lstm_dim = 1000\n",
    "recurrent_model = getSingleModel(20)\n",
    "#recurrent_model = load_model('check_points/2017_9_26_21_ScanModel_00-0.70.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling model...\n",
      "Initializing generators...\n",
      "Beginning training...\n",
      "Epoch 1/100\n",
      "890/891 [============================>.] - ETA: 1s - loss: 0.7811 - binary_accuracy: 0.2146 - binary_crossentropy: 0.7228Epoch 00000: val_binary_crossentropy improved from inf to 0.73197, saving model to check_points/2017_9_27_4_ScanModel_00-0.78.hdf5\n",
      "891/891 [==============================] - 1919s - loss: 0.7807 - binary_accuracy: 0.2144 - binary_crossentropy: 0.7228 - val_loss: 0.7771 - val_binary_accuracy: 0.1295 - val_binary_crossentropy: 0.7320\n",
      "Epoch 2/100\n",
      "890/891 [============================>.] - ETA: 1s - loss: 0.7397 - binary_accuracy: 0.1157 - binary_crossentropy: 0.7380Epoch 00001: val_binary_crossentropy did not improve\n",
      "891/891 [==============================] - 1924s - loss: 0.7394 - binary_accuracy: 0.1156 - binary_crossentropy: 0.7381 - val_loss: 0.7746 - val_binary_accuracy: 0.1295 - val_binary_crossentropy: 0.7541\n",
      "Epoch 3/100\n",
      "890/891 [============================>.] - ETA: 1s - loss: 0.7397 - binary_accuracy: 0.1157 - binary_crossentropy: 0.7587Epoch 00002: val_binary_crossentropy did not improve\n",
      "\n",
      "Epoch 00002: reducing learning rate to 0.00020000000949949026.\n",
      "891/891 [==============================] - 2161s - loss: 0.7394 - binary_accuracy: 0.1156 - binary_crossentropy: 0.7588 - val_loss: 0.7741 - val_binary_accuracy: 0.1295 - val_binary_crossentropy: 0.7594\n",
      "Epoch 4/100\n",
      "890/891 [============================>.] - ETA: 1s - loss: 0.7384 - binary_accuracy: 0.1157 - binary_crossentropy: 0.7590Epoch 00003: val_binary_crossentropy did not improve\n",
      "891/891 [==============================] - 2123s - loss: 0.7380 - binary_accuracy: 0.1156 - binary_crossentropy: 0.7590 - val_loss: 0.7742 - val_binary_accuracy: 0.1295 - val_binary_crossentropy: 0.7578\n",
      "Epoch 5/100\n",
      "890/891 [============================>.] - ETA: 1s - loss: 0.7384 - binary_accuracy: 0.1157 - binary_crossentropy: 0.7654Epoch 00004: val_binary_crossentropy did not improve\n",
      "891/891 [==============================] - 2260s - loss: 0.7380 - binary_accuracy: 0.1156 - binary_crossentropy: 0.7655 - val_loss: 0.7742 - val_binary_accuracy: 0.1295 - val_binary_crossentropy: 0.7579\n",
      "Epoch 6/100\n",
      "890/891 [============================>.] - ETA: 1s - loss: 0.7383 - binary_accuracy: 0.1157 - binary_crossentropy: 0.7587Epoch 00005: val_binary_crossentropy did not improve\n",
      "891/891 [==============================] - 2173s - loss: 0.7380 - binary_accuracy: 0.1156 - binary_crossentropy: 0.7587 - val_loss: 0.7743 - val_binary_accuracy: 0.1295 - val_binary_crossentropy: 0.7566\n",
      "Epoch 7/100\n",
      "890/891 [============================>.] - ETA: 1s - loss: 0.7384 - binary_accuracy: 0.1157 - binary_crossentropy: 0.7590Epoch 00006: val_binary_crossentropy did not improve\n",
      "891/891 [==============================] - 2120s - loss: 0.7380 - binary_accuracy: 0.1156 - binary_crossentropy: 0.7590 - val_loss: 0.7743 - val_binary_accuracy: 0.1295 - val_binary_crossentropy: 0.7569\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'History' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-13fa57379005>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m                                        \u001b[0;31m#validation_steps = 5,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                                        \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchkpt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreduce_lr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mestop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                                       use_multiprocessing =False,workers=1)\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"aws ec2 stop-instances --instance-ids i-0172c75d2de9bad71\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'History' object is not iterable"
     ]
    }
   ],
   "source": [
    "print(\"Compiling model...\")\n",
    "adam = Adam(lr,beta_1=beta1,beta_2=beta2)\n",
    "recurrent_model.compile(optimizer=adam,\n",
    "                          metrics=[binary_accuracy,binary_crossentropy],\n",
    "                         loss= weighted_binary_crossentropy)\n",
    "\n",
    "#Tensorboard\n",
    "x = datetime.today()\n",
    "stamp = \"{}-{}-{}_{}:{}:{}_lr-{}_beta1-{}_beta2-{}_lstmDim-{}_ANG-{}_{}\".format(x.year,x.month,\n",
    "                                                     x.day,x.hour,x.minute,\n",
    "                                                     x.second,lr,beta1,beta2,lstm_dim,ANGLES,description)\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(stamp),histogram_freq=0,batch_size=BATCH_SIZE,\n",
    "                          write_grads=False,write_images=False)\n",
    "#Model checkpoint\n",
    "check_point_dir = 'check_points/'\n",
    "if not os.path.isdir(check_point_dir):\n",
    "    os.makedirs(check_point_dir)    \n",
    "chkpt = ModelCheckpoint(os.path.join(check_point_dir,\"{}_{}_{}_{}_\".format(x.year,x.month,x.day,x.hour) + \"ScanModel_{epoch:02d}-{val_loss:.2f}.hdf5\"),\n",
    "                       monitor='val_binary_crossentropy',\n",
    "                       verbose=1,\n",
    "                       save_best_only=True)\n",
    "#Reduce learning rate on plateau\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_binary_crossentropy',\n",
    "                             factor=0.2,\n",
    "                             patience=1,\n",
    "                             verbose=1,\n",
    "                             min_lr=0.0001,\n",
    "                             cooldown = 4)\n",
    "#Notifications\n",
    "notify = SMSNotifier()\n",
    "\n",
    "#Early stopping callback\n",
    "estop = EarlyStopping(monitor='val_binary_crossentropy',min_delta=0.0001,patience=5)\n",
    "\n",
    "#Generators and fit\n",
    "print(\"Initializing generators...\")\n",
    "#Bucket with clean data\n",
    "UPLOAD_BUCKET = 'cleandhsdata' #bucket where clean data was stored\n",
    "key_id, secret_key = hfuncs.GetAWSCredentials()\n",
    "client = hfuncs.GetAWSClient(key_id,secret_key)\n",
    "bucket = client.Bucket(UPLOAD_BUCKET)\n",
    "\n",
    "#Initialize train sequencer\n",
    "mode =\"train_scan\"\n",
    "num_batches_train = (sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1)//BATCH_SIZE #train,test,val root directories have their own keys\n",
    "#num_batches = 660//BATCH_SIZE\n",
    "train_seq = SingleScanSequencer(num_batches_train,UPLOAD_BUCKET,mode=mode,target_zone=0)\n",
    "\n",
    "#Initialize validation sequencer\n",
    "mode = \"val_scan\"\n",
    "num_batches_val = (sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1)//BATCH_SIZE #train,test,val root directories have their own keys\n",
    "val_seq = SingleScanSequencer(num_batches_val,UPLOAD_BUCKET,mode=mode,target_zone=0)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Beginning training...\")\n",
    "try:\n",
    "    hist,model = recurrent_model.fit_generator(train_seq,\n",
    "                                       steps_per_epoch=num_batches_train,\n",
    "                                       #steps_per_epoch=5,\n",
    "                                       epochs=100,\n",
    "                                       validation_data = val_seq,\n",
    "                                       validation_steps = num_batches_val,\n",
    "                                       #validation_steps = 5,\n",
    "                                       callbacks=[tensorboard,chkpt,reduce_lr,notify,estop],\n",
    "                                      use_multiprocessing =False,workers=1)\n",
    "finally:\n",
    "    os.system(\"aws ec2 stop-instances --instance-ids i-0172c75d2de9bad71\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
