{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import HelperFuncs as hfuncs\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to first try training a CNN on the individual images.\n",
    "We will be using binary cross entropy across the 17 regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import HelperFuncs as hfuncs\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.data_utils import Sequence\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "FINAL_WIDTH = 400\n",
    "FINAL_HEIGHT = 600\n",
    "CHANNELS = 1\n",
    "ZONES = 17\n",
    "ANGLES = 16\n",
    "#Create directories for sequencer function if they don't exist\n",
    "for d in ['temp/train_scan/','temp/test_scan/','temp/val_scan/']:\n",
    "    if not os.path.isdir(d):\n",
    "        print(\"Created directory: {}\".format(d))\n",
    "        os.makedirs(d)\n",
    "        \n",
    "class ScanSequencer(Sequence):\n",
    "    idx_dict={}\n",
    "    \n",
    "    def __init__(self,num_batches,bucket_name,mode=\"train_scan\"):\n",
    "        self.num_batches = num_batches\n",
    "        self.bucket_name = bucket_name\n",
    "        self.mode = mode\n",
    "        self.key_id, self.secret_key = hfuncs.GetAWSCredentials()\n",
    "        self.mode = mode\n",
    "        self.angles = np.arange(0,64,64//ANGLES)\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "    def on_epoch_end(self):\n",
    "        pass\n",
    "    def __getitem__(self,idx):\n",
    "        #Get Client\n",
    "        client = hfuncs.GetAWSClient(self.key_id,self.secret_key)\n",
    "        bucket = client.Bucket(self.bucket_name)\n",
    "        \n",
    "        #Initialize vectors\n",
    "        X_train = np.zeros((BATCH_SIZE,ANGLES,FINAL_HEIGHT,FINAL_WIDTH,CHANNELS))\n",
    "        y_train = np.zeros((BATCH_SIZE,ZONES))\n",
    "        \n",
    "        j=0\n",
    "        for i in range(idx*BATCH_SIZE,(idx+1)*BATCH_SIZE):\n",
    "            #Download batch at index\n",
    "            path = \"temp/{}/batch_{}.hdf5\".format(self.mode,i)\n",
    "            key = \"{}/batch_{}.hdf5\".format(self.mode,i)\n",
    "            bucket.download_file(Key=key,Filename=path)\n",
    "        \n",
    "            f = h5py.File(path,\"r\")\n",
    "            try:\n",
    "                X_train[j,:,:,:,:] = np.swapaxes(hfuncs.ToRGBUnits(f['/image'].value[self.angles,:,:,:]),1,2)[:,::-1,:,:]\n",
    "                y_train[j,:] = f['/labels'].value\n",
    "                j += 1\n",
    "            finally:\n",
    "                f.close()\n",
    "                os.remove(path) \n",
    "        return X_train, y_train\n",
    "         \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from twilio.rest import Client\n",
    "import configparser\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "class SMSNotifier(Callback):\n",
    "    def on_epoch_end(self,epoch,logs=None):\n",
    "        #Execute every other epoch\n",
    "        if epoch % 2 == 0:\n",
    "            #Get config credentials\n",
    "            config = configparser.ConfigParser()\n",
    "            config.read('twilio.conf')\n",
    "            account_sid = config['DEFAULT']['AccountID']\n",
    "            auth_token = config['DEFAULT']['AuthToken']\n",
    "            #Get client\n",
    "            client = Client(account_sid, auth_token)\n",
    "            #Create message\n",
    "            if logs is not None:\n",
    "                message = \"Epoch {} complete. Loss: {} Val_loss: {} \".format(epoch,\n",
    "                                                                             logs.get('loss'),\n",
    "                                                                             logs.get('val_loss'))\n",
    "            else:\n",
    "                message = \"Epoch {} complete. No loss data available.\".format(epoch)\n",
    "            #Sendmessage\n",
    "            message = client.messages.create(\n",
    "                to=\"+16178884129\", \n",
    "                from_=\"+18572142288\",\n",
    "                body=message)\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def weighted_binary_crossentropy(y_true, y_pred):\n",
    "    x = 0.10 #Approximate percentage of positives in each of the 17 zones\n",
    "    POS_ADJ = 0.5/x\n",
    "    NEG_ADJ = 0.5/(1-x)\n",
    "    n_values = BATCH_SIZE\n",
    "    elems = (tf.unstack(y_true,num=n_values,axis=0)) \n",
    "    adj = tf.map_fn(lambda x:tuple([tf.cond(tf.equal(x[i],1.),lambda:POS_ADJ,lambda: NEG_ADJ) for i in range(n_values)]),\n",
    "                    elems, \n",
    "                    dtype = tuple([tf.float32 for i in range(n_values)]) )\n",
    "    adj = tf.stack(adj,axis=0)\n",
    "    return K.mean(tf.multiply(adj,K.binary_crossentropy(y_true,y_pred)),axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build pre-trained V2 model\n",
    "import numpy as np\n",
    "from keras.layers import Input,Flatten,Dense,Concatenate,Dropout,concatenate,MaxPooling2D\n",
    "from keras.models import Model\n",
    "from datetime import datetime\n",
    "from keras.callbacks import TensorBoard,EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import binary_accuracy\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.applications import inception_resnet_v2 as resv2\n",
    "from keras.layers.core import Lambda\n",
    "\n",
    "def getModel(lstm_dim = 1000):\n",
    "    #Single model image\n",
    "    input_img = Input(shape=(FINAL_HEIGHT,FINAL_WIDTH,CHANNELS))\n",
    "    #preprocess and extract channels\n",
    "    input_img_pp = Lambda(resv2.preprocess_input)(input_img)\n",
    "    extract_channels = resv2.conv2d_bn(input_img_pp,\n",
    "                  filters=3,\n",
    "                  kernel_size=1,\n",
    "                  strides=1,\n",
    "                  padding='same',\n",
    "                  activation='relu',\n",
    "                  use_bias=False,\n",
    "                  name=None)\n",
    "    #Load resnet\n",
    "    resnet = resv2.InceptionResNetV2(include_top=False,\n",
    "                          weights='imagenet',\n",
    "                          input_tensor=None,\n",
    "                          input_shape=(FINAL_HEIGHT,FINAL_WIDTH,3),\n",
    "                          pooling='max')\n",
    "    for l in resnet.layers:\n",
    "        l.trainable=False\n",
    "\n",
    "    #Take off top\n",
    "    reduced_resnet = Model(resnet.input,resnet.get_layer('mixed_6a').output)\n",
    "    output = reduced_resnet(extract_channels)\n",
    "    output = Flatten()(output)\n",
    "    intermediate_model = Model(input_img,output)\n",
    "\n",
    "    print(\"intermediate model: {}\".format(intermediate_model.input_shape))\n",
    "    #Time distributed model\n",
    "    input_scan = Input(shape=(ANGLES,FINAL_HEIGHT,FINAL_WIDTH,CHANNELS))  \n",
    "    sequenced_model = TimeDistributed(intermediate_model)(input_scan)\n",
    "    #model = Model(input_scan,sequenced_model)\n",
    "    #print(model.input_shape,model.output_shape)\n",
    "    #One lstm layer for now\n",
    "\n",
    "    lstm = LSTM(lstm_dim,recurrent_dropout=0.20)(sequenced_model)\n",
    "\n",
    "    #Finally, 17 dense layers connected to the output\n",
    "    output_nodes = []\n",
    "    for i in range(ZONES):\n",
    "        output_nodes.append(Dense(1,activation='sigmoid')(lstm))\n",
    "\n",
    "    out = concatenate(output_nodes)\n",
    "\n",
    "    #complete model\n",
    "    x = Model(input_scan, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer\n",
    "lr = 0.001\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "description = \"Pretrained model\"\n",
    "\n",
    "recurrent_model = getModel()\n",
    "\n",
    "adam = Adam(lr,beta_1=beta1,beta_2=beta2)\n",
    "print(\"Compiling model...\")\n",
    "recurrent_model.compile(optimizer=adam,\n",
    "                          metrics=[binary_accuracy,binary_crossentropy],\n",
    "                         loss= weighted_binary_crossentropy)\n",
    "\n",
    "#Tensorboard\n",
    "x = datetime.today()\n",
    "stamp = \"{}-{}-{}_{}:{}:{}_lr-{}_beta1-{}_beta2-{}_lstmDim-{}_ANG-{}_{}\".format(x.year,x.month,\n",
    "                                                     x.day,x.hour,x.minute,\n",
    "                                                     x.second,lr,beta1,beta2,lstm_dim,ANGLES,description)\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(stamp),histogram_freq=0,batch_size=BATCH_SIZE,\n",
    "                          write_grads=False,write_images=False)\n",
    "#Model checkpoint\n",
    "check_point_dir = 'check_points/'\n",
    "if not os.path.isdir(check_point_dir):\n",
    "    os.makedirs(check_point_dir)    \n",
    "chkpt = ModelCheckpoint(os.path.join(check_point_dir,\"{}_{}_{}_{}_\".format(x.year,x.month,x.day,x.hour) + \"ScanModel_{epoch:02d}-{val_loss:.2f}.hdf5\"),\n",
    "                       monitor='val_binary_crossentropy',\n",
    "                       verbose=1,\n",
    "                       save_best_only=True)\n",
    "#Reduce learning rate on plateau\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_binary_crossentropy',\n",
    "                             factor=0.2,\n",
    "                             patience=1,\n",
    "                             verbose=1,\n",
    "                             min_lr=0.0001,\n",
    "                             cooldown = 4)\n",
    "#Notifications\n",
    "notify = SMSNotifier()\n",
    "\n",
    "#Early stopping callback\n",
    "estop = EarlyStopping(monitor='val_binary_crossentropy',min_delta=0.0001,patience=10)\n",
    "\n",
    "#Generators and fit\n",
    "print(\"Initializing generators...\")\n",
    "#Bucket with clean data\n",
    "UPLOAD_BUCKET = 'cleandhsdata' #bucket where clean data was stored\n",
    "key_id, secret_key = hfuncs.GetAWSCredentials()\n",
    "client = hfuncs.GetAWSClient(key_id,secret_key)\n",
    "bucket = client.Bucket(UPLOAD_BUCKET)\n",
    "\n",
    "#Initialize train sequencer\n",
    "mode =\"train_scan\"\n",
    "num_batches_train = (sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1)//BATCH_SIZE #train,test,val root directories have their own keys\n",
    "#num_batches = 660//BATCH_SIZE\n",
    "train_seq = ScanSequencer(num_batches_train,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "#Initialize validation sequencer\n",
    "mode = \"val_scan\"\n",
    "num_batches_val = (sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1)//BATCH_SIZE #train,test,val root directories have their own keys\n",
    "val_seq = ScanSequencer(num_batches_val,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Beginning training...\")\n",
    "hist = recurrent_model.fit_generator(train_seq,\n",
    "                                       steps_per_epoch=num_batches_train,\n",
    "                                       #steps_per_epoch=5,\n",
    "                                       epochs=100,\n",
    "                                       validation_data = val_seq,\n",
    "                                       validation_steps = num_batches_val,\n",
    "                                       #validation_steps = 5,\n",
    "                                       callbacks=[tensorboard,chkpt,reduce_lr,notify,estop],\n",
    "                                      use_multiprocessing =True,workers=2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (DSHenv_3.5)",
   "language": "python",
   "name": "dhsenv_3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
