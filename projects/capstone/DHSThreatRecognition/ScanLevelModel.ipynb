{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import HelperFuncs as hfuncs\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.data_utils import Sequence\n",
    "import h5py\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a class that allows us to sequentially load entire scans. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "FINAL_WIDTH = 400\n",
    "FINAL_HEIGHT = 600\n",
    "CHANNELS = 1\n",
    "ZONES = 17\n",
    "\n",
    "class ScanSequencer(Sequence):\n",
    "    idx_dict={}\n",
    "    \n",
    "    def __init__(self,num_batches,bucket_name,mode=\"train_scan\"):\n",
    "        self.num_batches = num_batches\n",
    "        self.bucket_name = bucket_name\n",
    "        self.mode = mode\n",
    "        self.key_id, self.secret_key = hfuncs.GetAWSCredentials()\n",
    "        self.mode = mode\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "    def __getitem__(self,idx):\n",
    "        #Get Client\n",
    "        client = hfuncs.GetAWSClient(self.key_id,self.secret_key)\n",
    "        bucket = client.Bucket(self.bucket_name)\n",
    "        \n",
    "        #Download batch at index\n",
    "        path = \"temp/{}/batch_{}.hdf5\".format(self.mode,idx)\n",
    "        key = \"{}/batch_{}.hdf5\".format(self.mode,idx)\n",
    "        bucket.download_file(Key=key,Filename=path)\n",
    "        \n",
    "        f = h5py.File(path,\"r\")\n",
    "        try:\n",
    "            return f['/image'].value, f['/labels'].value\n",
    "        finally:\n",
    "            f.close()\n",
    "            os.remove(path) \n",
    "         \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "from keras.models import load_model\n",
    "\n",
    "#Load trained model\n",
    "BASE_MODEL = load_model(\"Final_CNNRes.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from inception_resnet_v1 import InceptionResNetV1\n",
    "from keras.layers import Input,Flatten,Dense,Concatenate,Dropout\n",
    "from keras.models import Model\n",
    "import keras\n",
    "from datetime import datetime\n",
    "from keras.callbacks import TensorBoard,EarlyStopping\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras import metrics\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "def trainCNNResRNNAdam(alpha,lr,beta1,beta2,pooling,dropout_rate,description=\"RNN\"):\n",
    "    #Number of angles per scan \n",
    "    ANGLES = 32\n",
    "\n",
    "    #Hidden dimensions\n",
    "    LSTM_OUTPUT_DIM = 1000\n",
    "\n",
    "    #Build new sequential model,Removing dense layers from model\n",
    "    input_scan = Input(shape=(ANGLES,FINAL_WIDTH,FINAL_HEIGHT,CHANNELS))\n",
    "    \n",
    "    #From our base model, we'll remove 18 extraneous layers (one concatenation and 17 dense) since these\n",
    "    #were only used for making predictions for each of the zones using the extracted features.  Now, we'll be making \n",
    "    #predictions only after we've seen all of the supplied angle slices. \n",
    "    sequenced_model = TimeDistributed(BASE_MODEL.layers[-19])(input_scan)\n",
    "\n",
    "    #One lstm layer for now\n",
    "    lstm = LSTM(LSTM_OUTPUT_DIM)(sequenced_model)\n",
    "\n",
    "    #Finally, 17 dense layers connected to the output\n",
    "    output_nodes = []\n",
    "    for i in range(ZONES):\n",
    "        output_nodes.append(Dense(1,activation='sigmoid')(lstm))\n",
    "\n",
    "    out = keras.layers.concatenate(output_nodes)\n",
    "\n",
    "    #complete model\n",
    "    recurrent_model = Model(input_scan, out)\n",
    "   \n",
    "    #optimizer\n",
    "    adam = Adam(lr,beta_1=beta1,beta_2=beta2)\n",
    "    multi_label_model.compile(optimizer=adam,\n",
    "                              metrics=[metrics.binary_accuracy],\n",
    "                             loss= 'binary_crossentropy')\n",
    "\n",
    "    #Tensorboard\n",
    "    x = datetime.today()\n",
    "    stamp = \"{}-{}-{}_{}:{}:{}_lr-{}_beta1-{}_beta2-{}_alpha-{}_pooling-{}_dropout-{}\".format(x.year,x.month,\n",
    "                                                         x.day,x.hour,x.minute,\n",
    "                                                         x.second,lr,beta1,beta2,alpha,pooling,dropout_rate)\n",
    "    tensorboard = TensorBoard(log_dir=\"logs/{}\".format(stamp),histogram_freq=0,batch_size=BATCH_SIZE,\n",
    "                              write_grads=False,write_images=True)\n",
    "\n",
    "    #Generators and fit\n",
    "\n",
    "    #Bucket with clean data\n",
    "    UPLOAD_BUCKET = 'cleandhsdata' #bucket where clean data was stored\n",
    "    key_id, secret_key = hfuncs.GetAWSCredentials()\n",
    "    client = hfuncs.GetAWSClient(key_id,secret_key)\n",
    "    bucket = client.Bucket(UPLOAD_BUCKET)\n",
    "\n",
    "    #Initialize train sequencer\n",
    "    mode =\"train_scan\"\n",
    "    num_batches = sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1 #train,test,val root directories have their own keys\n",
    "    train_seq = Sequencer(num_batches,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "    #Initialize validation sequencer\n",
    "    mode = \"val_scan\"\n",
    "    num_batches = sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1 #train,test,val root directories have their own keys\n",
    "    val_seq = Sequencer(num_batches,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "    #Early stopping callback\n",
    "    estop = EarlyStopping(monitor='loss',min_delta=0.005,patience=3)\n",
    "    \n",
    "    hist = multi_label_model.fit_generator(train_seq,\n",
    "                                           steps_per_epoch=20,\n",
    "                                           epochs=5,\n",
    "                                           validation_data = val_seq,\n",
    "                                           validation_steps = 50,\n",
    "                                           callbacks=[tensorboard,estop],\n",
    "                                          use_multiprocessing =True,workers=4)\n",
    "    return hist,multi_label_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load moodel that we trained on individual slices. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can remove the top layers of the model since these were used for making predictions at the slice level.  We know we can get rid of 18 superfluous layers, one concatenation, and 17 dense layers used to make predictions for each of the 17 regions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_format': 'channels_last', 'name': 'global_max_pooling2d_3', 'trainable': True}\n",
      "{'data_format': 'channels_last', 'name': 'global_max_pooling2d_3', 'trainable': True}\n"
     ]
    }
   ],
   "source": [
    "print(base_model.layers[-20].get_config())\n",
    "print(base_model.get_layer(name='global_max_pooling2d_3').get_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Input,Flatten,Dense,Concatenate,Dropout\n",
    "\n",
    "#Number of angles per scan \n",
    "ANGLES = 32\n",
    "\n",
    "#Hidden dimensions\n",
    "LSTM_OUTPUT_DIM = 1000\n",
    "\n",
    "#Build new sequential model,Removing dense layers from model\n",
    "input_scan = Input(shape=(ANGLES,FINAL_WIDTH,FINAL_HEIGHT,CHANNELS))\n",
    "sequenced_model = TimeDistributed(BASE_MODEL.get_layer(name='global_max_pooling2d_3'))(input_scan)\n",
    "\n",
    "#One lstm layer for now\n",
    "lstm = LSTM(LSTM_OUTPUT_DIM)(sequenced_model)\n",
    "\n",
    "#Finally, 17 dense layers connected to the output\n",
    "output_nodes = []\n",
    "for i in range(ZONES):\n",
    "    output_nodes.append(Dense(1,activation='sigmoid')(lstm))\n",
    "\n",
    "out = keras.layers.concatenate(output_nodes)\n",
    "\n",
    "#complete model\n",
    "multi_label_model = Model(input_scan, out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's try training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
