{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import HelperFuncs as hfuncs\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.data_utils import Sequence\n",
    "import h5py\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a class that allows us to sequentially load entire scans. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5\n",
    "FINAL_WIDTH = 400\n",
    "FINAL_HEIGHT = 600\n",
    "CHANNELS = 1\n",
    "ZONES = 17\n",
    "ANGLES = 16\n",
    "#Create directories for sequencer function if they don't exist\n",
    "for d in ['temp/train_scan/','temp/test_scan/','temp/val_scan/']:\n",
    "    if not os.path.isdir(d):\n",
    "        print(\"Created directory: {}\".format(d))\n",
    "        os.makedirs(d)\n",
    "        \n",
    "class ScanSequencer(Sequence):\n",
    "    idx_dict={}\n",
    "    \n",
    "    def __init__(self,num_batches,bucket_name,mode=\"train_scan\"):\n",
    "        self.num_batches = num_batches\n",
    "        self.bucket_name = bucket_name\n",
    "        self.mode = mode\n",
    "        self.key_id, self.secret_key = hfuncs.GetAWSCredentials()\n",
    "        self.mode = mode\n",
    "        self.angles = np.arange(0,64,64//ANGLES)\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "    def __getitem__(self,idx):\n",
    "        #Get Client\n",
    "        client = hfuncs.GetAWSClient(self.key_id,self.secret_key)\n",
    "        bucket = client.Bucket(self.bucket_name)\n",
    "        \n",
    "        #Initialize vectors\n",
    "        X_train = np.zeros((BATCH_SIZE,ANGLES,FINAL_WIDTH,FINAL_HEIGHT,CHANNELS))\n",
    "        y_train = np.zeros((BATCH_SIZE,ZONES))\n",
    "        \n",
    "        j=0\n",
    "        for i in range(idx*BATCH_SIZE,(idx+1)*BATCH_SIZE):\n",
    "            #Download batch at index\n",
    "            path = \"temp/{}/batch_{}.hdf5\".format(self.mode,i)\n",
    "            key = \"{}/batch_{}.hdf5\".format(self.mode,i)\n",
    "            bucket.download_file(Key=key,Filename=path)\n",
    "        \n",
    "            f = h5py.File(path,\"r\")\n",
    "            try:\n",
    "                X_train[j,:,:,:,:] = f['/image'].value[self.angles,:,:,:]\n",
    "                y_train[j,:] = f['/labels'].value\n",
    "                j += 1\n",
    "            finally:\n",
    "                f.close()\n",
    "                os.remove(path) \n",
    "        return X_train, y_train\n",
    "         \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from keras.models import load_model\n",
    "\n",
    "#Load trained model\n",
    "BASE_MODEL = load_model(\"Final_CNNRes.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at layers 19 and 20 to see where we should begin our new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'dropout_3', 'trainable': True, 'rate': 0.2}\n",
      "{'name': 'global_max_pooling2d_3', 'trainable': True, 'data_format': 'channels_last'}\n",
      "(None, 896)\n",
      "(None, 400, 600, 1)\n"
     ]
    }
   ],
   "source": [
    "print(BASE_MODEL.layers[-19].get_config())\n",
    "print(BASE_MODEL.get_layer(name='global_max_pooling2d_3').get_config())\n",
    "print(BASE_MODEL.layers[-19].output_shape)\n",
    "print(BASE_MODEL.layers[0].input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know from above that our base model will output 896 dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE_OUTPUT_DIM = BASE_MODEL.layers[-19].output_shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (20,8,400,600,1) into shape (8,400,600,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6753ef7f45ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mX_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (20,8,400,600,1) into shape (8,400,600,1)"
     ]
    }
   ],
   "source": [
    "#Load validaitno data on memory so that we can visualize the weights.\n",
    "UPLOAD_BUCKET = 'cleandhsdata' #bucket where clean data was stored\n",
    "key_id, secret_key = hfuncs.GetAWSCredentials()\n",
    "client = hfuncs.GetAWSClient(key_id,secret_key)\n",
    "bucket = client.Bucket(UPLOAD_BUCKET)\n",
    "\n",
    "#Initialize validation sequencer\n",
    "mode = \"val_scan\"\n",
    "num_batches = sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1 #train,test,val root directories have their own keys\n",
    "val_seq = ScanSequencer(num_batches,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "X_val = np.zeros(shape=(30,ANGLES,FINAL_WIDTH,FINAL_HEIGHT,CHANNELS))\n",
    "y_val = np.zeros(shape=(30,ZONES))\n",
    "\n",
    "#Val samples 1 and 2 were accidentally overwritten\n",
    "for i in range(0,30):\n",
    "    j = i+2\n",
    "    X_val[i,:,:,:,:], y_val[i,:] = val_seq.__getitem__(j)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input,Flatten,Dense,Concatenate,Dropout,concatenate\n",
    "from keras.models import Model\n",
    "from datetime import datetime\n",
    "from keras.callbacks import TensorBoard,EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras import metrics\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "def trainCNNResRNNAdamComplex(lr,beta1,beta2,lstm_dim,description=\"RNN_complex\"):\n",
    "    #Build new sequential model,Removing dense layers from model\n",
    "    print(\"Creating model...\")\n",
    "    input_scan = Input(shape=(ANGLES,FINAL_WIDTH,FINAL_HEIGHT,CHANNELS))\n",
    "    \n",
    "    #From our base model, we'll remove 18 extraneous layers (one concatenation and 17 dense) since these\n",
    "    #were only used for making predictions for each of the zones using the extracted features.  Now, we'll be making \n",
    "    #predictions only after we've seen all of the supplied angle slices.\n",
    "    new_model_bottom = Model(BASE_MODEL.input,BASE_MODEL.get_layer('dropout_3').output)\n",
    "    \n",
    "    sequenced_model = TimeDistributed(new_model_bottom)(input_scan)\n",
    "\n",
    "    #One lstm layer for now\n",
    "    lstm = LSTM(lstm_dim,recurrent_dropout=0.20)(sequenced_model)\n",
    "\n",
    "    #Finally, 17 dense layers connected to the output\n",
    "    output_nodes = []\n",
    "    for i in range(ZONES):\n",
    "        output_nodes.append(Dense(1,activation='sigmoid')(lstm))\n",
    "\n",
    "    out = concatenate(output_nodes)\n",
    "\n",
    "    #complete model\n",
    "    recurrent_model = Model(input_scan, out)\n",
    "   \n",
    "    #optimizer\n",
    "    adam = Adam(lr,beta_1=beta1,beta_2=beta2)\n",
    "    print(\"Compiling model...\")\n",
    "    recurrent_model.compile(optimizer=adam,\n",
    "                              metrics=[metrics.binary_accuracy],\n",
    "                             loss= 'binary_crossentropy')\n",
    "\n",
    "    #Tensorboard\n",
    "    x = datetime.today()\n",
    "    stamp = \"{}-{}-{}_{}:{}:{}_lr-{}_beta1-{}_beta2-{}_lstmDim-{}_ANG-{}_{}\".format(x.year,x.month,\n",
    "                                                         x.day,x.hour,x.minute,\n",
    "                                                         x.second,lr,beta1,beta2,lstm_dim,ANGLES,description)\n",
    "    tensorboard = TensorBoard(log_dir=\"logs/{}\".format(stamp),histogram_freq=0,batch_size=BATCH_SIZE,\n",
    "                              write_grads=False,write_images=False)\n",
    "\n",
    "    #Generators and fit\n",
    "    print(\"Initializing generators...\")\n",
    "    #Bucket with clean data\n",
    "    UPLOAD_BUCKET = 'cleandhsdata' #bucket where clean data was stored\n",
    "    key_id, secret_key = hfuncs.GetAWSCredentials()\n",
    "    client = hfuncs.GetAWSClient(key_id,secret_key)\n",
    "    bucket = client.Bucket(UPLOAD_BUCKET)\n",
    "\n",
    "    #Initialize train sequencer\n",
    "    mode =\"train_scan\"\n",
    "    num_batches = sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1 #train,test,val root directories have their own keys\n",
    "    train_seq = ScanSequencer(num_batches,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "    #Initialize validation sequencer\n",
    "    mode = \"val_scan\"\n",
    "    num_batches = sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1 #train,test,val root directories have their own keys\n",
    "    val_seq = ScanSequencer(num_batches,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "    #Early stopping callback\n",
    "    estop = EarlyStopping(monitor='val_loss',min_delta=0.005,patience=4)\n",
    "    print(\"Beginning training...\")\n",
    "    hist = recurrent_model.fit_generator(train_seq,\n",
    "                                           steps_per_epoch=660//BATCH_SIZE,\n",
    "                                           epochs=50,\n",
    "                                           validation_data = val_seq,\n",
    "                                           validation_steps = 200//BATCH_SIZE,\n",
    "                                           callbacks=[tensorboard,estop],\n",
    "                                          use_multiprocessing =False,workers=1)\n",
    "    return hist,recurrent_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input,Flatten,Dense,Concatenate,Dropout,concatenate,Activation\n",
    "from keras.models import Model\n",
    "from datetime import datetime\n",
    "from keras.callbacks import TensorBoard,EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras import metrics\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras import backend as K\n",
    "\n",
    "K.set_learning_phase(1)#Due to some keras bug, flag has to be constant. It works when I use dropout though?\n",
    "\n",
    "def trainCNNResRNNAdamSimple(lr,beta1,beta2,lstm_dim,description=\"RNN_simple\"):\n",
    "    #Build new sequential model,Removing dense layers from model\n",
    "    print(\"Creating model...\")\n",
    "    input_scan = Input(shape=(ANGLES,FINAL_WIDTH,FINAL_HEIGHT,CHANNELS))\n",
    "    \n",
    "    #From our base model, we'll remove 18 extraneous layers (one concatenation and 17 dense) since these\n",
    "    #were only used for making predictions for each of the zones using the extracted features.  Now, we'll be making \n",
    "    #predictions only after we've seen all of the supplied angle slices.    \n",
    "    new_model_bottom = Model(BASE_MODEL.input,BASE_MODEL.output)\n",
    "    sequenced_model = TimeDistributed(new_model_bottom)(input_scan)\n",
    "\n",
    "    #One lstm layer for now\n",
    "    lstm = LSTM(lstm_dim,recurrent_dropout=0)(sequenced_model)\n",
    "\n",
    "    #Treat output of LSTM as individual guessess for each of the 17 zones\n",
    "    \n",
    "\n",
    "    out = Activation('sigmoid')(lstm)\n",
    "\n",
    "    #complete model\n",
    "    recurrent_model = Model(input_scan, out)\n",
    "   \n",
    "    #optimizer\n",
    "    adam = Adam(lr,beta_1=beta1,beta_2=beta2)\n",
    "    print(\"Compiling model...\")\n",
    "    recurrent_model.compile(optimizer=adam,\n",
    "                              metrics=[metrics.binary_accuracy],\n",
    "                             loss= 'binary_crossentropy')\n",
    "\n",
    "    #Tensorboard\n",
    "    x = datetime.today()\n",
    "    stamp = \"{}-{}-{}_{}:{}:{}_lr-{}_beta1-{}_beta2-{}_lstmDim-{}_ANG-{}_{}\".format(x.year,x.month,\n",
    "                                                         x.day,x.hour,x.minute,\n",
    "                                                         x.second,lr,beta1,beta2,lstm_dim,ANGLES,description)\n",
    "    tensorboard = TensorBoard(log_dir=\"logs/{}\".format(stamp),histogram_freq=0,batch_size=BATCH_SIZE,\n",
    "                              write_grads=False,write_images=False)\n",
    "\n",
    "    #Generators and fit\n",
    "    print(\"Initializing generators...\")\n",
    "    #Bucket with clean data\n",
    "    UPLOAD_BUCKET = 'cleandhsdata' #bucket where clean data was stored\n",
    "    key_id, secret_key = hfuncs.GetAWSCredentials()\n",
    "    client = hfuncs.GetAWSClient(key_id,secret_key)\n",
    "    bucket = client.Bucket(UPLOAD_BUCKET)\n",
    "\n",
    "    #Initialize train sequencer\n",
    "    mode =\"train_scan\"\n",
    "    num_batches = (sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1)//BATCH_SIZE #train,test,val root directories have their own keys\n",
    "    train_seq = ScanSequencer(num_batches,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "    #Initialize validation sequencer\n",
    "    mode = \"val_scan\"\n",
    "    num_batches = (sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1)//BATCH_SIZE #train,test,val root directories have their own keys\n",
    "    val_seq = ScanSequencer(num_batches,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "    #Early stopping callback\n",
    "    estop = EarlyStopping(monitor='eval_loss',min_delta=0.005,patience=4)\n",
    "    print(\"Beginning training...\")\n",
    "    hist = recurrent_model.fit_generator(train_seq,\n",
    "                                           steps_per_epoch=660//5,\n",
    "                                           epochs=50,\n",
    "                                           validation_data = val_seq,\n",
    "                                           validation_steps = 200//BATCH_SIZE,\n",
    "                                           callbacks=[tensorboard,estop],\n",
    "                                          use_multiprocessing =True,workers=1)\n",
    "    return hist,recurrent_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input,Flatten,Dense,Concatenate,Dropout,concatenate,Activation\n",
    "from keras.models import Model\n",
    "from datetime import datetime\n",
    "from keras.callbacks import TensorBoard,EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras import metrics\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras import backend as K\n",
    "from keras.layers.merge import Average\n",
    "\n",
    "K.set_learning_phase(1)#Due to some keras bug, flag has to be constant. It works when I use dropout though?\n",
    "\n",
    "def trainCNNResAdamAverage(lr,beta1,beta2,lstm_dim,description=\"RNN_average\"):\n",
    "    #Build new sequential model,Removing dense layers from model\n",
    "    print(\"Creating model...\")\n",
    "    input_scan = Input(shape=(ANGLES,FINAL_WIDTH,FINAL_HEIGHT,CHANNELS))\n",
    "    \n",
    "    #From our base model, we'll remove 18 extraneous layers (one concatenation and 17 dense) since these\n",
    "    #were only used for making predictions for each of the zones using the extracted features.  Now, we'll be making \n",
    "    #predictions only after we've seen all of the supplied angle slices.    \n",
    "    new_model_bottom = Model(BASE_MODEL.input,BASE_MODEL.output)\n",
    "    sequenced_model = TimeDistributed(new_model_bottom)(input_scan)\n",
    "    \n",
    "    \n",
    "    #One lstm layer for now\n",
    "    out = Average()(sequenced_model)\n",
    "\n",
    "    #Treat output of LSTM as individual guessess for each of the 17 zones\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    #complete model\n",
    "    recurrent_model = Model(input_scan, out)\n",
    "   \n",
    "    #optimizer\n",
    "    adam = Adam(lr,beta_1=beta1,beta_2=beta2)\n",
    "    print(\"Compiling model...\")\n",
    "    recurrent_model.compile(optimizer=adam,\n",
    "                              metrics=[metrics.binary_accuracy],\n",
    "                             loss= 'binary_crossentropy')\n",
    "\n",
    "    #Tensorboard\n",
    "    x = datetime.today()\n",
    "    stamp = \"{}-{}-{}_{}:{}:{}_lr-{}_beta1-{}_beta2-{}_lstmDim-{}_ANG-{}_{}\".format(x.year,x.month,\n",
    "                                                         x.day,x.hour,x.minute,\n",
    "                                                         x.second,lr,beta1,beta2,lstm_dim,ANGLES,description)\n",
    "    tensorboard = TensorBoard(log_dir=\"logs/{}\".format(stamp),histogram_freq=0,batch_size=BATCH_SIZE,\n",
    "                              write_grads=False,write_images=False)\n",
    "\n",
    "    #Generators and fit\n",
    "    print(\"Initializing generators...\")\n",
    "    #Bucket with clean data\n",
    "    UPLOAD_BUCKET = 'cleandhsdata' #bucket where clean data was stored\n",
    "    key_id, secret_key = hfuncs.GetAWSCredentials()\n",
    "    client = hfuncs.GetAWSClient(key_id,secret_key)\n",
    "    bucket = client.Bucket(UPLOAD_BUCKET)\n",
    "\n",
    "    #Initialize train sequencer\n",
    "    mode =\"train_scan\"\n",
    "    num_batches = (sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1)//BATCH_SIZE #train,test,val root directories have their own keys\n",
    "    train_seq = ScanSequencer(num_batches,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "    #Initialize validation sequencer\n",
    "    mode = \"val_scan\"\n",
    "    num_batches = (sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1)//BATCH_SIZE #train,test,val root directories have their own keys\n",
    "    val_seq = ScanSequencer(num_batches,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "    #Early stopping callback\n",
    "    estop = EarlyStopping(monitor='eval_loss',min_delta=0.005,patience=4)\n",
    "    print(\"Beginning training...\")\n",
    "    hist = recurrent_model.fit_generator(train_seq,\n",
    "                                           steps_per_epoch=660//5,\n",
    "                                           epochs=100,\n",
    "                                           validation_data = val_seq,\n",
    "                                           validation_steps = 200//BATCH_SIZE,\n",
    "                                           callbacks=[tensorboard,estop],\n",
    "                                          use_multiprocessing =True,workers=1)\n",
    "    return hist,recurrent_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input,Flatten,Dense,Concatenate,Dropout,concatenate,Activation\n",
    "from keras.models import Model\n",
    "from datetime import datetime\n",
    "from keras.callbacks import TensorBoard,EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras import metrics\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras import backend as K\n",
    "\n",
    "K.set_learning_phase(1)#Due to some keras bug, flag has to be constant. It works when I use dropout though?\n",
    "\n",
    "def trainCNNResRNNAdamSimple(lr,beta1,beta2,lstm_dim,description=\"RNN_simple\"):\n",
    "    #Build new sequential model,Removing dense layers from model\n",
    "    print(\"Creating model...\")\n",
    "    input_scan = Input(shape=(ANGLES,FINAL_WIDTH,FINAL_HEIGHT,CHANNELS))\n",
    "    \n",
    "    #From our base model, we'll remove 18 extraneous layers (one concatenation and 17 dense) since these\n",
    "    #were only used for making predictions for each of the zones using the extracted features.  Now, we'll be making \n",
    "    #predictions only after we've seen all of the supplied angle slices.    \n",
    "    new_model_bottom = Model(BASE_MODEL.input,BASE_MODEL.output)\n",
    "    sequenced_model = TimeDistributed(new_model_bottom)(input_scan)\n",
    "\n",
    "    #One lstm layer for now\n",
    "    lstm = LSTM(lstm_dim,recurrent_dropout=0)(sequenced_model)\n",
    "\n",
    "    #Treat output of LSTM as individual guessess for each of the 17 zones\n",
    "    \n",
    "\n",
    "    out = Activation('sigmoid')(lstm)\n",
    "\n",
    "    #complete model\n",
    "    recurrent_model = Model(input_scan, out)\n",
    "   \n",
    "    #optimizer\n",
    "    adam = Adam(lr,beta_1=beta1,beta_2=beta2)\n",
    "    print(\"Compiling model...\")\n",
    "    recurrent_model.compile(optimizer=adam,\n",
    "                              metrics=[metrics.binary_accuracy],\n",
    "                             loss= 'binary_crossentropy')\n",
    "\n",
    "    #Tensorboard\n",
    "    x = datetime.today()\n",
    "    stamp = \"{}-{}-{}_{}:{}:{}_lr-{}_beta1-{}_beta2-{}_lstmDim-{}_ANG-{}_{}\".format(x.year,x.month,\n",
    "                                                         x.day,x.hour,x.minute,\n",
    "                                                         x.second,lr,beta1,beta2,lstm_dim,ANGLES,description)\n",
    "    tensorboard = TensorBoard(log_dir=\"logs/{}\".format(stamp),histogram_freq=0,batch_size=BATCH_SIZE,\n",
    "                              write_grads=False,write_images=False)\n",
    "\n",
    "    #Generators and fit\n",
    "    print(\"Initializing generators...\")\n",
    "    #Bucket with clean data\n",
    "    UPLOAD_BUCKET = 'cleandhsdata' #bucket where clean data was stored\n",
    "    key_id, secret_key = hfuncs.GetAWSCredentials()\n",
    "    client = hfuncs.GetAWSClient(key_id,secret_key)\n",
    "    bucket = client.Bucket(UPLOAD_BUCKET)\n",
    "\n",
    "    #Initialize train sequencer\n",
    "    mode =\"train_scan\"\n",
    "    num_batches = (sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1)//BATCH_SIZE #train,test,val root directories have their own keys\n",
    "    train_seq = ScanSequencer(num_batches,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "    #Initialize validation sequencer\n",
    "    mode = \"val_scan\"\n",
    "    num_batches = (sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1)//BATCH_SIZE #train,test,val root directories have their own keys\n",
    "    val_seq = ScanSequencer(num_batches,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "    #Early stopping callback\n",
    "    estop = EarlyStopping(monitor='eval_loss',min_delta=0.005,patience=4)\n",
    "    print(\"Beginning training...\")\n",
    "    hist = recurrent_model.fit_generator(train_seq,\n",
    "                                           steps_per_epoch=660//5,\n",
    "                                           epochs=100,\n",
    "                                           validation_data = val_seq,\n",
    "                                           validation_steps = 200//BATCH_SIZE,\n",
    "                                           callbacks=[tensorboard,estop],\n",
    "                                          use_multiprocessing =True,workers=1)\n",
    "    return hist,recurrent_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input,Flatten,Dense,Concatenate,Dropout,concatenate\n",
    "from keras.models import Model\n",
    "from datetime import datetime\n",
    "from keras.callbacks import TensorBoard,EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras import metrics\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import LSTM\n",
    "K.set_learning_phase(1)#Due to some keras bug, flag has to be constant. It works when I use dropout though?\n",
    "def trainCNNResRNNAdamComplexIsh(lr,beta1,beta2,lstm_dim,description=\"RNN_complexISH\"):\n",
    "    #Build new sequential model,Removing dense layers from model\n",
    "    print(\"Creating model...\")\n",
    "    input_scan = Input(shape=(ANGLES,FINAL_WIDTH,FINAL_HEIGHT,CHANNELS))\n",
    "    \n",
    "    #From our base model, we'll remove 18 extraneous layers (one concatenation and 17 dense) since these\n",
    "    #were only used for making predictions for each of the zones using the extracted features.  Now, we'll be making \n",
    "    #predictions only after we've seen all of the supplied angle slices.\n",
    "    new_model_bottom = Model(BASE_MODEL.input,BASE_MODEL.output)\n",
    "    \n",
    "    sequenced_model = TimeDistributed(new_model_bottom)(input_scan)\n",
    "\n",
    "    #One lstm layer for now\n",
    "    lstm = LSTM(lstm_dim)(sequenced_model)\n",
    "\n",
    "    #Finally, 17 dense layers connected to the output\n",
    "    output_nodes = []\n",
    "    for i in range(ZONES):\n",
    "        output_nodes.append(Dense(1,activation='sigmoid')(lstm))\n",
    "\n",
    "    out = concatenate(output_nodes)\n",
    "\n",
    "    #complete model\n",
    "    recurrent_model = Model(input_scan, out)\n",
    "   \n",
    "    #optimizer\n",
    "    adam = Adam(lr,beta_1=beta1,beta_2=beta2)\n",
    "    print(\"Compiling model...\")\n",
    "    recurrent_model.compile(optimizer=adam,\n",
    "                              metrics=[metrics.binary_accuracy],\n",
    "                             loss= 'binary_crossentropy')\n",
    "\n",
    "    #Tensorboard\n",
    "    x = datetime.today()\n",
    "    stamp = \"{}-{}-{}_{}:{}:{}_lr-{}_beta1-{}_beta2-{}_lstmDim-{}_ANG-{}_{}\".format(x.year,x.month,\n",
    "                                                         x.day,x.hour,x.minute,\n",
    "                                                         x.second,lr,beta1,beta2,lstm_dim,ANGLES,description)\n",
    "    tensorboard = TensorBoard(log_dir=\"logs/{}\".format(stamp),histogram_freq=0,batch_size=BATCH_SIZE,\n",
    "                              write_grads=False,write_images=False)\n",
    "\n",
    "    #Generators and fit\n",
    "    print(\"Initializing generators...\")\n",
    "    #Bucket with clean data\n",
    "    UPLOAD_BUCKET = 'cleandhsdata' #bucket where clean data was stored\n",
    "    key_id, secret_key = hfuncs.GetAWSCredentials()\n",
    "    client = hfuncs.GetAWSClient(key_id,secret_key)\n",
    "    bucket = client.Bucket(UPLOAD_BUCKET)\n",
    "\n",
    "    #Initialize train sequencer\n",
    "    mode =\"train_scan\"\n",
    "    num_batches = sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1 #train,test,val root directories have their own keys\n",
    "    train_seq = ScanSequencer(num_batches,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "    #Initialize validation sequencer\n",
    "    mode = \"val_scan\"\n",
    "    num_batches = sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1 #train,test,val root directories have their own keys\n",
    "    val_seq = ScanSequencer(num_batches,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "    #Early stopping callback\n",
    "    estop = EarlyStopping(monitor='val_loss',min_delta=0.005,patience=4)\n",
    "    print(\"Beginning training...\")\n",
    "    hist = recurrent_model.fit_generator(train_seq,\n",
    "                                           steps_per_epoch=660//BATCH_SIZE,\n",
    "                                           epochs=50,\n",
    "                                           validation_data = val_seq,\n",
    "                                           validation_steps = 200//BATCH_SIZE,\n",
    "                                           callbacks=[tensorboard,estop],\n",
    "                                          use_multiprocessing =False,workers=1)\n",
    "    return hist,recurrent_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's try training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model...\n",
      "Compiling model...\n",
      "Initializing generators...\n",
      "Beginning training...\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    hist,recurrent_model = trainCNNResRNNAdamComplex(lr=0.0001,beta1=0.9,beta2=0.99,lstm_dim=800)\n",
    "    del hist,recurrent_model\n",
    "except:\n",
    "    print(\"SOmething went wrong\")\n",
    "try:\n",
    "    hist,recurrent_model = trainCNNResAdamAverage(lr=0.0001,beta1=0.9,beta2=0.99,lstm_dim=0)\n",
    "    del hist,recurrent_model\n",
    "except:\n",
    "    print(\"Something went wrong\")                                                  \n",
    "try:\n",
    "    hist,recurrent_model = trainCNNResRNNAdamSimple(lr=0.0001,beta1=0.9,beta2=0.999,lstm_dim=17)\n",
    "    del hist,recurrent_model\n",
    "except:\n",
    "    print(\"Something went wrong\") \n",
    "try:\n",
    "    hist,recurrent_model = trainCNNResRNNAdamComplexIsh(lr=0.0001,beta1=0.9,beta2=0.999,lstm_dim=50)\n",
    "    del hist,recurrent_model\n",
    "except:\n",
    "    print(\"Something went wrong\") \n",
    "                                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Input,Flatten,Dense,Concatenate,Dropout\n",
    "\n",
    "#Number of angles per scan \n",
    "ANGLES = 32\n",
    "\n",
    "#Hidden dimensions\n",
    "LSTM_OUTPUT_DIM = 1000\n",
    "\n",
    "#Build new sequential model,Removing dense layers from model\n",
    "input_scan = Input(shape=(ANGLES,FINAL_WIDTH,FINAL_HEIGHT,CHANNELS))\n",
    "sequenced_model = TimeDistributed(BASE_MODEL.get_layer(name='global_max_pooling2d_3'))(input_scan)\n",
    "\n",
    "#One lstm layer for now\n",
    "lstm = LSTM(LSTM_OUTPUT_DIM)(sequenced_model)\n",
    "\n",
    "#Finally, 17 dense layers connected to the output\n",
    "output_nodes = []\n",
    "for i in range(ZONES):\n",
    "    output_nodes.append(Dense(1,activation='sigmoid')(lstm))\n",
    "\n",
    "out = keras.layers.concatenate(output_nodes)\n",
    "\n",
    "#complete model\n",
    "multi_label_model = Model(input_scan, out)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
