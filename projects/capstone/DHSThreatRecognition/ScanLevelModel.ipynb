{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import HelperFuncs as hfuncs\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.data_utils import Sequence\n",
    "import h5py\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a class that allows us to sequentially load entire scans. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "FINAL_WIDTH = 400\n",
    "FINAL_HEIGHT = 600\n",
    "CHANNELS = 1\n",
    "ZONES = 17\n",
    "ANGLES = 16\n",
    "#Create directories for sequencer function if they don't exist\n",
    "for d in ['temp/train_scan/','temp/test_scan/','temp/val_scan/']:\n",
    "    if not os.path.isdir(d):\n",
    "        print(\"Created directory: {}\".format(d))\n",
    "        os.makedirs(d)\n",
    "        \n",
    "class ScanSequencer(Sequence):\n",
    "    idx_dict={}\n",
    "    \n",
    "    def __init__(self,num_batches,bucket_name,mode=\"train_scan\"):\n",
    "        self.num_batches = num_batches\n",
    "        self.bucket_name = bucket_name\n",
    "        self.mode = mode\n",
    "        self.key_id, self.secret_key = hfuncs.GetAWSCredentials()\n",
    "        self.mode = mode\n",
    "        self.angles = np.arange(0,64,64//ANGLES)\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "    def on_epoch_end(self):\n",
    "        pass\n",
    "    def __getitem__(self,idx):\n",
    "        #Get Client\n",
    "        client = hfuncs.GetAWSClient(self.key_id,self.secret_key)\n",
    "        bucket = client.Bucket(self.bucket_name)\n",
    "        \n",
    "        #Initialize vectors\n",
    "        X_train = np.zeros((BATCH_SIZE,ANGLES,FINAL_WIDTH,FINAL_HEIGHT,CHANNELS))\n",
    "        y_train = np.zeros((BATCH_SIZE,ZONES))\n",
    "        \n",
    "        j=0\n",
    "        for i in range(idx*BATCH_SIZE,(idx+1)*BATCH_SIZE):\n",
    "            #Download batch at index\n",
    "            path = \"temp/{}/batch_{}.hdf5\".format(self.mode,i)\n",
    "            key = \"{}/batch_{}.hdf5\".format(self.mode,i)\n",
    "            bucket.download_file(Key=key,Filename=path)\n",
    "        \n",
    "            f = h5py.File(path,\"r\")\n",
    "            try:\n",
    "                X_train[j,:,:,:,:] = f['/image'].value[self.angles,:,:,:]\n",
    "                y_train[j,:] = f['/labels'].value\n",
    "                j += 1\n",
    "            finally:\n",
    "                f.close()\n",
    "                os.remove(path) \n",
    "        return X_train, y_train\n",
    "         \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from twilio.rest import Client\n",
    "import configparser\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "class SMSNotifier(Callback):\n",
    "    def on_epoch_end(self,epoch,logs=None):\n",
    "        #Execute every other epoch\n",
    "        if epoch % 2 == 0:\n",
    "            #Get config credentials\n",
    "            config = configparser.ConfigParser()\n",
    "            config.read('twilio.conf')\n",
    "            account_sid = config['DEFAULT']['AccountID']\n",
    "            auth_token = config['DEFAULT']['AuthToken']\n",
    "            #Get client\n",
    "            client = Client(account_sid, auth_token)\n",
    "            #Create message\n",
    "            if logs is not None:\n",
    "                message = \"Epoch {} complete. Loss: {} Val_loss: {} \".format(epoch,\n",
    "                                                                             logs.get('loss'),\n",
    "                                                                             logs.get('val_loss'))\n",
    "            else:\n",
    "                message = \"Epoch {} complete. No loss data available.\".format(epoch)\n",
    "            #Sendmessage\n",
    "            message = client.messages.create(\n",
    "                to=\"+16178884129\", \n",
    "                from_=\"+18572142288\",\n",
    "                body=message)\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom loss to take care of class imbalance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "def weighted_binary_crossentropy(y_true, y_pred):\n",
    "    x = 0.10 #Approximate percentage of positives in each of the 17 zones\n",
    "    POS_ADJ = 0.5/x\n",
    "    NEG_ADJ = 0.5/(1-x)\n",
    "    n_values = BATCH_SIZE\n",
    "    elems = (tf.unstack(y_true,num=n_values,axis=0)) \n",
    "    adj = tf.map_fn(lambda x:tuple([tf.cond(tf.equal(x[i],1.),lambda:POS_ADJ,lambda: NEG_ADJ) for i in range(n_values)]),\n",
    "                    elems, \n",
    "                    dtype = tuple([tf.float32 for i in range(n_values)]) )\n",
    "    adj = tf.stack(adj,axis=0)\n",
    "    return K.mean(tf.multiply(adj,K.binary_crossentropy(y_true,y_pred)),axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "from keras.models import load_model\n",
    "\n",
    "#Load trained model\n",
    "BASE_MODEL = load_model(\"check_points/2017_9_25_4_singleSGD_00-0.75.hdf5\",custom_objects={'weighted_binary_crossentropy':weighted_binary_crossentropy})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at layers 19 and 20 to see where we should begin our new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer-21\n",
      "{'arguments': {'scale': 1.0}, 'function_type': 'lambda', 'output_shape_type': 'raw', 'name': 'block8_5', 'trainable': True, 'output_shape': (11, 11, 896), 'function': ('ã\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x03\\x00\\x00\\x00S\\x00\\x00\\x00s\\x14\\x00\\x00\\x00|\\x00\\x00d\\x01\\x00\\x19|\\x00\\x00d\\x02\\x00\\x19|\\x01\\x00\\x14\\x17S)\\x03Né\\x00\\x00\\x00\\x00é\\x01\\x00\\x00\\x00©\\x00)\\x02Ú\\x06inputsÚ\\x05scaler\\x03\\x00\\x00\\x00r\\x03\\x00\\x00\\x00ú[/home/ubuntu/machine-learning/projects/capstone/DHSThreatRecognition/inception_resnet_v1.pyÚ\\x08<lambda>\\x9a\\x00\\x00\\x00ó\\x00\\x00\\x00\\x00', None, None)}\n",
      "Layer-20\n",
      "{'trainable': True, 'data_format': 'channels_last', 'name': 'global_max_pooling2d_2'}\n",
      "Layer-19\n",
      "{'trainable': True, 'rate': 0.25, 'name': 'dropout_2'}\n",
      "Layer-18\n",
      "{'bias_regularizer': None, 'kernel_constraint': None, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'mode': 'fan_avg', 'scale': 1.0, 'seed': None, 'distribution': 'uniform'}}, 'units': 1, 'trainable': True, 'bias_constraint': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'activation': 'sigmoid', 'name': 'dense_18', 'kernel_regularizer': None, 'activity_regularizer': None, 'use_bias': True}\n",
      "Layer-17\n",
      "{'bias_regularizer': None, 'kernel_constraint': None, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'mode': 'fan_avg', 'scale': 1.0, 'seed': None, 'distribution': 'uniform'}}, 'units': 1, 'trainable': True, 'bias_constraint': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'activation': 'sigmoid', 'name': 'dense_19', 'kernel_regularizer': None, 'activity_regularizer': None, 'use_bias': True}\n",
      "Layer-16\n",
      "{'bias_regularizer': None, 'kernel_constraint': None, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'mode': 'fan_avg', 'scale': 1.0, 'seed': None, 'distribution': 'uniform'}}, 'units': 1, 'trainable': True, 'bias_constraint': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'activation': 'sigmoid', 'name': 'dense_20', 'kernel_regularizer': None, 'activity_regularizer': None, 'use_bias': True}\n",
      "Layer-15\n",
      "{'bias_regularizer': None, 'kernel_constraint': None, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'mode': 'fan_avg', 'scale': 1.0, 'seed': None, 'distribution': 'uniform'}}, 'units': 1, 'trainable': True, 'bias_constraint': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'activation': 'sigmoid', 'name': 'dense_21', 'kernel_regularizer': None, 'activity_regularizer': None, 'use_bias': True}\n",
      "Layer-14\n",
      "{'bias_regularizer': None, 'kernel_constraint': None, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'mode': 'fan_avg', 'scale': 1.0, 'seed': None, 'distribution': 'uniform'}}, 'units': 1, 'trainable': True, 'bias_constraint': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'activation': 'sigmoid', 'name': 'dense_22', 'kernel_regularizer': None, 'activity_regularizer': None, 'use_bias': True}\n",
      "Layer-13\n",
      "{'bias_regularizer': None, 'kernel_constraint': None, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'mode': 'fan_avg', 'scale': 1.0, 'seed': None, 'distribution': 'uniform'}}, 'units': 1, 'trainable': True, 'bias_constraint': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'activation': 'sigmoid', 'name': 'dense_23', 'kernel_regularizer': None, 'activity_regularizer': None, 'use_bias': True}\n",
      "Layer-12\n",
      "{'bias_regularizer': None, 'kernel_constraint': None, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'mode': 'fan_avg', 'scale': 1.0, 'seed': None, 'distribution': 'uniform'}}, 'units': 1, 'trainable': True, 'bias_constraint': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'activation': 'sigmoid', 'name': 'dense_24', 'kernel_regularizer': None, 'activity_regularizer': None, 'use_bias': True}\n",
      "Layer-11\n",
      "{'bias_regularizer': None, 'kernel_constraint': None, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'mode': 'fan_avg', 'scale': 1.0, 'seed': None, 'distribution': 'uniform'}}, 'units': 1, 'trainable': True, 'bias_constraint': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'activation': 'sigmoid', 'name': 'dense_25', 'kernel_regularizer': None, 'activity_regularizer': None, 'use_bias': True}\n",
      "Layer-10\n",
      "{'bias_regularizer': None, 'kernel_constraint': None, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'mode': 'fan_avg', 'scale': 1.0, 'seed': None, 'distribution': 'uniform'}}, 'units': 1, 'trainable': True, 'bias_constraint': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'activation': 'sigmoid', 'name': 'dense_26', 'kernel_regularizer': None, 'activity_regularizer': None, 'use_bias': True}\n",
      "Layer-9\n",
      "{'bias_regularizer': None, 'kernel_constraint': None, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'mode': 'fan_avg', 'scale': 1.0, 'seed': None, 'distribution': 'uniform'}}, 'units': 1, 'trainable': True, 'bias_constraint': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'activation': 'sigmoid', 'name': 'dense_27', 'kernel_regularizer': None, 'activity_regularizer': None, 'use_bias': True}\n",
      "Layer-8\n",
      "{'bias_regularizer': None, 'kernel_constraint': None, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'mode': 'fan_avg', 'scale': 1.0, 'seed': None, 'distribution': 'uniform'}}, 'units': 1, 'trainable': True, 'bias_constraint': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'activation': 'sigmoid', 'name': 'dense_28', 'kernel_regularizer': None, 'activity_regularizer': None, 'use_bias': True}\n",
      "Layer-7\n",
      "{'bias_regularizer': None, 'kernel_constraint': None, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'mode': 'fan_avg', 'scale': 1.0, 'seed': None, 'distribution': 'uniform'}}, 'units': 1, 'trainable': True, 'bias_constraint': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'activation': 'sigmoid', 'name': 'dense_29', 'kernel_regularizer': None, 'activity_regularizer': None, 'use_bias': True}\n",
      "Layer-6\n",
      "{'bias_regularizer': None, 'kernel_constraint': None, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'mode': 'fan_avg', 'scale': 1.0, 'seed': None, 'distribution': 'uniform'}}, 'units': 1, 'trainable': True, 'bias_constraint': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'activation': 'sigmoid', 'name': 'dense_30', 'kernel_regularizer': None, 'activity_regularizer': None, 'use_bias': True}\n",
      "Layer-5\n",
      "{'bias_regularizer': None, 'kernel_constraint': None, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'mode': 'fan_avg', 'scale': 1.0, 'seed': None, 'distribution': 'uniform'}}, 'units': 1, 'trainable': True, 'bias_constraint': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'activation': 'sigmoid', 'name': 'dense_31', 'kernel_regularizer': None, 'activity_regularizer': None, 'use_bias': True}\n",
      "Layer-4\n",
      "{'bias_regularizer': None, 'kernel_constraint': None, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'mode': 'fan_avg', 'scale': 1.0, 'seed': None, 'distribution': 'uniform'}}, 'units': 1, 'trainable': True, 'bias_constraint': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'activation': 'sigmoid', 'name': 'dense_32', 'kernel_regularizer': None, 'activity_regularizer': None, 'use_bias': True}\n",
      "Layer-3\n",
      "{'bias_regularizer': None, 'kernel_constraint': None, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'mode': 'fan_avg', 'scale': 1.0, 'seed': None, 'distribution': 'uniform'}}, 'units': 1, 'trainable': True, 'bias_constraint': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'activation': 'sigmoid', 'name': 'dense_33', 'kernel_regularizer': None, 'activity_regularizer': None, 'use_bias': True}\n",
      "Layer-2\n",
      "{'bias_regularizer': None, 'kernel_constraint': None, 'kernel_initializer': {'class_name': 'VarianceScaling', 'config': {'mode': 'fan_avg', 'scale': 1.0, 'seed': None, 'distribution': 'uniform'}}, 'units': 1, 'trainable': True, 'bias_constraint': None, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'activation': 'sigmoid', 'name': 'dense_34', 'kernel_regularizer': None, 'activity_regularizer': None, 'use_bias': True}\n",
      "Layer-1\n",
      "{'trainable': True, 'axis': -1, 'name': 'concatenate_2'}\n"
     ]
    }
   ],
   "source": [
    "for i in range(-21,0):\n",
    "    print(\"Layer{}\".format(i))\n",
    "    print(BASE_MODEL.layers[i].get_config())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 896)\n"
     ]
    }
   ],
   "source": [
    "print(BASE_MODEL.layers[-19].output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know from above that our base model will output 896 dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE_OUTPUT_DIM = BASE_MODEL.layers[-19].output_shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (20,8,400,600,1) into shape (8,400,600,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6753ef7f45ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mX_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (20,8,400,600,1) into shape (8,400,600,1)"
     ]
    }
   ],
   "source": [
    "#Load validation data on memory so that we can visualize the weights.\n",
    "UPLOAD_BUCKET = 'cleandhsdata' #bucket where clean data was stored\n",
    "key_id, secret_key = hfuncs.GetAWSCredentials()\n",
    "client = hfuncs.GetAWSClient(key_id,secret_key)\n",
    "bucket = client.Bucket(UPLOAD_BUCKET)\n",
    "\n",
    "#Initialize validation sequencer\n",
    "mode = \"val_scan\"\n",
    "num_batches = sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1 #train,test,val root directories have their own keys\n",
    "val_seq = ScanSequencer(num_batches,UPLOAD_BUCKET,mode=mode)\n",
    "size =5\n",
    "#Val samples 1 and 2 were accidentally overwritten so get second batch\n",
    "X_val, y_val = val_seq.__getitem__()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input,Flatten,Dense,Concatenate,Dropout,concatenate\n",
    "from keras.models import Model\n",
    "from datetime import datetime\n",
    "from keras.callbacks import TensorBoard,EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import binary_accuracy\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.losses import binary_crossentropy\n",
    "import inception_resnet_v1 as rv1\n",
    "\n",
    "def trainCNNResRNNAdamComplex(lr,beta1,beta2,lstm_dim,description=\"RNN_complex\"):\n",
    "    #Build new sequential model,Removing dense layers from model\n",
    "    print(\"Creating model...\")\n",
    "    input_scan = Input(shape=(ANGLES,FINAL_WIDTH,FINAL_HEIGHT,CHANNELS))\n",
    "    input_image = Input(shape=(FINAL_WIDTH,FINAL_HEIGHT,CHANNELS))\n",
    "    #From our base model, we'll remove 18 extraneous layers (one concatenation and 17 dense) since these\n",
    "    #were only used for making predictions for each of the zones using the extracted features.  Now, we'll be making \n",
    "    #predictions only after we've seen all of the supplied angle slices.\n",
    "    #new_model_bottom = Model(BASE_MODEL.input,BASE_MODEL.get_layer('dropout_2').output)\n",
    "    modified_resnet = rv1.InceptionResNetV1(include_top=False,\n",
    "                      weights=None,\n",
    "                      input_tensor=input_image,\n",
    "                      pooling='max',\n",
    "                     alpha = 1)\n",
    "    \n",
    "    sequenced_model = TimeDistributed(modified_resnet)(input_scan)\n",
    "\n",
    "    #One lstm layer for now\n",
    "    lstm = LSTM(lstm_dim,recurrent_dropout=0.20)(sequenced_model)\n",
    "\n",
    "    #Finally, 17 dense layers connected to the output\n",
    "    output_nodes = []\n",
    "    for i in range(ZONES):\n",
    "        output_nodes.append(Dense(1,activation='sigmoid')(lstm))\n",
    "\n",
    "    out = concatenate(output_nodes)\n",
    "\n",
    "    #complete model\n",
    "    recurrent_model = Model(input_scan, out)\n",
    "   \n",
    "    #optimizer\n",
    "    adam = Adam(lr,beta_1=beta1,beta_2=beta2)\n",
    "    print(\"Compiling model...\")\n",
    "    recurrent_model.compile(optimizer=adam,\n",
    "                              metrics=[binary_accuracy,binary_crossentropy],\n",
    "                             loss= weighted_binary_crossentropy)\n",
    "\n",
    "    #Tensorboard\n",
    "    x = datetime.today()\n",
    "    stamp = \"{}-{}-{}_{}:{}:{}_lr-{}_beta1-{}_beta2-{}_lstmDim-{}_ANG-{}_{}\".format(x.year,x.month,\n",
    "                                                         x.day,x.hour,x.minute,\n",
    "                                                         x.second,lr,beta1,beta2,lstm_dim,ANGLES,description)\n",
    "    tensorboard = TensorBoard(log_dir=\"logs/{}\".format(stamp),histogram_freq=0,batch_size=BATCH_SIZE,\n",
    "                              write_grads=False,write_images=False)\n",
    "    #Model checkpoint\n",
    "    check_point_dir = 'check_points/'\n",
    "    if not os.path.isdir(check_point_dir):\n",
    "        os.makedirs(check_point_dir)    \n",
    "    chkpt = ModelCheckpoint(os.path.join(check_point_dir,\"{}_{}_{}_{}_\".format(x.year,x.month,x.day,x.hour) + \"ScanModel_{epoch:02d}-{val_loss:.2f}.hdf5\"),\n",
    "                           monitor='val_binary_crossentropy',\n",
    "                           verbose=1,\n",
    "                           save_best_only=True)\n",
    "    #Reduce learning rate on plateau\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_binary_crossentropy',\n",
    "                                 factor=0.2,\n",
    "                                 patience=1,\n",
    "                                 verbose=1,\n",
    "                                 min_lr=0.00005,\n",
    "                                 cooldown = 4)\n",
    "    #Notifications\n",
    "    notify = SMSNotifier()\n",
    "    \n",
    "    #Early stopping callback\n",
    "    estop = EarlyStopping(monitor='val_binary_crossentropy',min_delta=0.0001,patience=10)\n",
    "    \n",
    "    #Generators and fit\n",
    "    print(\"Initializing generators...\")\n",
    "    #Bucket with clean data\n",
    "    UPLOAD_BUCKET = 'cleandhsdata' #bucket where clean data was stored\n",
    "    key_id, secret_key = hfuncs.GetAWSCredentials()\n",
    "    client = hfuncs.GetAWSClient(key_id,secret_key)\n",
    "    bucket = client.Bucket(UPLOAD_BUCKET)\n",
    "\n",
    "    #Initialize train sequencer\n",
    "    mode =\"train_scan\"\n",
    "    num_batches_train = (sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1)//BATCH_SIZE #train,test,val root directories have their own keys\n",
    "    #num_batches = 660//BATCH_SIZE\n",
    "    train_seq = ScanSequencer(num_batches_train,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "    #Initialize validation sequencer\n",
    "    mode = \"val_scan\"\n",
    "    num_batches_val = (sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1)//BATCH_SIZE #train,test,val root directories have their own keys\n",
    "    val_seq = ScanSequencer(num_batches_val,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "    \n",
    "    \n",
    "    print(\"Beginning training...\")\n",
    "    hist = recurrent_model.fit_generator(train_seq,\n",
    "                                           steps_per_epoch=num_batches_train,\n",
    "                                           #steps_per_epoch=5,\n",
    "                                           epochs=100,\n",
    "                                           validation_data = val_seq,\n",
    "                                           validation_steps = num_batches_val,\n",
    "                                           #validation_steps = 5,\n",
    "                                           callbacks=[tensorboard,chkpt,reduce_lr,notify,estop],\n",
    "                                          use_multiprocessing =False,workers=1)\n",
    "    return hist,recurrent_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from keras.layers import Input,Flatten,Dense,Concatenate,Dropout,concatenate,Activation\n",
    "from keras.models import Model\n",
    "from datetime import datetime\n",
    "from keras.callbacks import TensorBoard,EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras import metrics\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras import backend as K\n",
    "\n",
    "K.set_learning_phase(1)#Due to some keras bug, flag has to be constant. It works when I use dropout though?\n",
    "\n",
    "def trainCNNResRNNAdamSimple(lr,beta1,beta2,lstm_dim,description=\"RNN_simple\"):\n",
    "    #Build new sequential model,Removing dense layers from model\n",
    "    print(\"Creating model...\")\n",
    "    input_scan = Input(shape=(ANGLES,FINAL_WIDTH,FINAL_HEIGHT,CHANNELS))\n",
    "    \n",
    "    #From our base model, we'll remove 18 extraneous layers (one concatenation and 17 dense) since these\n",
    "    #were only used for making predictions for each of the zones using the extracted features.  Now, we'll be making \n",
    "    #predictions only after we've seen all of the supplied angle slices.    \n",
    "    new_model_bottom = Model(BASE_MODEL.input,BASE_MODEL.output)\n",
    "    sequenced_model = TimeDistributed(new_model_bottom)(input_scan)\n",
    "\n",
    "    #One lstm layer for now\n",
    "    lstm = LSTM(lstm_dim,recurrent_dropout=0)(sequenced_model)\n",
    "\n",
    "    #Treat output of LSTM as individual guessess for each of the 17 zones\n",
    "    \n",
    "\n",
    "    out = Activation('sigmoid')(lstm)\n",
    "\n",
    "    #complete model\n",
    "    recurrent_model = Model(input_scan, out)\n",
    "   \n",
    "    #optimizer\n",
    "    adam = Adam(lr,beta_1=beta1,beta_2=beta2)\n",
    "    print(\"Compiling model...\")\n",
    "    recurrent_model.compile(optimizer=adam,\n",
    "                              metrics=[metrics.binary_accuracy],\n",
    "                             loss= 'binary_crossentropy')\n",
    "\n",
    "    #Tensorboard\n",
    "    x = datetime.today()\n",
    "    stamp = \"{}-{}-{}_{}:{}:{}_lr-{}_beta1-{}_beta2-{}_lstmDim-{}_ANG-{}_{}\".format(x.year,x.month,\n",
    "                                                         x.day,x.hour,x.minute,\n",
    "                                                         x.second,lr,beta1,beta2,lstm_dim,ANGLES,description)\n",
    "    tensorboard = TensorBoard(log_dir=\"logs/{}\".format(stamp),histogram_freq=0,batch_size=BATCH_SIZE,\n",
    "                              write_grads=False,write_images=False)\n",
    "\n",
    "    #Generators and fit\n",
    "    print(\"Initializing generators...\")\n",
    "    #Bucket with clean data\n",
    "    UPLOAD_BUCKET = 'cleandhsdata' #bucket where clean data was stored\n",
    "    key_id, secret_key = hfuncs.GetAWSCredentials()\n",
    "    client = hfuncs.GetAWSClient(key_id,secret_key)\n",
    "    bucket = client.Bucket(UPLOAD_BUCKET)\n",
    "\n",
    "    #Initialize train sequencer\n",
    "    mode =\"train_scan\"\n",
    "    num_batches = (sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1)//BATCH_SIZE #train,test,val root directories have their own keys\n",
    "    train_seq = ScanSequencer(num_batches,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "    #Initialize validation sequencer\n",
    "    mode = \"val_scan\"\n",
    "    num_batches = (sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1)//BATCH_SIZE #train,test,val root directories have their own keys\n",
    "    val_seq = ScanSequencer(num_batches,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "    #Early stopping callback\n",
    "    estop = EarlyStopping(monitor='eval_loss',min_delta=0.005,patience=4)\n",
    "    print(\"Beginning training...\")\n",
    "    hist = recurrent_model.fit_generator(train_seq,\n",
    "                                           steps_per_epoch=660//5,\n",
    "                                           epochs=50,\n",
    "                                           validation_data = val_seq,\n",
    "                                           validation_steps = 200//BATCH_SIZE,\n",
    "                                           callbacks=[tensorboard,estop],\n",
    "                                          use_multiprocessing =True,workers=1)\n",
    "    return hist,recurrent_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from keras.layers import Input,Flatten,Dense,Concatenate,Dropout,concatenate,Activation\n",
    "from keras.models import Model\n",
    "from datetime import datetime\n",
    "from keras.callbacks import TensorBoard,EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras import metrics\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras import backend as K\n",
    "from keras.layers.merge import Average\n",
    "\n",
    "K.set_learning_phase(1)#Due to some keras bug, flag has to be constant. It works when I use dropout though?\n",
    "\n",
    "def trainCNNResAdamAverage(lr,beta1,beta2,lstm_dim,description=\"RNN_average\"):\n",
    "    #Build new sequential model,Removing dense layers from model\n",
    "    print(\"Creating model...\")\n",
    "    input_scan = Input(shape=(ANGLES,FINAL_WIDTH,FINAL_HEIGHT,CHANNELS))\n",
    "    \n",
    "    #From our base model, we'll remove 18 extraneous layers (one concatenation and 17 dense) since these\n",
    "    #were only used for making predictions for each of the zones using the extracted features.  Now, we'll be making \n",
    "    #predictions only after we've seen all of the supplied angle slices.    \n",
    "    new_model_bottom = Model(BASE_MODEL.input,BASE_MODEL.output)\n",
    "    sequenced_model = TimeDistributed(new_model_bottom)(input_scan)\n",
    "    \n",
    "    \n",
    "    #One lstm layer for now\n",
    "    out = Average()(sequenced_model)\n",
    "\n",
    "    #Treat output of LSTM as individual guessess for each of the 17 zones\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    #complete model\n",
    "    recurrent_model = Model(input_scan, out)\n",
    "   \n",
    "    #optimizer\n",
    "    adam = Adam(lr,beta_1=beta1,beta_2=beta2)\n",
    "    print(\"Compiling model...\")\n",
    "    recurrent_model.compile(optimizer=adam,\n",
    "                              metrics=[metrics.binary_accuracy],\n",
    "                             loss= 'binary_crossentropy')\n",
    "\n",
    "    #Tensorboard\n",
    "    x = datetime.today()\n",
    "    stamp = \"{}-{}-{}_{}:{}:{}_lr-{}_beta1-{}_beta2-{}_lstmDim-{}_ANG-{}_{}\".format(x.year,x.month,\n",
    "                                                         x.day,x.hour,x.minute,\n",
    "                                                         x.second,lr,beta1,beta2,lstm_dim,ANGLES,description)\n",
    "    tensorboard = TensorBoard(log_dir=\"logs/{}\".format(stamp),histogram_freq=0,batch_size=BATCH_SIZE,\n",
    "                              write_grads=False,write_images=False)\n",
    "\n",
    "    #Generators and fit\n",
    "    print(\"Initializing generators...\")\n",
    "    #Bucket with clean data\n",
    "    UPLOAD_BUCKET = 'cleandhsdata' #bucket where clean data was stored\n",
    "    key_id, secret_key = hfuncs.GetAWSCredentials()\n",
    "    client = hfuncs.GetAWSClient(key_id,secret_key)\n",
    "    bucket = client.Bucket(UPLOAD_BUCKET)\n",
    "\n",
    "    #Initialize train sequencer\n",
    "    mode =\"train_scan\"\n",
    "    num_batches = (sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1)//BATCH_SIZE #train,test,val root directories have their own keys\n",
    "    train_seq = ScanSequencer(num_batches,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "    #Initialize validation sequencer\n",
    "    mode = \"val_scan\"\n",
    "    num_batches = (sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1)//BATCH_SIZE #train,test,val root directories have their own keys\n",
    "    val_seq = ScanSequencer(num_batches,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "    #Early stopping callback\n",
    "    estop = EarlyStopping(monitor='eval_loss',min_delta=0.005,patience=4)\n",
    "    print(\"Beginning training...\")\n",
    "    hist = recurrent_model.fit_generator(train_seq,\n",
    "                                           steps_per_epoch=660//5,\n",
    "                                           epochs=100,\n",
    "                                           validation_data = val_seq,\n",
    "                                           validation_steps = 200//BATCH_SIZE,\n",
    "                                           callbacks=[tensorboard,estop],\n",
    "                                          use_multiprocessing =True,workers=1)\n",
    "    return hist,recurrent_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from keras.layers import Input,Flatten,Dense,Concatenate,Dropout,concatenate,Activation\n",
    "from keras.models import Model\n",
    "from datetime import datetime\n",
    "from keras.callbacks import TensorBoard,EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras import metrics\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras import backend as K\n",
    "\n",
    "K.set_learning_phase(1)#Due to some keras bug, flag has to be constant. It works when I use dropout though?\n",
    "\n",
    "def trainCNNResRNNAdamSimple(lr,beta1,beta2,lstm_dim,description=\"RNN_simple\"):\n",
    "    #Build new sequential model,Removing dense layers from model\n",
    "    print(\"Creating model...\")\n",
    "    input_scan = Input(shape=(ANGLES,FINAL_WIDTH,FINAL_HEIGHT,CHANNELS))\n",
    "    \n",
    "    #From our base model, we'll remove 18 extraneous layers (one concatenation and 17 dense) since these\n",
    "    #were only used for making predictions for each of the zones using the extracted features.  Now, we'll be making \n",
    "    #predictions only after we've seen all of the supplied angle slices.    \n",
    "    new_model_bottom = Model(BASE_MODEL.input,BASE_MODEL.output)\n",
    "    sequenced_model = TimeDistributed(new_model_bottom)(input_scan)\n",
    "\n",
    "    #One lstm layer for now\n",
    "    lstm = LSTM(lstm_dim,recurrent_dropout=0)(sequenced_model)\n",
    "\n",
    "    #Treat output of LSTM as individual guessess for each of the 17 zones\n",
    "    \n",
    "\n",
    "    out = Activation('sigmoid')(lstm)\n",
    "\n",
    "    #complete model\n",
    "    recurrent_model = Model(input_scan, out)\n",
    "   \n",
    "    #optimizer\n",
    "    adam = Adam(lr,beta_1=beta1,beta_2=beta2)\n",
    "    print(\"Compiling model...\")\n",
    "    recurrent_model.compile(optimizer=adam,\n",
    "                              metrics=[metrics.binary_accuracy],\n",
    "                             loss= 'binary_crossentropy')\n",
    "\n",
    "    #Tensorboard\n",
    "    x = datetime.today()\n",
    "    stamp = \"{}-{}-{}_{}:{}:{}_lr-{}_beta1-{}_beta2-{}_lstmDim-{}_ANG-{}_{}\".format(x.year,x.month,\n",
    "                                                         x.day,x.hour,x.minute,\n",
    "                                                         x.second,lr,beta1,beta2,lstm_dim,ANGLES,description)\n",
    "    tensorboard = TensorBoard(log_dir=\"logs/{}\".format(stamp),histogram_freq=0,batch_size=BATCH_SIZE,\n",
    "                              write_grads=False,write_images=False)\n",
    "\n",
    "    #Generators and fit\n",
    "    print(\"Initializing generators...\")\n",
    "    #Bucket with clean data\n",
    "    UPLOAD_BUCKET = 'cleandhsdata' #bucket where clean data was stored\n",
    "    key_id, secret_key = hfuncs.GetAWSCredentials()\n",
    "    client = hfuncs.GetAWSClient(key_id,secret_key)\n",
    "    bucket = client.Bucket(UPLOAD_BUCKET)\n",
    "\n",
    "    #Initialize train sequencer\n",
    "    mode =\"train_scan\"\n",
    "    num_batches = (sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1)//BATCH_SIZE #train,test,val root directories have their own keys\n",
    "    train_seq = ScanSequencer(num_batches,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "    #Initialize validation sequencer\n",
    "    mode = \"val_scan\"\n",
    "    num_batches = (sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1)//BATCH_SIZE #train,test,val root directories have their own keys\n",
    "    val_seq = ScanSequencer(num_batches,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "    #Early stopping callback\n",
    "    estop = EarlyStopping(monitor='eval_loss',min_delta=0.005,patience=4)\n",
    "    print(\"Beginning training...\")\n",
    "    hist = recurrent_model.fit_generator(train_seq,\n",
    "                                           steps_per_epoch=660//5,\n",
    "                                           epochs=100,\n",
    "                                           validation_data = val_seq,\n",
    "                                           validation_steps = 200//BATCH_SIZE,\n",
    "                                           callbacks=[tensorboard,estop],\n",
    "                                          use_multiprocessing =True,workers=1)\n",
    "    return hist,recurrent_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from keras.layers import Input,Flatten,Dense,Concatenate,Dropout,concatenate\n",
    "from keras.models import Model\n",
    "from datetime import datetime\n",
    "from keras.callbacks import TensorBoard,EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras import metrics\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import LSTM\n",
    "K.set_learning_phase(1)#Due to some keras bug, flag has to be constant. It works when I use dropout though?\n",
    "def trainCNNResRNNAdamComplexIsh(lr,beta1,beta2,lstm_dim,description=\"RNN_complexISH\"):\n",
    "    #Build new sequential model,Removing dense layers from model\n",
    "    print(\"Creating model...\")\n",
    "    input_scan = Input(shape=(ANGLES,FINAL_WIDTH,FINAL_HEIGHT,CHANNELS))\n",
    "    \n",
    "    #From our base model, we'll remove 18 extraneous layers (one concatenation and 17 dense) since these\n",
    "    #were only used for making predictions for each of the zones using the extracted features.  Now, we'll be making \n",
    "    #predictions only after we've seen all of the supplied angle slices.\n",
    "    new_model_bottom = Model(BASE_MODEL.input,BASE_MODEL.output)\n",
    "    \n",
    "    sequenced_model = TimeDistributed(new_model_bottom)(input_scan)\n",
    "\n",
    "    #One lstm layer for now\n",
    "    lstm = LSTM(lstm_dim)(sequenced_model)\n",
    "\n",
    "    #Finally, 17 dense layers connected to the output\n",
    "    output_nodes = []\n",
    "    for i in range(ZONES):\n",
    "        output_nodes.append(Dense(1,activation='sigmoid')(lstm))\n",
    "\n",
    "    out = concatenate(output_nodes)\n",
    "\n",
    "    #complete model\n",
    "    recurrent_model = Model(input_scan, out)\n",
    "   \n",
    "    #optimizer\n",
    "    adam = Adam(lr,beta_1=beta1,beta_2=beta2)\n",
    "    print(\"Compiling model...\")\n",
    "    recurrent_model.compile(optimizer=adam,\n",
    "                              metrics=[metrics.binary_accuracy],\n",
    "                             loss= 'binary_crossentropy')\n",
    "\n",
    "    #Tensorboard\n",
    "    x = datetime.today()\n",
    "    stamp = \"{}-{}-{}_{}:{}:{}_lr-{}_beta1-{}_beta2-{}_lstmDim-{}_ANG-{}_{}\".format(x.year,x.month,\n",
    "                                                         x.day,x.hour,x.minute,\n",
    "                                                         x.second,lr,beta1,beta2,lstm_dim,ANGLES,description)\n",
    "    tensorboard = TensorBoard(log_dir=\"logs/{}\".format(stamp),histogram_freq=0,batch_size=BATCH_SIZE,\n",
    "                              write_grads=False,write_images=False)\n",
    "\n",
    "    #Generators and fit\n",
    "    print(\"Initializing generators...\")\n",
    "    #Bucket with clean data\n",
    "    UPLOAD_BUCKET = 'cleandhsdata' #bucket where clean data was stored\n",
    "    key_id, secret_key = hfuncs.GetAWSCredentials()\n",
    "    client = hfuncs.GetAWSClient(key_id,secret_key)\n",
    "    bucket = client.Bucket(UPLOAD_BUCKET)\n",
    "\n",
    "    #Initialize train sequencer\n",
    "    mode =\"train_scan\"\n",
    "    num_batches = sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1 #train,test,val root directories have their own keys\n",
    "    train_seq = ScanSequencer(num_batches,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "    #Initialize validation sequencer\n",
    "    mode = \"val_scan\"\n",
    "    num_batches = sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1 #train,test,val root directories have their own keys\n",
    "    val_seq = ScanSequencer(num_batches,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "    #Early stopping callback\n",
    "    estop = EarlyStopping(monitor='val_loss',min_delta=0.005,patience=4)\n",
    "    print(\"Beginning training...\")\n",
    "    hist = recurrent_model.fit_generator(train_seq,\n",
    "                                           steps_per_epoch=660//BATCH_SIZE,\n",
    "                                           epochs=50,\n",
    "                                           validation_data = val_seq,\n",
    "                                           validation_steps = 200//BATCH_SIZE,\n",
    "                                           callbacks=[tensorboard,estop],\n",
    "                                          use_multiprocessing =False,workers=1)\n",
    "    return hist,recurrent_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's try training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model...\n",
      "Compiling model...\n",
      "Initializing generators...\n",
      "Beginning training...\n",
      "Epoch 1/100\n",
      "444/445 [============================>.] - ETA: 2s - loss: 0.7719 - binary_accuracy: 0.5142 - binary_crossentropy: 0.7351Epoch 00000: val_binary_crossentropy improved from inf to 0.76446, saving model to check_points/2017_9_26_15_ScanModel_00-0.80.hdf5\n",
      "445/445 [==============================] - 1415s - loss: 0.7727 - binary_accuracy: 0.5140 - binary_crossentropy: 0.7353 - val_loss: 0.8007 - val_binary_accuracy: 0.4706 - val_binary_crossentropy: 0.7645\n",
      "Epoch 2/100\n",
      "444/445 [============================>.] - ETA: 2s - loss: 0.7618 - binary_accuracy: 0.5202 - binary_crossentropy: 0.7435Epoch 00001: val_binary_crossentropy improved from 0.76446 to 0.66730, saving model to check_points/2017_9_26_15_ScanModel_01-0.72.hdf5\n",
      "445/445 [==============================] - 1379s - loss: 0.7621 - binary_accuracy: 0.5205 - binary_crossentropy: 0.7432 - val_loss: 0.7210 - val_binary_accuracy: 0.5289 - val_binary_crossentropy: 0.6673\n",
      "Epoch 3/100\n",
      "444/445 [============================>.] - ETA: 2s - loss: 0.7539 - binary_accuracy: 0.5284 - binary_crossentropy: 0.7378Epoch 00002: val_binary_crossentropy did not improve\n",
      "445/445 [==============================] - 1358s - loss: 0.7531 - binary_accuracy: 0.5288 - binary_crossentropy: 0.7376 - val_loss: 0.7229 - val_binary_accuracy: 0.6633 - val_binary_crossentropy: 0.6859\n",
      "Epoch 4/100\n",
      "444/445 [============================>.] - ETA: 2s - loss: 0.7523 - binary_accuracy: 0.5244 - binary_crossentropy: 0.7373Epoch 00003: val_binary_crossentropy did not improve\n",
      "\n",
      "Epoch 00003: reducing learning rate to 0.0004000000189989805.\n",
      "445/445 [==============================] - 1381s - loss: 0.7524 - binary_accuracy: 0.5243 - binary_crossentropy: 0.7376 - val_loss: 0.7663 - val_binary_accuracy: 0.4386 - val_binary_crossentropy: 0.8306\n",
      "Epoch 5/100\n",
      "119/445 [=======>......................] - ETA: 716s - loss: 0.6977 - binary_accuracy: 0.5492 - binary_crossentropy: 0.6904"
     ]
    }
   ],
   "source": [
    "hist,recurrent_model = trainCNNResRNNAdamComplex(lr=0.002,beta1=0.9,beta2=0.99,lstm_dim=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(os.system(\"aws ec2 stop-instances --instance-ids i-0172c75d2de9bad71\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    hist,recurrent_model = trainCNNResRNNAdamComplex(lr=0.0001,beta1=0.9,beta2=0.99,lstm_dim=800)\n",
    "    del hist,recurrent_model\n",
    "except:\n",
    "    print(\"SOmething went wrong\")\n",
    "try:\n",
    "    hist,recurrent_model = trainCNNResAdamAverage(lr=0.0001,beta1=0.9,beta2=0.99,lstm_dim=0)\n",
    "    del hist,recurrent_model\n",
    "except:\n",
    "    print(\"Something went wrong\")                                                  \n",
    "try:\n",
    "    hist,recurrent_model = trainCNNResRNNAdamSimple(lr=0.0001,beta1=0.9,beta2=0.999,lstm_dim=17)\n",
    "    del hist,recurrent_model\n",
    "except:\n",
    "    print(\"Something went wrong\") \n",
    "try:\n",
    "    hist,recurrent_model = trainCNNResRNNAdamComplexIsh(lr=0.0001,beta1=0.9,beta2=0.999,lstm_dim=50)\n",
    "    del hist,recurrent_model\n",
    "except:\n",
    "    print(\"Something went wrong\") \n",
    "                                                  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
