{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to first try training a CNN on the individual images.\n",
    "We will be using binary cross entropy across the 17 regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import HelperFuncs as hfuncs\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.data_utils import Sequence\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "FINAL_WIDTH = 400\n",
    "FINAL_HEIGHT = 600\n",
    "CHANNELS = 1\n",
    "ZONES = 17\n",
    "ANGLES = 4 #Just using 4 angles here\n",
    "#Create directories for sequencer function if they don't exist\n",
    "for d in ['temp/train_scan/','temp/test_scan/','temp/val_scan/']:\n",
    "    if not os.path.isdir(d):\n",
    "        print(\"Created directory: {}\".format(d))\n",
    "        os.makedirs(d)\n",
    "        \n",
    "class LegScanSequencer(Sequence):\n",
    "    idx_dict={}\n",
    "    \n",
    "    def __init__(self,num_batches,bucket_name,mode=\"train_scan\"):\n",
    "        self.num_batches = num_batches\n",
    "        self.bucket_name = bucket_name\n",
    "        self.mode = mode\n",
    "        self.key_id, self.secret_key = hfuncs.GetAWSCredentials()\n",
    "        self.mode = mode\n",
    "        self.angles = np.arange(0,64,64//ANGLES)\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "    def on_epoch_end(self):\n",
    "        pass\n",
    "    def __getitem__(self,idx):\n",
    "        #Get Client\n",
    "        client = hfuncs.GetAWSClient(self.key_id,self.secret_key)\n",
    "        bucket = client.Bucket(self.bucket_name)\n",
    "        \n",
    "        #Initialize vectors\n",
    "        X_train = np.zeros((BATCH_SIZE,ANGLES,FINAL_WIDTH,FINAL_HEIGHT//2,CHANNELS))\n",
    "        y_train = np.zeros((BATCH_SIZE,1))\n",
    "        \n",
    "        j=0\n",
    "        for i in range(idx*BATCH_SIZE,(idx+1)*BATCH_SIZE):\n",
    "            #Download batch at index\n",
    "            path = \"temp/{}/batch_{}.hdf5\".format(self.mode,i)\n",
    "            key = \"{}/batch_{}.hdf5\".format(self.mode,i)\n",
    "            bucket.download_file(Key=key,Filename=path)\n",
    "        \n",
    "            f = h5py.File(path,\"r\")\n",
    "            try:\n",
    "                X_train[j,:,:,:,:] = f['/image'].value[self.angles,:,:FINAL_HEIGHT//2,:]\n",
    "                r_leg = [12,14,13,15]\n",
    "                l_leg = [13,15]\n",
    "                r_y = np.amax(f['/labels'].value[r_leg])\n",
    "                #l_y = np.amax(f['/labels'].value[l_leg])\n",
    "                y_train[j,:] = r_y\n",
    "                j += 1\n",
    "            finally:\n",
    "                f.close()\n",
    "                os.remove(path) \n",
    "        return X_train, y_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from twilio.rest import Client\n",
    "import configparser\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "class SMSNotifier(Callback):\n",
    "    def on_epoch_end(self,epoch,logs=None):\n",
    "        #Execute every other epoch\n",
    "        if epoch % 2 == 0:\n",
    "            #Get config credentials\n",
    "            config = configparser.ConfigParser()\n",
    "            config.read('twilio.conf')\n",
    "            account_sid = config['DEFAULT']['AccountID']\n",
    "            auth_token = config['DEFAULT']['AuthToken']\n",
    "            #Get client\n",
    "            client = Client(account_sid, auth_token)\n",
    "            #Create message\n",
    "            if logs is not None:\n",
    "                message = \"Layer {} complete.\".format(epoch)\n",
    "            else:\n",
    "                message = \"Layer {} complete.\".format(epoch)\n",
    "            #Sendmessage\n",
    "            message = client.messages.create(\n",
    "                to=\"+16178884129\", \n",
    "                from_=\"+18572142288\",\n",
    "                body=message)\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Build pre-trained V2 model\n",
    "import numpy as np\n",
    "from keras.layers import Input,Flatten,Dense,Concatenate,Dropout,concatenate,GlobalMaxPool2D\n",
    "from keras.models import Model\n",
    "from datetime import datetime\n",
    "from keras.callbacks import TensorBoard,EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import binary_accuracy\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.applications.inception_v3 import InceptionV3,preprocess_input\n",
    "from keras.layers.core import Lambda\n",
    "import tensorflow as tf\n",
    "\n",
    "def ToRGB(x):\n",
    "    max_v = tf.reduce_max(x)\n",
    "    min_v = tf.reduce_min(x)\n",
    "    max_rgb = tf.constant(255,dtype=x.dtype)\n",
    "    x = tf.floordiv(tf.multiply(tf.subtract(x,min_v),max_rgb),tf.subtract(max_v,min_v))\n",
    "    return x\n",
    "def ToGreyScale(x):\n",
    "    #Divide RGB into 3\n",
    "    scalar = tf.constant(3,dtype=x.dtype)\n",
    "    x = tf.floordiv(x,scalar)\n",
    "    shape = x.get_shape()\n",
    "    #assume channel_last\n",
    "    mult = [[1 for d in shape[:-1]],[3]]\n",
    "    mult = [val for sublist in mult for val in sublist]\n",
    "    return tf.tile(x,mult)\n",
    "def ToNewShape(x):\n",
    "    ndim = len(x.shape)\n",
    "    if ndim == 5:\n",
    "        return tf.reverse(tf.transpose(x,[0,1,3,2,4]),[-3])\n",
    "    elif ndim == 4:\n",
    "        return tf.reverse(tf.transpose(x,[0,2,1,3]),[-3])\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected number of dims!\")\n",
    "\n",
    "def getSingleLegModel(layer_idx):\n",
    "    #Single model image\n",
    "    input_img = Input(shape=(FINAL_WIDTH,FINAL_HEIGHT//2,CHANNELS))\n",
    "    \n",
    "    #preprocess and extract channels\n",
    "    input_img_pp = Lambda(ToRGB)(input_img)\n",
    "    input_img_pp = Lambda(ToGreyScale)(input_img_pp)\n",
    "    input_img_pp = Lambda(preprocess_input)(input_img_pp)\n",
    "    input_img_pp = Lambda(ToNewShape)(input_img_pp)\n",
    "    \n",
    "    #Load resnet\n",
    "    incep = InceptionV3(include_top=False,\n",
    "                          weights='imagenet',\n",
    "                          input_tensor=None,\n",
    "                          input_shape=(FINAL_HEIGHT//2,FINAL_WIDTH,3),\n",
    "                          pooling='max')\n",
    "    for l in incep.layers:\n",
    "        l.trainable=False\n",
    "\n",
    "    #Take off top\n",
    "    reduced_net = Model(incep.input,incep.get_layer('mixed{}'.format(layer_idx)).output)\n",
    "    \n",
    "    #Add to rest of the model\n",
    "    output = reduced_net(input_img_pp)\n",
    "    output = Flatten()(output)\n",
    "    intermediate_model = Model(input_img,output)\n",
    "    \n",
    "    #Time distributed model\n",
    "    input_scan = Input(shape=(ANGLES,FINAL_WIDTH,FINAL_HEIGHT//2,CHANNELS))  \n",
    "    sequenced_model = TimeDistributed(intermediate_model)(input_scan)\n",
    "  \n",
    "    #One lstm layer for now\n",
    "    #lstm = LSTM(lstm_dim,recurrent_dropout=0.10)(sequenced_model)\n",
    "    \n",
    "    #Finally, 1 dense layers\n",
    "    #out = Dense(1,activation='sigmoid',use_bias=False)(lstm)\n",
    "    #complete model\n",
    "    try:\n",
    "        return Model(input_scan,sequenced_model)#Model(input_scan, out)\n",
    "    finally:\n",
    "        del intermediate_model,sequenced_model,incep\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total=687,pos=126\n"
     ]
    }
   ],
   "source": [
    "#Test how many positive samples\n",
    "import pickle\n",
    "labels = hfuncs.GetLabelsDict(r'stage1_labels.csv')\n",
    "filename = \"data_separated.pickle\"\n",
    "with open(filename,\"rb\") as f:\n",
    "   save = pickle.load(f)\n",
    "   K_test= save['K_test']\n",
    "   K_val = save['K_val']\n",
    "   K_train = save['K_train']\n",
    "s = 0\n",
    "pos = 0\n",
    "for k in K_train:\n",
    "    k_clean = k.replace(\"DHSData/\",\"\").replace(\".a3daps\",\"\")\n",
    "    if k_clean in labels.keys():\n",
    "        label = np.array(labels[k_clean])\n",
    "        val = np.amax(label[[12,14]])\n",
    "        if val == 1:\n",
    "            s += 1\n",
    "            pos += 1\n",
    "        else:\n",
    "            s += 1\n",
    "print(\"total={},pos={}\".format(s,pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Use model as a feature extractor and use traditional ML to sdeterine whther features have any predictive power\n",
    "import h5py\n",
    "from keras import backend as K\n",
    "K.set_learning_phase(0)\n",
    "\n",
    "#Bucket with clean data\n",
    "UPLOAD_BUCKET = 'cleandhsdata' #bucket where clean data was stored\n",
    "TEMP_DIR = 'temp' #Directory for file upload/downloads\n",
    "key_id, secret_key = hfuncs.GetAWSCredentials()\n",
    "client = hfuncs.GetAWSClient(key_id,secret_key)\n",
    "bucket = client.Bucket(UPLOAD_BUCKET)\n",
    "\n",
    "#Initialize train sequencer\n",
    "mode =\"train_scan\"\n",
    "num_batches_train = (sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1)//BATCH_SIZE #train,test,val root directories have their own keys\n",
    "train_seq = LegScanSequencer(num_batches_train,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "#Initialize validation sequencer\n",
    "mode = \"val_scan\"\n",
    "num_batches_val = (sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1)//BATCH_SIZE #train,test,val root directories have their own keys\n",
    "val_seq = LegScanSequencer(num_batches_val,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "#Create notifier\n",
    "notify = SMSNotifier()\n",
    "\n",
    "#Create function that creates data set for given layer\n",
    "def CreateFeatureDataSet(layer_idx,dir_name = 'featureextraction',max_batches=800):\n",
    "    #Get model and output size\n",
    "    model = getSingleLegModel(layer_idx)\n",
    "    output_size = model.output_shape[1]*model.output_shape[2]\n",
    "    \n",
    "    #Variables to iterate over\n",
    "    #modes = ['train','val']\n",
    "    #num_batches = [num_batches_train,num_batches_val]\n",
    "    #generators = [train_seq,val_seq]\n",
    "    modes = ['train']\n",
    "    num_batches = [num_batches_train]\n",
    "    generators = [train_seq]\n",
    "    \n",
    "    \n",
    "    for mode,num_b,gen in zip(modes,num_batches,generators):\n",
    "        #Initialize dataset array\n",
    "        X_d = np.zeros((min(num_b,max_batches),output_size))\n",
    "        y_d = np.zeros((min(num_b,max_batches)))\n",
    "\n",
    "        #For every item in train generator, transform data and store in dataset array\n",
    "        for i in range(min(num_b,max_batches)):\n",
    "            print(\"Storing {} in {} set...\".format(i,mode))\n",
    "            X, y = gen.__getitem__(i)\n",
    "            X = model.predict(X)\n",
    "            X_d[i,:] = X.flatten()\n",
    "            y_d[i] = y[0,0]\n",
    "            i += 1\n",
    "\n",
    "        #Store data set in s3\n",
    "        key_suffix = \"{}_layer{}.hdf5\".format(mode,layer_idx)\n",
    "        filename = os.path.join(TEMP_DIR,key_suffix)\n",
    "        key = \"{}/{}\".format(dir_name,key_suffix)\n",
    "\n",
    "        #Save in local hdf5 file\n",
    "        with h5py.File(filename,\"w\") as f:\n",
    "            dset = f.create_dataset('features',data=X_d)\n",
    "            dset2 = f.create_dataset('labels',data=y_d)\n",
    "\n",
    "        #Upload file to bucket, then delete\n",
    "        try:\n",
    "            bucket.upload_file(Filename=filename,Key=key)\n",
    "            print(\"Completed {} upload for layer {}\".format(mode,layer_idx))\n",
    "        finally:\n",
    "            os.remove(filename)\n",
    "\n",
    "        #Delete train arrays to save memory\n",
    "        del X_d,y_d\n",
    "    \n",
    "    #Send notification that job was completed\n",
    "    try:\n",
    "        notify.on_epoch_end(layer_idx)\n",
    "    except:\n",
    "        print(\"Couldn't send notification!\")\n",
    "        \n",
    "#for l in range(11):\n",
    " #   try:\n",
    " #       CreateFeatureDataSet(l)\n",
    " #   except:\n",
    "   #     print(\"Failed to create feature set {}\".format(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For every layer in the data set, we're going to train a gardient booster classifier \n",
    "#using hyperparameters we selected from before\n",
    "import gc\n",
    "import sys\n",
    "import os\n",
    "\n",
    "xgboost_dir = '/home/ubuntu/xgboost/python-package'\n",
    "if xgboost_dir not in sys.path:\n",
    "    sys.path.append(xgboost_dir)\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "import pickle\n",
    "\n",
    "def getScores(layer_idx,dir_name = 'featureextraction'):\n",
    "    try:\n",
    "        #Classifier\n",
    "        clf = XGBClassifier(min_samples_split=3,max_depth=5,learning_rate=0.1,n_estimators=100,silent=False,njobs=2)\n",
    "\n",
    "        #Grab data\n",
    "        #Download dataset\n",
    "        print(\"Downloading dataset for layer {}...\".format(layer_idx))\n",
    "        mode = 'train'\n",
    "        key_suffix = \"{}_layer{}.hdf5\".format(mode,layer_idx)\n",
    "        filename = os.path.join(TEMP_DIR,key_suffix)\n",
    "        key = \"{}/{}\".format(dir_name,key_suffix)\n",
    "\n",
    "        if not os.path.exists(filename):\n",
    "            bucket.download_file(Key=key,Filename=filename)\n",
    "\n",
    "        #Open downloaded file and load data\n",
    "        with h5py.File(filename,\"r\") as f:    \n",
    "            #Get train data\n",
    "            print(\"Reading data...\")\n",
    "            d = f['/features']\n",
    "            X_train = np.zeros((700,d.shape[1]))\n",
    "            d.read_direct(X_train,np.s_[:700],np.s_[:700])   \n",
    "            d = f['/labels']\n",
    "            y_train = np.zeros(700)\n",
    "            d.read_direct(y_train,np.s_[:700],np.s_[:700])\n",
    "\n",
    "            #Val data\n",
    "            X_test = np.zeros((100,f['/features'].shape[1]))\n",
    "            f['/features'].read_direct(X_test,np.s_[700:800],np.s_[:100])\n",
    "            y_test = np.zeros(100)\n",
    "            f['/labels'].read_direct(y_test,np.s_[700:800],np.s_[:100])\n",
    "\n",
    "        #Fit model    \n",
    "        print(\"Fitting model...\")\n",
    "        clf.fit(X_train,y_train,eval_set = [(X_test,y_test)],\n",
    "                eval_metric = \"logloss\",\n",
    "                early_stopping_rounds = 10,\n",
    "                verbose=True )\n",
    "\n",
    "        #Val score\n",
    "        results = clf.evals_result()       \n",
    "\n",
    "        #Pickle results\n",
    "        out_file = \"results_{}\".format(layer_idx)\n",
    "        result_dict = {'results':results,'model':clf}\n",
    "        with open(out_file,\"wb\") as f:\n",
    "            pickle.dump(result_dict,f)\n",
    "    finally:\n",
    "        os.remove(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training layer 0\n",
      "Downloading dataset for layer 0...\n",
      "Reading data...\n",
      "Fitting model...\n",
      "[0]\tvalidation_0-logloss:0.673841\n",
      "Will train until validation_0-logloss hasn't improved in 10 rounds.\n",
      "[1]\tvalidation_0-logloss:0.667271\n",
      "[2]\tvalidation_0-logloss:0.666667\n",
      "[3]\tvalidation_0-logloss:0.647717\n",
      "[4]\tvalidation_0-logloss:0.646411\n",
      "[5]\tvalidation_0-logloss:0.629494\n",
      "[6]\tvalidation_0-logloss:0.622249\n",
      "[7]\tvalidation_0-logloss:0.622563\n",
      "[8]\tvalidation_0-logloss:0.621171\n",
      "[9]\tvalidation_0-logloss:0.622467\n",
      "[10]\tvalidation_0-logloss:0.613897\n",
      "[11]\tvalidation_0-logloss:0.608803\n",
      "[12]\tvalidation_0-logloss:0.610813\n",
      "[13]\tvalidation_0-logloss:0.605142\n",
      "[14]\tvalidation_0-logloss:0.599064\n",
      "[15]\tvalidation_0-logloss:0.598248\n",
      "[16]\tvalidation_0-logloss:0.601616\n",
      "[17]\tvalidation_0-logloss:0.600945\n",
      "[18]\tvalidation_0-logloss:0.602762\n",
      "[19]\tvalidation_0-logloss:0.601178\n",
      "[20]\tvalidation_0-logloss:0.599841\n",
      "[21]\tvalidation_0-logloss:0.603794\n",
      "[22]\tvalidation_0-logloss:0.60051\n",
      "[23]\tvalidation_0-logloss:0.596205\n",
      "[24]\tvalidation_0-logloss:0.596854\n",
      "[25]\tvalidation_0-logloss:0.602278\n",
      "[26]\tvalidation_0-logloss:0.60019\n",
      "[27]\tvalidation_0-logloss:0.598034\n",
      "[28]\tvalidation_0-logloss:0.60247\n",
      "[29]\tvalidation_0-logloss:0.602623\n",
      "[30]\tvalidation_0-logloss:0.605619\n",
      "[31]\tvalidation_0-logloss:0.613045\n",
      "[32]\tvalidation_0-logloss:0.613194\n",
      "[33]\tvalidation_0-logloss:0.615043\n",
      "Stopping. Best iteration:\n",
      "[23]\tvalidation_0-logloss:0.596205\n",
      "\n",
      "Training layer 1\n",
      "Downloading dataset for layer 1...\n",
      "Reading data...\n",
      "Fitting model...\n",
      "[0]\tvalidation_0-logloss:0.680203\n",
      "Will train until validation_0-logloss hasn't improved in 10 rounds.\n",
      "[1]\tvalidation_0-logloss:0.666005\n",
      "[2]\tvalidation_0-logloss:0.651801\n",
      "[3]\tvalidation_0-logloss:0.64685\n",
      "[4]\tvalidation_0-logloss:0.6309\n",
      "[5]\tvalidation_0-logloss:0.623016\n",
      "[6]\tvalidation_0-logloss:0.614838\n",
      "[7]\tvalidation_0-logloss:0.610561\n",
      "[8]\tvalidation_0-logloss:0.599902\n",
      "[9]\tvalidation_0-logloss:0.602247\n",
      "[10]\tvalidation_0-logloss:0.604532\n",
      "[11]\tvalidation_0-logloss:0.595866\n",
      "[12]\tvalidation_0-logloss:0.595666\n",
      "[13]\tvalidation_0-logloss:0.592865\n",
      "[14]\tvalidation_0-logloss:0.596719\n",
      "[15]\tvalidation_0-logloss:0.599049\n",
      "[16]\tvalidation_0-logloss:0.601155\n",
      "[17]\tvalidation_0-logloss:0.592971\n",
      "[18]\tvalidation_0-logloss:0.591771\n",
      "[19]\tvalidation_0-logloss:0.592168\n",
      "[20]\tvalidation_0-logloss:0.59564\n",
      "[21]\tvalidation_0-logloss:0.595239\n",
      "[22]\tvalidation_0-logloss:0.599191\n",
      "[23]\tvalidation_0-logloss:0.594274\n",
      "[24]\tvalidation_0-logloss:0.589918\n",
      "[25]\tvalidation_0-logloss:0.587664\n",
      "[26]\tvalidation_0-logloss:0.584793\n",
      "[27]\tvalidation_0-logloss:0.59026\n",
      "[28]\tvalidation_0-logloss:0.589267\n",
      "[29]\tvalidation_0-logloss:0.594676\n",
      "[30]\tvalidation_0-logloss:0.59515\n",
      "[31]\tvalidation_0-logloss:0.595608\n",
      "[32]\tvalidation_0-logloss:0.596325\n",
      "[33]\tvalidation_0-logloss:0.60153\n",
      "[34]\tvalidation_0-logloss:0.602323\n",
      "[35]\tvalidation_0-logloss:0.602109\n",
      "[36]\tvalidation_0-logloss:0.604312\n",
      "Stopping. Best iteration:\n",
      "[26]\tvalidation_0-logloss:0.584793\n",
      "\n",
      "Training layer 2\n",
      "Downloading dataset for layer 2...\n",
      "Reading data...\n",
      "Fitting model...\n",
      "[0]\tvalidation_0-logloss:0.694042\n",
      "Will train until validation_0-logloss hasn't improved in 10 rounds.\n",
      "[1]\tvalidation_0-logloss:0.696385\n",
      "[2]\tvalidation_0-logloss:0.683939\n",
      "[3]\tvalidation_0-logloss:0.676803\n",
      "[4]\tvalidation_0-logloss:0.664616\n",
      "[5]\tvalidation_0-logloss:0.672672\n",
      "[6]\tvalidation_0-logloss:0.668547\n",
      "[7]\tvalidation_0-logloss:0.65599\n",
      "[8]\tvalidation_0-logloss:0.643778\n",
      "[9]\tvalidation_0-logloss:0.63968\n",
      "[10]\tvalidation_0-logloss:0.643114\n",
      "[11]\tvalidation_0-logloss:0.633838\n",
      "[12]\tvalidation_0-logloss:0.626208\n",
      "[13]\tvalidation_0-logloss:0.622827\n",
      "[14]\tvalidation_0-logloss:0.622949\n",
      "[15]\tvalidation_0-logloss:0.618846\n",
      "[16]\tvalidation_0-logloss:0.61803\n",
      "[17]\tvalidation_0-logloss:0.619803\n",
      "[18]\tvalidation_0-logloss:0.621943\n",
      "[19]\tvalidation_0-logloss:0.621276\n",
      "[20]\tvalidation_0-logloss:0.625359\n",
      "[21]\tvalidation_0-logloss:0.622342\n",
      "[22]\tvalidation_0-logloss:0.618241\n",
      "[23]\tvalidation_0-logloss:0.618838\n",
      "[24]\tvalidation_0-logloss:0.621285\n",
      "[25]\tvalidation_0-logloss:0.619256\n",
      "[26]\tvalidation_0-logloss:0.619972\n",
      "Stopping. Best iteration:\n",
      "[16]\tvalidation_0-logloss:0.61803\n",
      "\n",
      "Training layer 3\n",
      "Downloading dataset for layer 3...\n",
      "Reading data...\n",
      "Fitting model...\n",
      "[0]\tvalidation_0-logloss:0.683799\n",
      "Will train until validation_0-logloss hasn't improved in 10 rounds.\n",
      "[1]\tvalidation_0-logloss:0.674025\n",
      "[2]\tvalidation_0-logloss:0.665328\n",
      "[3]\tvalidation_0-logloss:0.658948\n",
      "[4]\tvalidation_0-logloss:0.649321\n",
      "[5]\tvalidation_0-logloss:0.639888\n",
      "[6]\tvalidation_0-logloss:0.639284\n",
      "[7]\tvalidation_0-logloss:0.624328\n",
      "[8]\tvalidation_0-logloss:0.618207\n",
      "[9]\tvalidation_0-logloss:0.615769\n",
      "[10]\tvalidation_0-logloss:0.610807\n",
      "[11]\tvalidation_0-logloss:0.603434\n",
      "[12]\tvalidation_0-logloss:0.60283\n",
      "[13]\tvalidation_0-logloss:0.607259\n",
      "[14]\tvalidation_0-logloss:0.607827\n",
      "[15]\tvalidation_0-logloss:0.600547\n",
      "[16]\tvalidation_0-logloss:0.59415\n",
      "[17]\tvalidation_0-logloss:0.59486\n",
      "[18]\tvalidation_0-logloss:0.591185\n",
      "[19]\tvalidation_0-logloss:0.58937\n",
      "[20]\tvalidation_0-logloss:0.59136\n",
      "[21]\tvalidation_0-logloss:0.595417\n",
      "[22]\tvalidation_0-logloss:0.594419\n",
      "[23]\tvalidation_0-logloss:0.58973\n",
      "[24]\tvalidation_0-logloss:0.590915\n",
      "[25]\tvalidation_0-logloss:0.595626\n",
      "[26]\tvalidation_0-logloss:0.599955\n",
      "[27]\tvalidation_0-logloss:0.589835\n",
      "[28]\tvalidation_0-logloss:0.587645\n",
      "[29]\tvalidation_0-logloss:0.58464\n",
      "[30]\tvalidation_0-logloss:0.586174\n",
      "[31]\tvalidation_0-logloss:0.588741\n",
      "[32]\tvalidation_0-logloss:0.58624\n",
      "[33]\tvalidation_0-logloss:0.588211\n",
      "[34]\tvalidation_0-logloss:0.590648\n",
      "[35]\tvalidation_0-logloss:0.591503\n",
      "[36]\tvalidation_0-logloss:0.590544\n",
      "[37]\tvalidation_0-logloss:0.58558\n",
      "[38]\tvalidation_0-logloss:0.590145\n",
      "[39]\tvalidation_0-logloss:0.588115\n",
      "Stopping. Best iteration:\n",
      "[29]\tvalidation_0-logloss:0.58464\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for l in range(0,4):\n",
    "    gc.collect()\n",
    "    print(\"Training layer {}\".format(l))\n",
    "    getScores(l)\n",
    "    notify.on_epoch_end(l*2)\n",
    "#os.system(\"aws ec2 stop-instances --instance-ids i-0409f4210349f600d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (DSHenv_3.5)",
   "language": "python",
   "name": "dhsenv_3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
