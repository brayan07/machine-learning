{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to first try training a CNN on the individual images.\n",
    "We will be using binary cross entropy across the 17 regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import HelperFuncs as hfuncs\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.data_utils import Sequence\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "FINAL_WIDTH = 400\n",
    "FINAL_HEIGHT = 600\n",
    "CHANNELS = 1\n",
    "ZONES = 17\n",
    "ANGLES = 4 #Just using 4 angles here\n",
    "#Create directories for sequencer function if they don't exist\n",
    "for d in ['temp/train_scan/','temp/test_scan/','temp/val_scan/']:\n",
    "    if not os.path.isdir(d):\n",
    "        print(\"Created directory: {}\".format(d))\n",
    "        os.makedirs(d)\n",
    "        \n",
    "class LegScanSequencer(Sequence):\n",
    "    idx_dict={}\n",
    "    \n",
    "    def __init__(self,num_batches,bucket_name,mode=\"train_scan\"):\n",
    "        self.num_batches = num_batches\n",
    "        self.bucket_name = bucket_name\n",
    "        self.mode = mode\n",
    "        self.key_id, self.secret_key = hfuncs.GetAWSCredentials()\n",
    "        self.mode = mode\n",
    "        self.angles = np.arange(0,64,64//ANGLES)\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "    def on_epoch_end(self):\n",
    "        pass\n",
    "    def __getitem__(self,idx):\n",
    "        #Get Client\n",
    "        client = hfuncs.GetAWSClient(self.key_id,self.secret_key)\n",
    "        bucket = client.Bucket(self.bucket_name)\n",
    "        \n",
    "        #Initialize vectors\n",
    "        X_train = np.zeros((BATCH_SIZE,ANGLES,FINAL_WIDTH,FINAL_HEIGHT//2,CHANNELS))\n",
    "        y_train = np.zeros((BATCH_SIZE,1))\n",
    "        \n",
    "        j=0\n",
    "        for i in range(idx*BATCH_SIZE,(idx+1)*BATCH_SIZE):\n",
    "            #Download batch at index\n",
    "            path = \"temp/{}/batch_{}.hdf5\".format(self.mode,i)\n",
    "            key = \"{}/batch_{}.hdf5\".format(self.mode,i)\n",
    "            bucket.download_file(Key=key,Filename=path)\n",
    "        \n",
    "            f = h5py.File(path,\"r\")\n",
    "            try:\n",
    "                X_train[j,:,:,:,:] = f['/image'].value[self.angles,:,:FINAL_HEIGHT//2,:]\n",
    "                r_leg = [12,14,13,15]\n",
    "                l_leg = [13,15]\n",
    "                r_y = np.amax(f['/labels'].value[r_leg])\n",
    "                #l_y = np.amax(f['/labels'].value[l_leg])\n",
    "                y_train[j,:] = r_y\n",
    "                j += 1\n",
    "            finally:\n",
    "                f.close()\n",
    "                os.remove(path) \n",
    "        return X_train, y_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from twilio.rest import Client\n",
    "import configparser\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "class SMSNotifier(Callback):\n",
    "    def on_epoch_end(self,epoch,logs=None):\n",
    "        #Execute every other epoch\n",
    "        if epoch % 2 == 0:\n",
    "            #Get config credentials\n",
    "            config = configparser.ConfigParser()\n",
    "            config.read('twilio.conf')\n",
    "            account_sid = config['DEFAULT']['AccountID']\n",
    "            auth_token = config['DEFAULT']['AuthToken']\n",
    "            #Get client\n",
    "            client = Client(account_sid, auth_token)\n",
    "            #Create message\n",
    "            if logs is not None:\n",
    "                message = \"Layer {} complete.\".format(epoch)\n",
    "            else:\n",
    "                message = \"Layer {} complete.\".format(epoch)\n",
    "            #Sendmessage\n",
    "            message = client.messages.create(\n",
    "                to=\"+16178884129\", \n",
    "                from_=\"+18572142288\",\n",
    "                body=message)\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Build pre-trained V2 model\n",
    "import numpy as np\n",
    "from keras.layers import Input,Flatten,Dense,Concatenate,Dropout,concatenate,GlobalMaxPool2D\n",
    "from keras.models import Model\n",
    "from datetime import datetime\n",
    "from keras.callbacks import TensorBoard,EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import binary_accuracy\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.applications.inception_v3 import InceptionV3,preprocess_input\n",
    "from keras.layers.core import Lambda\n",
    "import tensorflow as tf\n",
    "\n",
    "def ToRGB(x):\n",
    "    max_v = tf.reduce_max(x)\n",
    "    min_v = tf.reduce_min(x)\n",
    "    max_rgb = tf.constant(255,dtype=x.dtype)\n",
    "    x = tf.floordiv(tf.multiply(tf.subtract(x,min_v),max_rgb),tf.subtract(max_v,min_v))\n",
    "    return x\n",
    "def ToGreyScale(x):\n",
    "    #Divide RGB into 3\n",
    "    scalar = tf.constant(3,dtype=x.dtype)\n",
    "    x = tf.floordiv(x,scalar)\n",
    "    shape = x.get_shape()\n",
    "    #assume channel_last\n",
    "    mult = [[1 for d in shape[:-1]],[3]]\n",
    "    mult = [val for sublist in mult for val in sublist]\n",
    "    return tf.tile(x,mult)\n",
    "def ToNewShape(x):\n",
    "    ndim = len(x.shape)\n",
    "    if ndim == 5:\n",
    "        return tf.reverse(tf.transpose(x,[0,1,3,2,4]),[-3])\n",
    "    elif ndim == 4:\n",
    "        return tf.reverse(tf.transpose(x,[0,2,1,3]),[-3])\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected number of dims!\")\n",
    "\n",
    "def getSingleLegModel(layer_idx):\n",
    "    #Single model image\n",
    "    input_img = Input(shape=(FINAL_WIDTH,FINAL_HEIGHT//2,CHANNELS))\n",
    "    \n",
    "    #preprocess and extract channels\n",
    "    input_img_pp = Lambda(ToRGB)(input_img)\n",
    "    input_img_pp = Lambda(ToGreyScale)(input_img_pp)\n",
    "    input_img_pp = Lambda(preprocess_input)(input_img_pp)\n",
    "    input_img_pp = Lambda(ToNewShape)(input_img_pp)\n",
    "    \n",
    "    #Load resnet\n",
    "    incep = InceptionV3(include_top=False,\n",
    "                          weights='imagenet',\n",
    "                          input_tensor=None,\n",
    "                          input_shape=(FINAL_HEIGHT//2,FINAL_WIDTH,3),\n",
    "                          pooling='max')\n",
    "    for l in incep.layers:\n",
    "        l.trainable=False\n",
    "\n",
    "    #Take off top\n",
    "    reduced_net = Model(incep.input,incep.get_layer('mixed{}'.format(layer_idx)).output)\n",
    "    \n",
    "    #Add to rest of the model\n",
    "    output = reduced_net(input_img_pp)\n",
    "    output = Flatten()(output)\n",
    "    intermediate_model = Model(input_img,output)\n",
    "    \n",
    "    #Time distributed model\n",
    "    input_scan = Input(shape=(ANGLES,FINAL_WIDTH,FINAL_HEIGHT//2,CHANNELS))  \n",
    "    sequenced_model = TimeDistributed(intermediate_model)(input_scan)\n",
    "  \n",
    "    #One lstm layer for now\n",
    "    #lstm = LSTM(lstm_dim,recurrent_dropout=0.10)(sequenced_model)\n",
    "    \n",
    "    #Finally, 1 dense layers\n",
    "    #out = Dense(1,activation='sigmoid',use_bias=False)(lstm)\n",
    "    #complete model\n",
    "    try:\n",
    "        return Model(input_scan,sequenced_model)#Model(input_scan, out)\n",
    "    finally:\n",
    "        del intermediate_model,sequenced_model,incep\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total=687,pos=126\n"
     ]
    }
   ],
   "source": [
    "#Test how many positive samples\n",
    "import pickle\n",
    "labels = hfuncs.GetLabelsDict(r'stage1_labels.csv')\n",
    "filename = \"data_separated.pickle\"\n",
    "with open(filename,\"rb\") as f:\n",
    "   save = pickle.load(f)\n",
    "   K_test= save['K_test']\n",
    "   K_val = save['K_val']\n",
    "   K_train = save['K_train']\n",
    "s = 0\n",
    "pos = 0\n",
    "for k in K_train:\n",
    "    k_clean = k.replace(\"DHSData/\",\"\").replace(\".a3daps\",\"\")\n",
    "    if k_clean in labels.keys():\n",
    "        label = np.array(labels[k_clean])\n",
    "        val = np.amax(label[[12,14]])\n",
    "        if val == 1:\n",
    "            s += 1\n",
    "            pos += 1\n",
    "        else:\n",
    "            s += 1\n",
    "print(\"total={},pos={}\".format(s,pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Use model as a feature extractor and use traditional ML to sdeterine whther features have any predictive power\n",
    "import h5py\n",
    "from keras import backend as K\n",
    "K.set_learning_phase(0)\n",
    "\n",
    "#Bucket with clean data\n",
    "UPLOAD_BUCKET = 'cleandhsdata' #bucket where clean data was stored\n",
    "TEMP_DIR = 'temp' #Directory for file upload/downloads\n",
    "key_id, secret_key = hfuncs.GetAWSCredentials()\n",
    "client = hfuncs.GetAWSClient(key_id,secret_key)\n",
    "bucket = client.Bucket(UPLOAD_BUCKET)\n",
    "\n",
    "#Initialize train sequencer\n",
    "mode =\"train_scan\"\n",
    "num_batches_train = (sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1)//BATCH_SIZE #train,test,val root directories have their own keys\n",
    "train_seq = LegScanSequencer(num_batches_train,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "#Initialize validation sequencer\n",
    "mode = \"val_scan\"\n",
    "num_batches_val = (sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1)//BATCH_SIZE #train,test,val root directories have their own keys\n",
    "val_seq = LegScanSequencer(num_batches_val,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "#Create notifier\n",
    "notify = SMSNotifier()\n",
    "\n",
    "#Create function that creates data set for given layer\n",
    "def CreateFeatureDataSet(layer_idx,dir_name = 'featureextraction',max_batches=800):\n",
    "    #Get model and output size\n",
    "    model = getSingleLegModel(layer_idx)\n",
    "    output_size = model.output_shape[1]*model.output_shape[2]\n",
    "    \n",
    "    #Variables to iterate over\n",
    "    #modes = ['train','val']\n",
    "    #num_batches = [num_batches_train,num_batches_val]\n",
    "    #generators = [train_seq,val_seq]\n",
    "    modes = ['train']\n",
    "    num_batches = [num_batches_train]\n",
    "    generators = [train_seq]\n",
    "    \n",
    "    \n",
    "    for mode,num_b,gen in zip(modes,num_batches,generators):\n",
    "        #Initialize dataset array\n",
    "        X_d = np.zeros((min(num_b,max_batches),output_size))\n",
    "        y_d = np.zeros((min(num_b,max_batches)))\n",
    "\n",
    "        #For every item in train generator, transform data and store in dataset array\n",
    "        for i in range(min(num_b,max_batches)):\n",
    "            print(\"Storing {} in {} set...\".format(i,mode))\n",
    "            X, y = gen.__getitem__(i)\n",
    "            X = model.predict(X)\n",
    "            X_d[i,:] = X.flatten()\n",
    "            y_d[i] = y[0,0]\n",
    "            i += 1\n",
    "\n",
    "        #Store data set in s3\n",
    "        key_suffix = \"{}_layer{}.hdf5\".format(mode,layer_idx)\n",
    "        filename = os.path.join(TEMP_DIR,key_suffix)\n",
    "        key = \"{}/{}\".format(dir_name,key_suffix)\n",
    "\n",
    "        #Save in local hdf5 file\n",
    "        with h5py.File(filename,\"w\") as f:\n",
    "            dset = f.create_dataset('features',data=X_d)\n",
    "            dset2 = f.create_dataset('labels',data=y_d)\n",
    "\n",
    "        #Upload file to bucket, then delete\n",
    "        try:\n",
    "            bucket.upload_file(Filename=filename,Key=key)\n",
    "            print(\"Completed {} upload for layer {}\".format(mode,layer_idx))\n",
    "        finally:\n",
    "            os.remove(filename)\n",
    "\n",
    "        #Delete train arrays to save memory\n",
    "        del X_d,y_d\n",
    "    \n",
    "    #Send notification that job was completed\n",
    "    try:\n",
    "        notify.on_epoch_end(layer_idx)\n",
    "    except:\n",
    "        print(\"Couldn't send notification!\")\n",
    "        \n",
    "#for l in range(11):\n",
    " #   try:\n",
    " #       CreateFeatureDataSet(l)\n",
    " #   except:\n",
    "   #     print(\"Failed to create feature set {}\".format(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For every layer in the data set, we're going to train a gardient booster classifier \n",
    "#using hyperparameters we selected from before\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import log_loss,make_scorer\n",
    "from sklearn.model_selection import KFold\n",
    "from matplotlib import pyplot as plt\n",
    "import gc\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def getScores(layer_idx,dir_name = 'featureextraction'):\n",
    "    #Scorer\n",
    "    log_loss_scorer = make_scorer(log_loss,greater_is_better=False,needs_proba=True)\n",
    "    \n",
    "    #Classifier\n",
    "    clf = GradientBoostingClassifier(min_samples_split=3,max_depth=5,learning_rate=0.05,n_estimators=100)\n",
    "    \n",
    "    #Grab data\n",
    "    #Download dataset\n",
    "    print(\"Downloading dataset for layer {}...\".format(layer_idx))\n",
    "    mode = 'train'\n",
    "    key_suffix = \"{}_layer{}.hdf5\".format(mode,layer_idx)\n",
    "    filename = os.path.join(TEMP_DIR,key_suffix)\n",
    "    key = \"{}/{}\".format(dir_name,key_suffix)\n",
    " \n",
    "    bucket.download_file(Key=key,Filename=filename)\n",
    "    \n",
    "    try:\n",
    "        #Cross validation parameters\n",
    "        #cv = 5\n",
    "        train_score_ary = []\n",
    "        test_score_ary = []\n",
    "        #X_index = np.zeros((800,1))\n",
    "       # kf = KFold(n_splits=cv,random_state = 0,shuffle = True)\n",
    "\n",
    "        #Open downloaded file and load data\n",
    "        with h5py.File(filename,\"r\") as f:\n",
    "            print(\"Running cross validation...\")\n",
    "            #for train_index,test_index in kf.split(X_index):\n",
    "            gc.collect()\n",
    "            #Fit on train data\n",
    "            X_train = f['/features'].vaue[:700]\n",
    "            y_train = f['/labels'].value[:700]\n",
    "            clf.fit(X_train,y_train)\n",
    "\n",
    "            #Train Score\n",
    "            train_score = log_loss_scorer(clf,X_train,y_train)\n",
    "            del X_train,y_train #Save memory\n",
    "\n",
    "            #Val Score\n",
    "            X_test = f['/features'].value[700:800] \n",
    "            y_test = f['/labels'].value[700:800]\n",
    "\n",
    "            #Val score\n",
    "            test_score = log_loss_scorer(clf,X_test,y_test)\n",
    "            del X_test, y_test\n",
    "\n",
    "            #Append to lists\n",
    "            train_score_ary.append(train_score)\n",
    "            test_score_ary.append(test_score)         \n",
    "\n",
    "        #Return dictionary with cross-validation data using 5 fold stratified validation\n",
    "        mean_train = np.mean(train_score_ary)\n",
    "        #std_train = np.std(train_score_ary)\n",
    "        mean_test = np.mean(test_score_ary)\n",
    "       # std_test = np.std(test_score_ary)\n",
    "\n",
    "        return mean_test,mean_train\n",
    "    finally:\n",
    "        os.remove(filename)\n",
    "\n",
    "#num_layers = 8\n",
    "#mean_train_ary = np.zeros(num_layers)\n",
    "#std_train_ary = np.zeros(num_layers)\n",
    "#mean_test_ary = np.zeros(num_layers)\n",
    "#std_test_ary = np.zeros(num_layers)\n",
    "\n",
    "#for l in range(1):\n",
    "#    gc.collect()\n",
    "#    print(\"Training layer {}\".format(l))\n",
    "#    mean_test,std_test,mean_train,std_train = getScores(l)\n",
    "#    mean_train_ary[l] = mean_train\n",
    "#    std_train_ary[l] = std_train\n",
    "#    mean_test_ary[l] = mean_test\n",
    "#    std_test_ary[l] = std_test\n",
    "\n",
    "#layers = np.arange(0,8)\n",
    "#plt.figure()\n",
    "#plt.errorbar(x=layers,y=mean_test_ary,yerr=std_test_ary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset for layer 1...\n",
      "Running cross validation...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ea123cc06b60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_direct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m700\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m700\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m#Train Score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DHSenv_3.5/lib/python3.5/site-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m   1027\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Presorting is not supported for sparse matrices.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m                 X_idx_sorted = np.asfortranarray(np.argsort(X, axis=0),\n\u001b[0m\u001b[1;32m   1030\u001b[0m                                                  dtype=np.int32)\n\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DHSenv_3.5/lib/python3.5/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margsort\u001b[0;34m(a, axis, kind, order)\u001b[0m\n\u001b[1;32m    905\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m     \"\"\"\n\u001b[0;32m--> 907\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argsort'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/DHSenv_3.5/lib/python3.5/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# An AttributeError occurs if the object does not have\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#For every layer in the data set, we're going to train a gardient booster classifier \n",
    "#using hyperparameters we selected from before\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import log_loss,make_scorer\n",
    "from sklearn.model_selection import KFold\n",
    "from matplotlib import pyplot as plt\n",
    "import gc\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#Cross validation parameters\n",
    "#Scorer\n",
    "layer_idx = 1\n",
    "dir_name = 'featureextraction'\n",
    " #Scorer\n",
    "log_loss_scorer = make_scorer(log_loss,greater_is_better=False,needs_proba=True)\n",
    "\n",
    "#Classifier\n",
    "clf = GradientBoostingClassifier(min_samples_split=3,max_depth=5,learning_rate=0.05,n_estimators=100)\n",
    "\n",
    "#Grab data\n",
    "#Download dataset\n",
    "print(\"Downloading dataset for layer {}...\".format(layer_idx))\n",
    "mode = 'train'\n",
    "key_suffix = \"{}_layer{}.hdf5\".format(mode,layer_idx)\n",
    "filename = os.path.join(TEMP_DIR,key_suffix)\n",
    "key = \"{}/{}\".format(dir_name,key_suffix)\n",
    "\n",
    "if not os.path.exists(filename):\n",
    "    bucket.download_file(Key=key,Filename=filename)\n",
    "#Cross validation parameters\n",
    "#cv = 5\n",
    "train_score_ary = []\n",
    "test_score_ary = []\n",
    "#X_index = np.zeros((800,1))\n",
    "# kf = KFold(n_splits=cv,random_state = 0,shuffle = True)\n",
    "\n",
    "#Open downloaded file and load data\n",
    "with h5py.File(filename,\"r\") as f:\n",
    "    print(\"Running cross validation...\")\n",
    "    #for train_index,test_index in kf.split(X_index):\n",
    "    #Fit on train data\n",
    "    #del X_train\n",
    "    gc.collect()\n",
    "    d = f['/features']\n",
    "    X_train = np.zeros((700,d.shape[1]))\n",
    "    d.read_direct(X_train,np.s_[:700],np.s_[:700])\n",
    "    \n",
    "    d = f['/labels']\n",
    "    y_train = np.zeros(700)\n",
    "    d.read_direct(y_train,np.s_[:700],np.s_[:700])\n",
    "    \n",
    "    clf.fit(X_train,y_train)\n",
    "\n",
    "    #Train Score\n",
    "    train_score = log_loss_scorer(clf,X_train,y_train)\n",
    "    del X_train,y_train #Save memory\n",
    "\n",
    "    #Val Score\n",
    "    X_test = np.zeros((100,f['/features'].shape[1]))\n",
    "    X_test = f['/features'].read_direct(X_test,np.s_[700:800],np.s_[700:800])\n",
    "    y_test = np.zeros(100)\n",
    "    y_test = f['/labels'].read_direct(y_test,np.s_[700:800],np.s_[700:800])\n",
    "\n",
    "    #Val score\n",
    "    test_score = log_loss_scorer(clf,X_test,y_test)\n",
    "    del X_test, y_test\n",
    "\n",
    "    #Append to lists\n",
    "    train_score_ary.append(train_score)\n",
    "    test_score_ary.append(test_score)         \n",
    "\n",
    "#Return dictionary with cross-validation data using 5 fold stratified validation\n",
    "mean_train = np.mean(train_score_ary)\n",
    "mean_test = np.mean(test_score_ary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (DSHenv_3.5)",
   "language": "python",
   "name": "dhsenv_3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
