{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to first try training a CNN on the individual images.\n",
    "We will be using binary cross entropy across the 17 regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import HelperFuncs as hfuncs\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.data_utils import Sequence\n",
    "import h5py\n",
    "import os\n",
    "import gc \n",
    "\n",
    "gc.collect()\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "FINAL_WIDTH = 400\n",
    "FINAL_HEIGHT = 600\n",
    "CHANNELS = 1\n",
    "ZONES = 17\n",
    "ANGLES = 16\n",
    "#Create directories for sequencer function if they don't exist\n",
    "for d in ['temp/train_scan/','temp/test_scan/','temp/val_scan/']:\n",
    "    if not os.path.isdir(d):\n",
    "        print(\"Created directory: {}\".format(d))\n",
    "        os.makedirs(d)\n",
    "        \n",
    "class ScanSequencer(Sequence):\n",
    "    idx_dict={}\n",
    "    \n",
    "    def __init__(self,num_batches,bucket_name,mode=\"train_scan\"):\n",
    "        self.num_batches = num_batches\n",
    "        self.bucket_name = bucket_name\n",
    "        self.mode = mode\n",
    "        self.key_id, self.secret_key = hfuncs.GetAWSCredentials()\n",
    "        self.mode = mode\n",
    "        self.angles = np.arange(0,64,64//ANGLES)\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "    def on_epoch_end(self):\n",
    "        pass\n",
    "    def __getitem__(self,idx):\n",
    "        #Get Client\n",
    "        client = hfuncs.GetAWSClient(self.key_id,self.secret_key)\n",
    "        bucket = client.Bucket(self.bucket_name)\n",
    "        \n",
    "        #Initialize vectors\n",
    "        X_train = np.zeros((BATCH_SIZE,ANGLES,FINAL_WIDTH,FINAL_HEIGHT,CHANNELS))\n",
    "        y_train = np.zeros((BATCH_SIZE,ZONES))\n",
    "        \n",
    "        j=0\n",
    "        for i in range(idx*BATCH_SIZE,(idx+1)*BATCH_SIZE):\n",
    "            #Download batch at index\n",
    "            path = \"temp/{}/batch_{}.hdf5\".format(self.mode,i)\n",
    "            key = \"{}/batch_{}.hdf5\".format(self.mode,i)\n",
    "            bucket.download_file(Key=key,Filename=path)\n",
    "        \n",
    "            f = h5py.File(path,\"r\")\n",
    "            try:\n",
    "                X_train[j,:,:,:,:] = f['/image'].value[self.angles,:,:,:]\n",
    "                y_train[j,:] = f['/labels'].value\n",
    "                j += 1\n",
    "            finally:\n",
    "                f.close()\n",
    "                os.remove(path) \n",
    "        return X_train, y_train\n",
    "class LegScanSequencer(Sequence):\n",
    "    idx_dict={}\n",
    "    \n",
    "    def __init__(self,num_batches,bucket_name,mode=\"train_scan\",batch_size=BATCH_SIZE):\n",
    "        self.num_batches = num_batches\n",
    "        self.bucket_name = bucket_name\n",
    "        self.mode = mode\n",
    "        self.key_id, self.secret_key = hfuncs.GetAWSCredentials()\n",
    "        self.mode = mode\n",
    "        self.angles = np.arange(0,64,64//ANGLES)\n",
    "        self.batch_size = batch_size\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "    def on_epoch_end(self):\n",
    "        pass\n",
    "    def __getitem__(self,idx):\n",
    "        #Get Client\n",
    "        client = hfuncs.GetAWSClient(self.key_id,self.secret_key)\n",
    "        bucket = client.Bucket(self.bucket_name)\n",
    "        \n",
    "        #Initialize vectors\n",
    "        X_train = np.zeros((self.batch_size,ANGLES,FINAL_WIDTH,FINAL_HEIGHT*3//5,CHANNELS))\n",
    "        y_train = np.zeros((self.batch_size,1))\n",
    "        s_weights = np.zeros((self.batch_size))\n",
    "        j=0\n",
    "        for i in range(idx*self.batch_size,(idx+1)*self.batch_size):\n",
    "            #Download batch at index\n",
    "            path = \"temp/{}/batch_{}.hdf5\".format(self.mode,i)\n",
    "            key = \"{}/batch_{}.hdf5\".format(self.mode,i)\n",
    "            bucket.download_file(Key=key,Filename=path)\n",
    "        \n",
    "            f = h5py.File(path,\"r\")\n",
    "            try:\n",
    "                X_train[j,:,:,:,:] = f['/image'].value[self.angles,:,:FINAL_HEIGHT*3//5,:]\n",
    "                r_leg = [7,8,9,10,11,12,13,14,15]\n",
    "                l_leg = [13,15]\n",
    "                r_y = np.amax(f['/labels'].value[r_leg])\n",
    "                #l_y = np.amax(f['/labels'].value[l_leg])\n",
    "                y_train[j,:] = r_y\n",
    "                s_weights[j] =np.squeeze(r_y*0 + 1) #np.squeeze(r_y + (-1 * r_y + 1)*2)\n",
    "                j += 1\n",
    "            finally:\n",
    "                f.close()\n",
    "                os.remove(path) \n",
    "        return X_train, y_train,s_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from twilio.rest import Client\n",
    "import configparser\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "class SMSNotifier(Callback):\n",
    "    def on_epoch_end(self,epoch,logs=None):\n",
    "        #Execute every other epoch\n",
    "        if epoch % 2 == 0:\n",
    "            #Get config credentials\n",
    "            config = configparser.ConfigParser()\n",
    "            config.read('twilio.conf')\n",
    "            account_sid = config['DEFAULT']['AccountID']\n",
    "            auth_token = config['DEFAULT']['AuthToken']\n",
    "            #Get client\n",
    "            client = Client(account_sid, auth_token)\n",
    "            #Create message\n",
    "            if logs is not None:\n",
    "                message = \"Epoch {} complete. Loss: {} Val_loss: {} \".format(epoch,\n",
    "                                                                             logs.get('loss'),\n",
    "                                                                             logs.get('val_loss'))\n",
    "            else:\n",
    "                message = \"Epoch {} complete. No loss data available.\".format(epoch)\n",
    "            #Sendmessage\n",
    "            message = client.messages.create(\n",
    "                to=\"+16178884129\", \n",
    "                from_=\"+18572142288\",\n",
    "                body=message)\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Build pre-trained V2 model\n",
    "import numpy as np\n",
    "from keras.layers import Input,Flatten,Dense,Concatenate,Dropout,concatenate,GlobalMaxPool2D,GlobalAveragePooling1D\n",
    "from keras.models import Model\n",
    "from datetime import datetime\n",
    "from keras.callbacks import TensorBoard,EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\n",
    "from keras.optimizers import Adam,Adadelta,SGD\n",
    "from keras.metrics import binary_accuracy\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import GRUCell,GRU,LSTM\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.applications.inception_v3 import InceptionV3,preprocess_input\n",
    "from keras.layers.core import Lambda\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.initializers import RandomUniform,glorot_uniform,Orthogonal\n",
    "#from keras.applications.vgg19 import VGG19\n",
    "\n",
    "#K.set_learning_phase(1)\n",
    "def ToRGB(x):\n",
    "    max_v = tf.reduce_max(x)\n",
    "    min_v = tf.reduce_min(x)\n",
    "    max_rgb = tf.constant(255,dtype=x.dtype)\n",
    "    min_rgb = tf.constant(255//2,dtype=x.dtype)\n",
    "    x = tf.add(tf.floordiv(tf.multiply(tf.subtract(x,min_v),tf.subtract(max_rgb,min_rgb)),tf.subtract(max_v,min_v)),min_rgb)\n",
    "    return x\n",
    "def ToGreyScale(x):\n",
    "    #Divide RGB into 3\n",
    "   # scalar = tf.constant(3,dtype=x.dtype)\n",
    "    #x = tf.floordiv(x,scalar)\n",
    "    shape = x.get_shape()\n",
    "    #assume channel_last\n",
    "    mult = [[1 for d in shape[:-1]],[3]]\n",
    "    mult = [val for sublist in mult for val in sublist]\n",
    "    return tf.tile(x,mult)\n",
    "def ToNewShape(x):\n",
    "    ndim = len(x.shape)\n",
    "    if ndim == 5:\n",
    "        return tf.reverse(tf.transpose(x,[0,1,3,2,4]),[-3])\n",
    "    elif ndim == 4:\n",
    "        return tf.reverse(tf.transpose(x,[0,2,1,3]),[-3])\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected number of dims!\")\n",
    "def PrintActivation(x):\n",
    "    meanv = tf.reduce_mean(x)\n",
    "    minv = tf.reduce_min(x)\n",
    "    maxv = tf.reduce_max(x)\n",
    "    #print('Mean:{},Min:{},Max:{}'.format(meanv.eval(),minv.eval(),maxv.eval()))\n",
    "    return [meanv,minv,maxv]\n",
    "def getSingleLegModel(lstm_dim=10):\n",
    "    #Single model image\n",
    "    input_img = Input(shape=(FINAL_WIDTH,FINAL_HEIGHT*3//5,CHANNELS))\n",
    "    \n",
    "    #preprocess and extract channels\n",
    "    input_img_pp = Lambda(ToRGB)(input_img)\n",
    "    input_img_pp = Lambda(ToGreyScale)(input_img_pp)\n",
    "    input_img_pp = Lambda(preprocess_input)(input_img_pp)\n",
    "    input_img_pp = Lambda(ToNewShape)(input_img_pp)\n",
    "    \n",
    "    #Load resnet\n",
    "    incep = InceptionV3(include_top=False,\n",
    "                          weights='imagenet',\n",
    "                          input_tensor=None,\n",
    "                          input_shape=(FINAL_HEIGHT*3//5,FINAL_WIDTH,3),\n",
    "                          pooling='None')\n",
    "    for l in incep.layers:\n",
    "        l.trainable=False\n",
    "\n",
    "    #Take off top\n",
    "    reduced_net = Model(incep.input,incep.get_layer('mixed5').output)\n",
    "    output = reduced_net(input_img_pp)\n",
    "    output = GlobalMaxPool2D()(output)\n",
    "\n",
    "    intermediate_model = Model(input_img,output)\n",
    "    #Time distributed model\n",
    "    input_scan = Input(shape=(ANGLES,FINAL_WIDTH,FINAL_HEIGHT*3//5,CHANNELS))\n",
    "    sequenced_model = TimeDistributed(intermediate_model)(input_scan)\n",
    "    sequenced_model._uses_learning_phase = True\n",
    "    #One GRU layer for now\n",
    "    state = K.zeros(shape=(1,100))\n",
    "    \n",
    "    gru = GRU(100,dropout=0.20,kernel_initializer=RandomUniform(minval=-1.0,maxval=1.0,seed=0),\n",
    "               recurrent_initializer=Orthogonal(gain=5,seed=0))(sequenced_model,initial_state=[state])\n",
    "  \n",
    "    #Finally, 1 dense layer\n",
    "    out = Dense(1,activation='sigmoid',use_bias=False,\n",
    "                kernel_initializer=RandomUniform(minval=-0.5,maxval=0.5,seed=0))(gru)\n",
    "    #complete model\n",
    "    #out = GlobalAveragePooling1D()(sequenced_model)\n",
    "    model = Model(input_scan,out)\n",
    "    try:\n",
    "        return model#Model(input_scan, out)\n",
    "    finally:\n",
    "        del intermediate_model,incep,reduced_net\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a layer specific wrapper that plots histograms only for cpecified layers and loads validation data onto memory\n",
    "class TensorBoardWrapper(TensorBoard):\n",
    "    '''Sets the self.validation_data property for use with TensorBoard callback.'''\n",
    "\n",
    "    def __init__(self, batch_gen, nb_steps,layer_ary, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.batch_gen = batch_gen # The generator.\n",
    "        self.nb_steps = nb_steps     # Number of times to call next() on the generator.\n",
    "        self.layer_ary = layer_ary\n",
    "    def set_model(self,model):\n",
    "        self.model = model\n",
    "        self.sess = K.get_session()\n",
    "        if self.histogram_freq and self.merged is None:\n",
    "            for l in self.layer_ary:\n",
    "                layer = self.model.layers[l]\n",
    "                for weight in layer.weights:\n",
    "                    mapped_weight_name = weight.name.replace(':', '_')\n",
    "                    tf.summary.histogram(mapped_weight_name, weight)\n",
    "                    if self.write_grads:\n",
    "                        grads = model.optimizer.get_gradients(model.total_loss,\n",
    "                                                              weight)\n",
    "\n",
    "                        def is_indexed_slices(grad):\n",
    "                            return type(grad).__name__ == 'IndexedSlices'\n",
    "                        grads = [\n",
    "                            grad.values if is_indexed_slices(grad) else grad\n",
    "                            for grad in grads]\n",
    "                        tf.summary.histogram('{}_grad'.format(mapped_weight_name), grads)\n",
    "                    if self.write_images:\n",
    "                        w_img = tf.squeeze(weight)\n",
    "                        shape = K.int_shape(w_img)\n",
    "                        if len(shape) == 2:  # dense layer kernel case\n",
    "                            if shape[0] > shape[1]:\n",
    "                                w_img = tf.transpose(w_img)\n",
    "                                shape = K.int_shape(w_img)\n",
    "                            w_img = tf.reshape(w_img, [1,\n",
    "                                                       shape[0],\n",
    "                                                       shape[1],\n",
    "                                                       1])\n",
    "                        elif len(shape) == 3:  # convnet case\n",
    "                            if K.image_data_format() == 'channels_last':\n",
    "                                # switch to channels_first to display\n",
    "                                # every kernel as a separate image\n",
    "                                w_img = tf.transpose(w_img, perm=[2, 0, 1])\n",
    "                                shape = K.int_shape(w_img)\n",
    "                            w_img = tf.reshape(w_img, [shape[0],\n",
    "                                                       shape[1],\n",
    "                                                       shape[2],\n",
    "                                                       1])\n",
    "                        elif len(shape) == 1:  # bias case\n",
    "                            w_img = tf.reshape(w_img, [1,\n",
    "                                                       shape[0],\n",
    "                                                       1,\n",
    "                                                       1])\n",
    "                        else:\n",
    "                            # not possible to handle 3D convnets etc.\n",
    "                            continue\n",
    "\n",
    "                        shape = K.int_shape(w_img)\n",
    "                        assert len(shape) == 4 and shape[-1] in [1, 3, 4]\n",
    "                        tf.summary.image(mapped_weight_name, w_img)\n",
    "\n",
    "                if hasattr(layer, 'output'):\n",
    "                    tf.summary.histogram('{}_out'.format(layer.name),\n",
    "                                         layer.output)\n",
    "        self.merged = tf.summary.merge_all()\n",
    "\n",
    "        if self.write_graph:\n",
    "            self.writer = tf.summary.FileWriter(self.log_dir,\n",
    "                                                self.sess.graph)\n",
    "        else:\n",
    "            self.writer = tf.summary.FileWriter(self.log_dir)\n",
    "\n",
    "        if self.embeddings_freq:\n",
    "            embeddings_layer_names = self.embeddings_layer_names\n",
    "\n",
    "            if not embeddings_layer_names:\n",
    "                embeddings_layer_names = [layer.name for layer in self.model.layers\n",
    "                                          if type(layer).__name__ == 'Embedding']\n",
    "\n",
    "            embeddings = {layer.name: layer.weights[0]\n",
    "                          for layer in self.model.layers\n",
    "                          if layer.name in embeddings_layer_names}\n",
    "\n",
    "            self.saver = tf.train.Saver(list(embeddings.values()))\n",
    "\n",
    "            embeddings_metadata = {}\n",
    "\n",
    "            if not isinstance(self.embeddings_metadata, str):\n",
    "                embeddings_metadata = self.embeddings_metadata\n",
    "            else:\n",
    "                embeddings_metadata = {layer_name: self.embeddings_metadata\n",
    "                                       for layer_name in embeddings.keys()}\n",
    "\n",
    "            config = projector.ProjectorConfig()\n",
    "            self.embeddings_ckpt_path = os.path.join(self.log_dir,\n",
    "                                                     'keras_embedding.ckpt')\n",
    "\n",
    "            for layer_name, tensor in embeddings.items():\n",
    "                embedding = config.embeddings.add()\n",
    "                embedding.tensor_name = tensor.name\n",
    "\n",
    "                if layer_name in embeddings_metadata:\n",
    "                    embedding.metadata_path = embeddings_metadata[layer_name]\n",
    "\n",
    "            projector.visualize_embeddings(self.writer, config)\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        # Fill in the `validation_data` property. Obviously this is specific to how your generator works.\n",
    "        # Below is an example that yields images and classification tags.\n",
    "        # After it's filled in, the regular on_epoch_end method has access to the validation_data.\n",
    "        X_val,y_val,w_val = None, None, None\n",
    "        for s in range(self.nb_steps):\n",
    "            X, y, w = self.batch_gen.__getitem__(s+1)\n",
    "            if X_val is None:\n",
    "                X_val = np.zeros((self.nb_steps * X.shape[0], *X.shape[1:]))\n",
    "                y_val = np.zeros((self.nb_steps * y.shape[0], *y.shape[1:]))\n",
    "                w_val = np.zeros((self.nb_steps * w.shape[0]))\n",
    "            X_val[s * X.shape[0]:(s + 1) * X.shape[0]] = X\n",
    "            y_val[s * y.shape[0]:(s + 1) * y.shape[0]] = y\n",
    "            w_val[s * w.shape[0]:(s + 1) * w.shape[0]] = w\n",
    "        self.validation_data = (X_val,y_val,w_val,0)\n",
    "        print(self.validation_data[1])\n",
    "        return super().on_epoch_end(epoch, logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rnn Model Retrieved\n"
     ]
    }
   ],
   "source": [
    "rnn_model = getSingleLegModel()\n",
    "print(\"Rnn Model Retrieved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for l in rnn_model.layers:\n",
    "    print(l.name, l.trainable,l.output_shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing generators...\n",
      "Beginning training...\n",
      "Epoch 1/500\n",
      "100/100 [==============================] - 624s - loss: 1.0771 - binary_accuracy: 0.5000 - val_loss: 0.8990 - val_binary_accuracy: 0.4600\n",
      "Epoch 2/500\n",
      " 26/100 [======>.......................] - ETA: 292s - loss: 0.8656 - binary_accuracy: 0.5000"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "description = \"Average_layer5\"\n",
    "#adad = Adadelta()\n",
    "#print(\"Loading weights\")\n",
    "#rnn_model.load_weights(\"check_points/TestAverageModel_03-0.63.hdf5\")\n",
    "adam = Adam(lr=0.001,beta_1=0.9,beta_2=0.999)\n",
    "#sgd = SGD(lr = 0.0001,momentum = 0.25,nesterov=True)\n",
    "rnn_model.compile(optimizer=adam,\n",
    "                          metrics=[binary_accuracy],\n",
    "                         loss= binary_crossentropy,)\n",
    "\n",
    "\n",
    "#Model checkpoint\n",
    "x = datetime.today()\n",
    "check_point_dir = 'check_points/'\n",
    "if not os.path.isdir(check_point_dir):\n",
    "    os.makedirs(check_point_dir)    \n",
    "chkpt = ModelCheckpoint(os.path.join(check_point_dir,\"TestAverageModel_{epoch:02d}-{val_loss:.2f}.hdf5\"),\n",
    "                       monitor='val_loss',\n",
    "                       verbose=1,\n",
    "                       save_best_only=True)\n",
    "#Reduce learning rate on plateau\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                             factor=0.2,\n",
    "                             patience=2,\n",
    "                             verbose=1,\n",
    "                             min_lr=0.00001,\n",
    "                             cooldown = 10)\n",
    "#Notifications\n",
    "notify = SMSNotifier()\n",
    "\n",
    "#Early stopping callback\n",
    "estop = EarlyStopping(monitor='val_loss',min_delta=0.001,patience=25)\n",
    "\n",
    "#Generators and fit\n",
    "print(\"Initializing generators...\")\n",
    "#Bucket with clean data\n",
    "UPLOAD_BUCKET = 'cleandhsdata' #bucket where clean data was stored\n",
    "key_id, secret_key = hfuncs.GetAWSCredentials()\n",
    "client = hfuncs.GetAWSClient(key_id,secret_key)\n",
    "bucket = client.Bucket(UPLOAD_BUCKET)\n",
    "\n",
    "#Initialize train sequencer\n",
    "mode =\"train_scan\"\n",
    "num_batches_train = 50#(sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1)//BATCH_SIZE #train,test,val root directories have their own keys\n",
    "#num_batches = 660//BATCH_SIZE\n",
    "train_seq = LegScanSequencer(num_batches_train,UPLOAD_BUCKET,mode=mode)\n",
    "#Initialize validation sequencer\n",
    "mode = \"val_scan\"\n",
    "num_batches_val = 50 #(sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1)//BATCH_SIZE #train,test,val root directories have their own keys\n",
    "val_seq = LegScanSequencer(num_batches_val,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "#Tensorboard\n",
    "stamp = \"Test79\"\n",
    "tensorboard = TensorBoardWrapper(val_seq,nb_steps=4,layer_ary=[2,3],\n",
    "                                 log_dir=\"logs/{}\".format(stamp),\n",
    "                                 histogram_freq=1,\n",
    "                                 batch_size=2,#Must match regular batch size it seems\n",
    "                              write_grads=True,\n",
    "                                 write_images=False)\n",
    "\n",
    "print(\"Beginning training...\")\n",
    "#recurrent_model.load_weights('check_points/2017_10_3_17_ScanModel_01-1.29.hdf5')\n",
    "\n",
    "hist= rnn_model.fit_generator(train_seq,\n",
    "                                   steps_per_epoch = num_batches_train*2,\n",
    "                                   epochs = 500,\n",
    "                                   validation_data = val_seq,\n",
    "                                    validation_steps = num_batches_val,\n",
    "                                  # callbacks=[tensorboard],#,estop,reduce_lr],\n",
    "                                  use_multiprocessing = False,workers=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Bucket with clean data\n",
    "UPLOAD_BUCKET = 'cleandhsdata' #bucket where clean data was stored\n",
    "key_id, secret_key = hfuncs.GetAWSCredentials()\n",
    "client = hfuncs.GetAWSClient(key_id,secret_key)\n",
    "bucket = client.Bucket(UPLOAD_BUCKET)\n",
    "\n",
    "#Initialize train sequencer\n",
    "mode =\"train_scan\"\n",
    "num_batches_train = 25 #(sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1)//BATCH_SIZE #train,test,val root directories have their own keys\n",
    "#num_batches = 660//BATCH_SIZE\n",
    "train_seq = LegScanSequencer(num_batches_train,UPLOAD_BUCKET,mode=mode)\n",
    "X,y,w =train_seq.__getitem__(14)\n",
    "print(X.shape)\n",
    "print(y,w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img,img_to_array\n",
    "from matplotlib import pyplot as plt \n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "img = load_img('individualImage.png',grayscale=False,target_size=(FINAL_WIDTH,FINAL_HEIGHT*3//5))\n",
    "img_ary = np.array([img_to_array(img)])\n",
    "print(img_ary.shape)\n",
    "fig = plt.figure(figsize = (16,16))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(img_ary[0])#, cmap = 'viridis')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(img_ary.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Test modelchanges\n",
    "sess = tf.InteractiveSession()\n",
    "slm = getSingleLegModel(16)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "result = rnn_model.predict(X)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (16,16))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(result[0,1,:,:,:])#, cmap = 'viridis')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Test how many positive samples\n",
    "import pickle\n",
    "labels = hfuncs.GetLabelsDict(r'stage1_labels.csv')\n",
    "#filename = \"data_separated.pickle\"\n",
    "#with open(filename,\"rb\") as f:\n",
    "#   save = pickle.load(f)\n",
    "#   K_test= save['K_test']\n",
    "#   K_val = save['K_val']\n",
    "#   K_train = save['K_train']\n",
    "#s = 0\n",
    "#pos = 0\n",
    "s= 0\n",
    "pos = 0\n",
    "for k_clean in labels.keys():\n",
    "    label = np.array(labels[k_clean])\n",
    "    val = np.amax(label[[7,8,9,10,11,12,13,14,15]])\n",
    "    if val == 1:\n",
    "        s += 1\n",
    "        pos += 1\n",
    "    else:\n",
    "        s += 1\n",
    "print(\"total={},pos={}\".format(s,pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "710/1147"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (DSHenv_3.5)",
   "language": "python",
   "name": "dhsenv_3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
