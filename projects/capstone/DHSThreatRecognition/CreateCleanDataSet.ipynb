{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Clean HDF5 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Generator Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting demo environment variables...\n"
     ]
    }
   ],
   "source": [
    "import HelperFuncs as hfuncs\n",
    "\n",
    "class myScanGenerator:\n",
    "    # AWS and Directory information \n",
    "    bucketName = hfuncs.RAW_DATA_BUCKET\n",
    "    dataDir = hfuncs.RAW_DATA_DIRECTORY\n",
    "    temp_dir = hfuncs.TEMP_DIR\n",
    "    labels_dir = hfuncs.LABELS_FILE\n",
    "    \n",
    "    # Connect to AWS\n",
    "    key_id, secret_key = hfuncs.GetAWSCredentials()\n",
    "    client = hfuncs.GetAWSClient(key_id,secret_key)\n",
    "    bucket = client.Bucket(bucketName)\n",
    "    extension = hfuncs.EXTENSION\n",
    "    \n",
    "    # Labels and keys\n",
    "    labels_dict = hfuncs.GetLabelsDict(labels_dir)\n",
    "    key_ary = None\n",
    "    \n",
    "    # Samples to load at a time (reduce if memory available is low)\n",
    "    n_samples = 0\n",
    "    \n",
    "    # Requester\n",
    "    batch_requester = None\n",
    "    \n",
    "    def __init__(self,keys,n_samples):\n",
    "        # Keys of samples to process\n",
    "        self.key_ary = keys\n",
    "        \n",
    "        # Samples to load at a time\n",
    "        self.n_samples = n_samples\n",
    "        \n",
    "        # Initialize AWS Batch Requester\n",
    "        self.batchrequester = hfuncs.BatchRequester(self.bucket,self.key_ary,self.labels_dict,self.dataDir,self.temp_dir,self.extension)\n",
    "    \n",
    "    def GenerateSamples(self):\n",
    "        '''Returns generator that retireves n_sample scans at a time, applies cleaning function, and yields\n",
    "        numpy data array with the clean data.  The shape of the array is ()'''\n",
    "        #While there is data left, yield batch\n",
    "        while self.batchrequester.DoItemsRemain():\n",
    "            #Request data\n",
    "            print(\"Retrieving samples..\")\n",
    "            pointer = self.batchrequester.key_pointer\n",
    "            X,y = self.batchrequester.NextBatch(self.n_samples)\n",
    "            n_angles = X.shape[3] \n",
    "\n",
    "            print(\"Samples retrieved\")\n",
    "\n",
    "            # Initialize output arrays\n",
    "            X_train = np.zeros((X.shape[0],n_angles,hfuncs.FINAL_WIDTH,hfuncs.FINAL_HEIGHT,hfuncs.CHANNELS))\n",
    "            y_train = np.zeros((X.shape[0],hfuncs.ZONES))\n",
    "            \n",
    "\n",
    "            # Set scan counter to 0, channel to 1\n",
    "            i = 0\n",
    "            chan = 0 # Only one channel in these data.  In the future, this can be changed for multichannel videos or scans.\n",
    "\n",
    "            \n",
    "            # For each scan, clean each angle slice and store it in output array.\n",
    "            for i in range(X.shape[0]):\n",
    "                # Clean each angle slice\n",
    "                for j in range(n_angles):\n",
    "                    X_train[i,j,:,:,chan] = hfuncs.CropCleanResize(X[i,:,:,j],hfuncs.FINAL_WIDTH,hfuncs.FINAL_HEIGHT)\n",
    "                \n",
    "                # Store label\n",
    "                y_train[i,:] = y[i,:]\n",
    "                \n",
    "                # Yield scans, one by one\n",
    "                yield X_train[i,:,:,:,:],y_train[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def getTrainTestSplit(labels_dir=hfuncs.LABELS_FILE,extension=hfuncs.EXTENSION,\n",
    "                        dataDir=hfuncs.RAW_DATA_DIRECTORY,bucketName=hfuncs.RAW_DATA_BUCKET):\n",
    "    '''Retrieves all samples in raw data that have labels \n",
    "    and splits data into a train and val sets. '''\n",
    "    # Labels        \n",
    "    labels_dict = hfuncs.GetLabelsDict(labels_dir)\n",
    "\n",
    "    # AWS Bucket\n",
    "    key_id, secret_key = hfuncs.GetAWSCredentials()\n",
    "    client = hfuncs.GetAWSClient(key_id,secret_key)\n",
    "    bucket = client.Bucket(bucketName)\n",
    "    \n",
    "    # Get shuffled keys \n",
    "    key_ary = hfuncs.GetShuffledKeys(bucket)\n",
    "    \n",
    "    # Make sure keys correspond to samples that have labels\n",
    "    key_ary = hfuncs.CleanKeyAry(key_ary,labels_dict,dataDir,extension)\n",
    "    \n",
    "    # Perform train-test split\n",
    "    K_train,K_val = train_test_split(key_ary,test_size=0.20,random_state=0)\n",
    "    \n",
    "    return K_train, K_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data and Upload to S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning train_scan data...\n",
      "Retrieving samples..\n",
      "Samples retrieved\n",
      "Completing batch 1 of 5\n",
      "Completing batch 2 of 5\n",
      "Completing batch 3 of 5\n",
      "Completing batch 4 of 5\n",
      "Completing batch 5 of 5\n",
      "Cleaning val_scan data...\n",
      "Retrieving samples..\n",
      "Samples retrieved\n",
      "Completing batch 1 of 5\n",
      "Completing batch 2 of 5\n",
      "Completing batch 3 of 5\n",
      "Completing batch 4 of 5\n",
      "Completing batch 5 of 5\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import os\n",
    "import setup_file as S\n",
    "import numpy as np\n",
    "\n",
    "K_train, K_val = getTrainTestSplit()\n",
    "\n",
    "# If it is a demo, only get 5 samples\n",
    "if hfuncs.IS_DEMO:\n",
    "    n_train = 5\n",
    "    n_val = 5\n",
    "    K_train = np.random.choice(K_train,n_train,replace=False)\n",
    "    K_val = np.random.choice(K_val,n_val,replace=False)\n",
    "\n",
    "\n",
    "# Connect to AWS S3\n",
    "key_id, secret_key = hfuncs.GetAWSCredentials()\n",
    "client = hfuncs.GetAWSClient(key_id,secret_key)\n",
    "bucket = client.Bucket(hfuncs.CLEAN_DATA_BUCKET)\n",
    "\n",
    "# Clean and upload data!\n",
    "for mode, keys in zip([\"train_scan\",\"val_scan\"],[K_train,K_val]):\n",
    "    print(\"Cleaning {} data...\".format(mode))\n",
    "    key_root = mode\n",
    "    trainGen = myScanGenerator(keys,5)\n",
    "    i = 0\n",
    "    \n",
    "    for X, y in trainGen.GenerateSamples():\n",
    "        print(\"Completing batch {} of {}\".format(i+1,len(keys)))\n",
    "        filename = os.path.join(hfuncs.TEMP_DIR,\"batch_{}.hdf5\".format(i))\n",
    "        key = \"{}/{}\".format(key_root,\"batch_{}.hdf5\".format(i))\n",
    "        \n",
    "        # Write data to h5py file\n",
    "        with h5py.File(filename,\"w\") as f:\n",
    "            dset = f.create_dataset('image',data=X)\n",
    "            dset2 = f.create_dataset('labels',data=y)\n",
    "\n",
    "        # Upload file to bucket and delete local copy\n",
    "        bucket.upload_file(Filename=filename,Key=key)\n",
    "        os.remove(filename)\n",
    "        i += 1\n",
    "        \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DemoDHSenv_3.5)",
   "language": "python",
   "name": "demodhsenv_3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
