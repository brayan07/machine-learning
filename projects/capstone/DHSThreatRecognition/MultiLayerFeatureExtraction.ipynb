{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to first try training a CNN on the individual images.\n",
    "We will be using binary cross entropy across the 17 regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import HelperFuncs as hfuncs\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.data_utils import Sequence\n",
    "import h5py\n",
    "import os\n",
    "import gc \n",
    "\n",
    "gc.collect()\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "FINAL_WIDTH = 400\n",
    "FINAL_HEIGHT = 600\n",
    "CHANNELS = 1\n",
    "ZONES = 17\n",
    "ANGLES = 16\n",
    "#Create directories for sequencer function if they don't exist\n",
    "for d in ['temp/train_scan/','temp/test_scan/','temp/val_scan/']:\n",
    "    if not os.path.isdir(d):\n",
    "        print(\"Created directory: {}\".format(d))\n",
    "        os.makedirs(d)\n",
    "        \n",
    "class ScanSequencer(Sequence):\n",
    "    idx_dict={}\n",
    "    \n",
    "    def __init__(self,num_batches,bucket_name,mode=\"train_scan\"):\n",
    "        self.num_batches = num_batches\n",
    "        self.bucket_name = bucket_name\n",
    "        self.mode = mode\n",
    "        self.key_id, self.secret_key = hfuncs.GetAWSCredentials()\n",
    "        self.mode = mode\n",
    "        self.angles = np.arange(0,64,64//ANGLES)\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "    def on_epoch_end(self):\n",
    "        pass\n",
    "    def __getitem__(self,idx):\n",
    "        #Get Client\n",
    "        client = hfuncs.GetAWSClient(self.key_id,self.secret_key)\n",
    "        bucket = client.Bucket(self.bucket_name)\n",
    "        \n",
    "        #Initialize vectors\n",
    "        X_train = np.zeros((BATCH_SIZE,ANGLES,FINAL_WIDTH,FINAL_HEIGHT,CHANNELS))\n",
    "        y_train = np.zeros((BATCH_SIZE,ZONES))\n",
    "        \n",
    "        j=0\n",
    "        for i in range(idx*BATCH_SIZE,(idx+1)*BATCH_SIZE):\n",
    "            #Download batch at index\n",
    "            path = \"temp/{}/batch_{}.hdf5\".format(self.mode,i)\n",
    "            key = \"{}/batch_{}.hdf5\".format(self.mode,i)\n",
    "            bucket.download_file(Key=key,Filename=path)\n",
    "        \n",
    "            f = h5py.File(path,\"r\")\n",
    "            try:\n",
    "                X_train[j,:,:,:,:] = f['/image'].value[self.angles,:,:,:]\n",
    "                y_train[j,:] = f['/labels'].value\n",
    "                j += 1\n",
    "            finally:\n",
    "                f.close()\n",
    "                os.remove(path) \n",
    "        return X_train, y_train\n",
    "class LegScanSequencer(Sequence):\n",
    "    idx_dict={}\n",
    "    \n",
    "    def __init__(self,num_batches,bucket_name,mode=\"train_scan\",batch_size=BATCH_SIZE):\n",
    "        self.num_batches = num_batches\n",
    "        self.bucket_name = bucket_name\n",
    "        self.mode = mode\n",
    "        self.key_id, self.secret_key = hfuncs.GetAWSCredentials()\n",
    "        self.mode = mode\n",
    "        self.angles = np.arange(0,64,64//ANGLES)\n",
    "        self.batch_size = batch_size\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "    def on_epoch_end(self):\n",
    "        pass\n",
    "    def __getitem__(self,idx):\n",
    "        #Get Client\n",
    "        client = hfuncs.GetAWSClient(self.key_id,self.secret_key)\n",
    "        bucket = client.Bucket(self.bucket_name)\n",
    "        \n",
    "        #Initialize vectors\n",
    "        X_train = np.zeros((self.batch_size,ANGLES,FINAL_WIDTH,FINAL_HEIGHT*3//5,CHANNELS))\n",
    "        y_train = np.zeros((self.batch_size,1))\n",
    "        s_weights = np.zeros((self.batch_size))\n",
    "        j=0\n",
    "        for i in range(idx*self.batch_size,(idx+1)*self.batch_size):\n",
    "            #Download batch at index\n",
    "            path = \"temp/{}/batch_{}.hdf5\".format(self.mode,i)\n",
    "            key = \"{}/batch_{}.hdf5\".format(self.mode,i)\n",
    "            bucket.download_file(Key=key,Filename=path)\n",
    "        \n",
    "            f = h5py.File(path,\"r\")\n",
    "            try:\n",
    "                X_train[j,:,:,:,:] = f['/image'].value[self.angles,:,:FINAL_HEIGHT*3//5,:]\n",
    "                r_leg = [7,8,9,10,11,12,13,14,15]\n",
    "                l_leg = [13,15]\n",
    "                r_y = np.amax(f['/labels'].value[r_leg])\n",
    "                #l_y = np.amax(f['/labels'].value[l_leg])\n",
    "                y_train[j,:] = r_y\n",
    "                s_weights[j] =np.squeeze(r_y*0 + 1) #np.squeeze(r_y + (-1 * r_y + 1)*2)\n",
    "                j += 1\n",
    "            finally:\n",
    "                f.close()\n",
    "                os.remove(path) \n",
    "        return X_train, y_train,s_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from twilio.rest import Client\n",
    "import configparser\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "class SMSNotifier(Callback):\n",
    "    def on_epoch_end(self,epoch,logs=None):\n",
    "        #Execute every other epoch\n",
    "        if epoch % 2 == 0:\n",
    "            #Get config credentials\n",
    "            config = configparser.ConfigParser()\n",
    "            config.read('twilio.conf')\n",
    "            account_sid = config['DEFAULT']['AccountID']\n",
    "            auth_token = config['DEFAULT']['AuthToken']\n",
    "            #Get client\n",
    "            client = Client(account_sid, auth_token)\n",
    "            #Create message\n",
    "            if logs is not None:\n",
    "                message = \"Epoch {} complete. Loss: {} Val_loss: {} \".format(epoch,\n",
    "                                                                             logs.get('loss'),\n",
    "                                                                             logs.get('val_loss'))\n",
    "            else:\n",
    "                message = \"Epoch {} complete. No loss data available.\".format(epoch)\n",
    "            #Sendmessage\n",
    "            message = client.messages.create(\n",
    "                to=\"+16178884129\", \n",
    "                from_=\"+18572142288\",\n",
    "                body=message)\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Build pre-trained V2 model\n",
    "import numpy as np\n",
    "from keras.layers import Input,Flatten,Dense,Concatenate,Dropout,concatenate,GlobalMaxPool2D,GlobalAveragePooling1D\n",
    "from keras.models import Model\n",
    "from datetime import datetime\n",
    "from keras.callbacks import TensorBoard,EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\n",
    "from keras.optimizers import Adam,Adadelta,SGD\n",
    "from keras.metrics import binary_accuracy\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import GRUCell,GRU,LSTM\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.applications.inception_v3 import InceptionV3,preprocess_input\n",
    "from keras.layers.core import Lambda\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.initializers import RandomUniform,glorot_uniform,Orthogonal\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "#from keras.applications.vgg19 import VGG19\n",
    "\n",
    "def ScaleTanh(x):\n",
    "    return 1.7159 * K.tanh(2./3 * x)\n",
    "    \n",
    "#K.set_learning_phase(1)\n",
    "def ToRGB(x):\n",
    "    max_v = tf.reduce_max(x)\n",
    "    min_v = tf.reduce_min(x)\n",
    "    max_rgb = tf.constant(255,dtype=x.dtype)\n",
    "    min_rgb = tf.constant(255//2,dtype=x.dtype)\n",
    "    x = tf.add(tf.floordiv(tf.multiply(tf.subtract(x,min_v),tf.subtract(max_rgb,min_rgb)),tf.subtract(max_v,min_v)),min_rgb)\n",
    "    return x\n",
    "def ToGreyScale(x):\n",
    "    #Divide RGB into 3\n",
    "   # scalar = tf.constant(3,dtype=x.dtype)\n",
    "    #x = tf.floordiv(x,scalar)\n",
    "    shape = x.get_shape()\n",
    "    #assume channel_last\n",
    "    mult = [[1 for d in shape[:-1]],[3]]\n",
    "    mult = [val for sublist in mult for val in sublist]\n",
    "    return tf.tile(x,mult)\n",
    "def ToNewShape(x):\n",
    "    ndim = len(x.shape)\n",
    "    if ndim == 5:\n",
    "        return tf.reverse(tf.transpose(x,[0,1,3,2,4]),[-3])\n",
    "    elif ndim == 4:\n",
    "        return tf.reverse(tf.transpose(x,[0,2,1,3]),[-3])\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected number of dims!\")\n",
    "def ReduceTimeDist(x):\n",
    "    shape = x.get_shape()\n",
    "    final = int(shape[-1] * shape[-2])\n",
    "    return tf.reshape(x,[tf.shape(x)[0],final])\n",
    "def PrintActivation(x):\n",
    "    meanv = tf.reduce_mean(x)\n",
    "    minv = tf.reduce_min(x)\n",
    "    maxv = tf.reduce_max(x)\n",
    "    #print('Mean:{},Min:{},Max:{}'.format(meanv.eval(),minv.eval(),maxv.eval()))\n",
    "    return [meanv,minv,maxv]\n",
    "def getSingleLegModel(lstm_dim=10):\n",
    "    #Single model image\n",
    "    input_img = Input(shape=(FINAL_WIDTH,FINAL_HEIGHT*3//5,CHANNELS))\n",
    "    \n",
    "    #preprocess and extract channels\n",
    "    input_img_pp = Lambda(ToRGB)(input_img)\n",
    "    input_img_pp = Lambda(ToGreyScale)(input_img_pp)\n",
    "    input_img_pp = Lambda(preprocess_input)(input_img_pp)\n",
    "    input_img_pp = Lambda(ToNewShape)(input_img_pp)\n",
    "    \n",
    "    #Load resnet\n",
    "    incep = InceptionV3(include_top=False,\n",
    "                          weights='imagenet',\n",
    "                          input_tensor=None,\n",
    "                          input_shape=(FINAL_HEIGHT*3//5,FINAL_WIDTH,3),\n",
    "                          pooling='None')\n",
    "    for l in incep.layers:\n",
    "        l.trainable=False\n",
    "\n",
    "    #Take off top and output feaatures at various levels of complexity\n",
    "    reduced_net = Model(incep.input,[incep.get_layer('mixed0').output,\n",
    "                                     incep.get_layer('mixed1').output,\n",
    "                                     incep.get_layer('mixed2').output,\n",
    "                                    incep.get_layer('mixed3').output,\n",
    "                                    incep.get_layer('mixed4').output,\n",
    "                                    incep.get_layer('mixed5').output,\n",
    "                                    incep.get_layer('mixed6').output,\n",
    "                                    incep.get_layer('mixed7').output,\n",
    "                                    incep.get_layer('mixed8').output,\n",
    "                                    incep.get_layer('mixed9').output])\n",
    "    \n",
    "    #MaxPool and concatenation\n",
    "    output = reduced_net(input_img_pp)\n",
    "    output_ary = []\n",
    "    for o in output:\n",
    "        output_ary.append(GlobalMaxPool2D()(o))\n",
    "    output = Concatenate()(output_ary)\n",
    "    #FInalize intermediate model\n",
    "    intermediate_model = Model(input_img,output)\n",
    "    \n",
    "    #Time distributed model\n",
    "    input_scan = Input(shape=(ANGLES,FINAL_WIDTH,FINAL_HEIGHT*3//5,CHANNELS))\n",
    "    sequenced_model = TimeDistributed(intermediate_model)(input_scan)\n",
    "    sequenced_model._uses_learning_phase = True\n",
    "    \n",
    "    #Finally,concatenate time dist outputs\n",
    "    out = Lambda(ReduceTimeDist)(sequenced_model)\n",
    "    \n",
    "    #Complete model\n",
    "    model = Model(input_scan,out)\n",
    "    \n",
    "    try:\n",
    "        return model\n",
    "    finally:\n",
    "        del intermediate_model,incep,reduced_net\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = getSingleLegModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_3 False (None, 16, 400, 360, 1)\n",
      "time_distributed_1 True (None, 16, 8000)\n",
      "lambda_5 True (None, 128000)\n"
     ]
    }
   ],
   "source": [
    "for l in rnn_model.layers:\n",
    "    print(l.name, l.trainable,l.output_shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use model as a feature extractor and use traditional ML to sdeterine whther features have any predictive power\n",
    "import h5py\n",
    "from keras import backend as K\n",
    "\n",
    "TEMP_DIR = 'temp'\n",
    "#Bucket with clean data\n",
    "UPLOAD_BUCKET = 'cleandhsdata' #bucket where clean data was stored\n",
    "key_id, secret_key = hfuncs.GetAWSCredentials()\n",
    "client = hfuncs.GetAWSClient(key_id,secret_key)\n",
    "bucket = client.Bucket(UPLOAD_BUCKET)\n",
    "\n",
    "#Initialize train sequencer\n",
    "mode =\"train_scan\"\n",
    "num_batches_train = (sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1)//BATCH_SIZE #train,test,val root directories have their own keys\n",
    "#num_batches = 660//BATCH_SIZE\n",
    "train_seq = LegScanSequencer(num_batches_train,UPLOAD_BUCKET,mode=mode)\n",
    "#Initialize validation sequencer\n",
    "mode = \"val_scan\"\n",
    "num_batches_val = (sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1)//BATCH_SIZE #train,test,val root directories have their own keys\n",
    "val_seq = LegScanSequencer(num_batches_val,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "\n",
    "#Create function that creates data set for given layer\n",
    "def CreateFeatureDataSet(model,dir_name = 'fullfeatureextraction',max_batches=1200):\n",
    "    #Get model and output size\n",
    "    output_size = model.output_shape[1]\n",
    "    \n",
    "    #Variables to iterate over\n",
    "    #modes = ['train','val']\n",
    "    #num_batches = [num_batches_train,num_batches_val]\n",
    "    #generators = [train_seq,val_seq]\n",
    "    modes = ['val']\n",
    "    num_batches = [num_batches_val]\n",
    "    generators = [val_seq]\n",
    "    \n",
    "    \n",
    "    for mode,num_b,gen in zip(modes,num_batches,generators):\n",
    "        #Initialize dataset array\n",
    "        X_d = np.zeros((min(num_b,max_batches),output_size))\n",
    "        y_d = np.zeros((min(num_b,max_batches)))\n",
    "\n",
    "        #For every item in train generator, transform data and store in dataset array\n",
    "        for i in range(min(num_b,max_batches)):\n",
    "            print(\"Storing {} in {} set...\".format(i,mode))\n",
    "            X, y,s = gen.__getitem__(i)\n",
    "            X = model.predict(X)\n",
    "            X_d[i,:] = X.flatten()\n",
    "            y_d[i] = y[0,0]\n",
    "            i += 1\n",
    "\n",
    "        #Store data set in s3\n",
    "        key_suffix = \"{}_data.hdf5\".format(mode)\n",
    "        filename = os.path.join(TEMP_DIR,key_suffix)\n",
    "        key = \"{}/{}\".format(dir_name,key_suffix)\n",
    "\n",
    "        #Save in local hdf5 file\n",
    "        with h5py.File(filename,\"w\") as f:\n",
    "            dset = f.create_dataset('features',data=X_d)\n",
    "            dset2 = f.create_dataset('labels',data=y_d)\n",
    "\n",
    "        #Upload file to bucket, then delete\n",
    "        try:\n",
    "            bucket.upload_file(Filename=filename,Key=key)\n",
    "            print(\"Completed {} upload\".format(mode))\n",
    "        finally:\n",
    "            print(\"Done\")\n",
    "            #os.remove(filename)\n",
    "\n",
    "        #Delete train arrays to save memory\n",
    "        del X_d,y_d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing 0 in val set...\n",
      "Storing 1 in val set...\n",
      "Storing 2 in val set...\n",
      "Storing 3 in val set...\n",
      "Storing 4 in val set...\n",
      "Storing 5 in val set...\n",
      "Storing 6 in val set...\n",
      "Storing 7 in val set...\n",
      "Storing 8 in val set...\n",
      "Storing 9 in val set...\n",
      "Storing 10 in val set...\n",
      "Storing 11 in val set...\n",
      "Storing 12 in val set...\n",
      "Storing 13 in val set...\n",
      "Storing 14 in val set...\n",
      "Storing 15 in val set...\n",
      "Storing 16 in val set...\n",
      "Storing 17 in val set...\n",
      "Storing 18 in val set...\n",
      "Storing 19 in val set...\n",
      "Storing 20 in val set...\n",
      "Storing 21 in val set...\n",
      "Storing 22 in val set...\n",
      "Storing 23 in val set...\n",
      "Storing 24 in val set...\n",
      "Storing 25 in val set...\n",
      "Storing 26 in val set...\n",
      "Storing 27 in val set...\n",
      "Storing 28 in val set...\n",
      "Storing 29 in val set...\n",
      "Storing 30 in val set...\n",
      "Storing 31 in val set...\n",
      "Storing 32 in val set...\n",
      "Storing 33 in val set...\n",
      "Storing 34 in val set...\n",
      "Storing 35 in val set...\n",
      "Storing 36 in val set...\n",
      "Storing 37 in val set...\n",
      "Storing 38 in val set...\n",
      "Storing 39 in val set...\n",
      "Storing 40 in val set...\n",
      "Storing 41 in val set...\n",
      "Storing 42 in val set...\n",
      "Storing 43 in val set...\n",
      "Storing 44 in val set...\n",
      "Storing 45 in val set...\n",
      "Storing 46 in val set...\n",
      "Storing 47 in val set...\n",
      "Storing 48 in val set...\n",
      "Storing 49 in val set...\n",
      "Storing 50 in val set...\n",
      "Storing 51 in val set...\n",
      "Storing 52 in val set...\n",
      "Storing 53 in val set...\n",
      "Storing 54 in val set...\n",
      "Storing 55 in val set...\n",
      "Storing 56 in val set...\n",
      "Storing 57 in val set...\n",
      "Storing 58 in val set...\n",
      "Storing 59 in val set...\n",
      "Storing 60 in val set...\n",
      "Storing 61 in val set...\n",
      "Storing 62 in val set...\n",
      "Storing 63 in val set...\n",
      "Storing 64 in val set...\n",
      "Storing 65 in val set...\n",
      "Storing 66 in val set...\n",
      "Storing 67 in val set...\n",
      "Storing 68 in val set...\n",
      "Storing 69 in val set...\n",
      "Storing 70 in val set...\n",
      "Storing 71 in val set...\n",
      "Storing 72 in val set...\n",
      "Storing 73 in val set...\n",
      "Storing 74 in val set...\n",
      "Storing 75 in val set...\n",
      "Storing 76 in val set...\n",
      "Storing 77 in val set...\n",
      "Storing 78 in val set...\n",
      "Storing 79 in val set...\n",
      "Storing 80 in val set...\n",
      "Storing 81 in val set...\n",
      "Storing 82 in val set...\n",
      "Storing 83 in val set...\n",
      "Storing 84 in val set...\n",
      "Storing 85 in val set...\n",
      "Storing 86 in val set...\n",
      "Storing 87 in val set...\n",
      "Storing 88 in val set...\n",
      "Storing 89 in val set...\n",
      "Storing 90 in val set...\n",
      "Storing 91 in val set...\n",
      "Storing 92 in val set...\n",
      "Storing 93 in val set...\n",
      "Storing 94 in val set...\n",
      "Storing 95 in val set...\n",
      "Storing 96 in val set...\n",
      "Storing 97 in val set...\n",
      "Storing 98 in val set...\n",
      "Storing 99 in val set...\n",
      "Storing 100 in val set...\n",
      "Storing 101 in val set...\n",
      "Storing 102 in val set...\n",
      "Storing 103 in val set...\n",
      "Storing 104 in val set...\n",
      "Storing 105 in val set...\n",
      "Storing 106 in val set...\n",
      "Storing 107 in val set...\n",
      "Storing 108 in val set...\n",
      "Storing 109 in val set...\n",
      "Storing 110 in val set...\n",
      "Storing 111 in val set...\n",
      "Storing 112 in val set...\n",
      "Storing 113 in val set...\n",
      "Storing 114 in val set...\n",
      "Storing 115 in val set...\n",
      "Storing 116 in val set...\n",
      "Storing 117 in val set...\n",
      "Storing 118 in val set...\n",
      "Storing 119 in val set...\n",
      "Storing 120 in val set...\n",
      "Storing 121 in val set...\n",
      "Storing 122 in val set...\n",
      "Storing 123 in val set...\n",
      "Storing 124 in val set...\n",
      "Storing 125 in val set...\n",
      "Storing 126 in val set...\n",
      "Storing 127 in val set...\n",
      "Storing 128 in val set...\n",
      "Storing 129 in val set...\n",
      "Storing 130 in val set...\n",
      "Storing 131 in val set...\n",
      "Storing 132 in val set...\n",
      "Storing 133 in val set...\n",
      "Storing 134 in val set...\n",
      "Storing 135 in val set...\n",
      "Storing 136 in val set...\n",
      "Storing 137 in val set...\n",
      "Storing 138 in val set...\n",
      "Storing 139 in val set...\n",
      "Storing 140 in val set...\n",
      "Storing 141 in val set...\n",
      "Storing 142 in val set...\n",
      "Storing 143 in val set...\n",
      "Storing 144 in val set...\n",
      "Storing 145 in val set...\n",
      "Storing 146 in val set...\n",
      "Storing 147 in val set...\n",
      "Storing 148 in val set...\n",
      "Storing 149 in val set...\n",
      "Storing 150 in val set...\n",
      "Storing 151 in val set...\n",
      "Storing 152 in val set...\n",
      "Storing 153 in val set...\n",
      "Storing 154 in val set...\n",
      "Storing 155 in val set...\n",
      "Storing 156 in val set...\n",
      "Storing 157 in val set...\n",
      "Storing 158 in val set...\n",
      "Storing 159 in val set...\n",
      "Storing 160 in val set...\n",
      "Storing 161 in val set...\n",
      "Storing 162 in val set...\n",
      "Storing 163 in val set...\n",
      "Storing 164 in val set...\n",
      "Storing 165 in val set...\n",
      "Storing 166 in val set...\n",
      "Storing 167 in val set...\n",
      "Storing 168 in val set...\n",
      "Storing 169 in val set...\n",
      "Storing 170 in val set...\n",
      "Storing 171 in val set...\n",
      "Storing 172 in val set...\n",
      "Storing 173 in val set...\n",
      "Storing 174 in val set...\n",
      "Storing 175 in val set...\n",
      "Storing 176 in val set...\n",
      "Storing 177 in val set...\n",
      "Storing 178 in val set...\n",
      "Storing 179 in val set...\n",
      "Storing 180 in val set...\n",
      "Storing 181 in val set...\n",
      "Storing 182 in val set...\n",
      "Storing 183 in val set...\n",
      "Storing 184 in val set...\n",
      "Storing 185 in val set...\n",
      "Storing 186 in val set...\n",
      "Storing 187 in val set...\n",
      "Storing 188 in val set...\n",
      "Storing 189 in val set...\n",
      "Storing 190 in val set...\n",
      "Storing 191 in val set...\n",
      "Storing 192 in val set...\n",
      "Storing 193 in val set...\n",
      "Storing 194 in val set...\n",
      "Storing 195 in val set...\n",
      "Storing 196 in val set...\n",
      "Storing 197 in val set...\n",
      "Storing 198 in val set...\n",
      "Storing 199 in val set...\n",
      "Storing 200 in val set...\n",
      "Storing 201 in val set...\n",
      "Storing 202 in val set...\n",
      "Storing 203 in val set...\n",
      "Storing 204 in val set...\n",
      "Storing 205 in val set...\n",
      "Storing 206 in val set...\n",
      "Storing 207 in val set...\n",
      "Storing 208 in val set...\n",
      "Storing 209 in val set...\n",
      "Storing 210 in val set...\n",
      "Storing 211 in val set...\n",
      "Storing 212 in val set...\n",
      "Storing 213 in val set...\n",
      "Storing 214 in val set...\n",
      "Storing 215 in val set...\n",
      "Storing 216 in val set...\n",
      "Storing 217 in val set...\n",
      "Storing 218 in val set...\n",
      "Storing 219 in val set...\n",
      "Storing 220 in val set...\n",
      "Storing 221 in val set...\n",
      "Storing 222 in val set...\n",
      "Storing 223 in val set...\n",
      "Completed val upload\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "CreateFeatureDataSet(rnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1204800.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1200/5*5020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DHSenv_3.5]",
   "language": "python",
   "name": "conda-env-DHSenv_3.5-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
