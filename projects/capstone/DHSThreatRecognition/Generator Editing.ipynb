{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import HelperFuncs as hfuncs\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.data_utils import Sequence\n",
    "import h5py\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to first try training a CNN on the individual images.\n",
    "We will be using binary cross entropy across the 17 regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "FINAL_WIDTH = 400\n",
    "FINAL_HEIGHT = 600\n",
    "CHANNELS = 1\n",
    "ZONES = 17\n",
    "\n",
    "class Sequencer(Sequence):\n",
    "    idx_dict={}\n",
    "    \n",
    "    def __init__(self,num_batches,bucket_name,mode=\"train\"):\n",
    "        self.num_batches = num_batches\n",
    "        self.bucket_name = bucket_name\n",
    "        self.mode = mode\n",
    "        self.key_id, self.secret_key = hfuncs.GetAWSCredentials()\n",
    "        self.mode = mode\n",
    "        \n",
    "        order = np.arange(self.num_batches)\n",
    "        np.random.seed(0)\n",
    "        np.random.shuffle(order)\n",
    "        k = 0\n",
    "        for k in range(len(order)):\n",
    "            self.idx_dict[k]=order[k]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "    def __getitem__(self,idx):\n",
    "        #Get Client\n",
    "        client = hfuncs.GetAWSClient(self.key_id,self.secret_key)\n",
    "        bucket = client.Bucket(self.bucket_name)\n",
    "        \n",
    "        idx = self.idx_dict[idx] #Mix up order of the batches \n",
    "        \n",
    "        path = \"temp/{}/batch_{}.hdf5\".format(self.mode,idx)\n",
    "        key = \"{}/batch_{}.hdf5\".format(self.mode,idx)\n",
    "        bucket.download_file(Key=key,Filename=path)\n",
    "        \n",
    "        f = h5py.File(path,\"r\")\n",
    "        try:\n",
    "            return f['/image'].value, f['/labels'].value\n",
    "        finally:\n",
    "            f.close()\n",
    "            os.remove(path)        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 75, 38, 64)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D , AveragePooling2D,Flatten\n",
    "from keras.models import Model\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import keras\n",
    "\n",
    "#Build Basic model\n",
    "input_img = Input(shape=(FINAL_WIDTH,FINAL_HEIGHT,CHANNELS))\n",
    "input_norm = BatchNormalization(axis=3)(input_img)\n",
    "\n",
    "pooling_1 = MaxPooling2D((2,2),padding='same')(input_img)\n",
    "\n",
    "tower_1 = Conv2D(16, (1, 1), padding='same', activation='relu')(pooling_1)\n",
    "tower_1 = Conv2D(16, (3, 3), padding='same', activation='relu')(tower_1)\n",
    "\n",
    "tower_2 = Conv2D(16, (1, 1), padding='same', activation='relu')(pooling_1)\n",
    "tower_2 = Conv2D(16, (5, 5), padding='same', activation='relu')(tower_2)\n",
    "\n",
    "tower_3 = MaxPooling2D((2, 2), strides=(1, 1), padding='same')(pooling_1)\n",
    "tower_3 = Conv2D(16, (1, 1), padding='same', activation='relu')(tower_3)\n",
    "\n",
    "output_inception = keras.layers.concatenate([tower_1, tower_2, tower_3], axis=1)\n",
    "\n",
    "pooling_2 = MaxPooling2D((6,4),padding='same')(output_inception)\n",
    "#pooling_2 = BatchNormalization(axis=3)(pooling_2)\n",
    "\n",
    "tower_1_2 = Conv2D(32, (1, 1), padding='same', activation='relu')(pooling_2)\n",
    "tower_1_2 = Conv2D(32, (3, 3), padding='same', activation='relu')(tower_1_2)\n",
    "\n",
    "tower_2_2 = Conv2D(32, (1, 1), padding='same', activation='relu')(pooling_2)\n",
    "tower_2_2 = Conv2D(32, (5, 5), padding='same', activation='relu')(tower_2_2)\n",
    "\n",
    "tower_3_2 = MaxPooling2D((2, 2), strides=(1, 1), padding='same')(pooling_2)\n",
    "tower_3_2 = Conv2D(32, (1, 1), padding='same', activation='relu')(tower_3_2)\n",
    "\n",
    "output_inception_2 = keras.layers.concatenate([tower_1_2, tower_2_2, tower_3_2], axis=1)\n",
    "\n",
    "#output_inception_2 = Dropout(0.10)(output_inception_2)\n",
    "output_inception_2 = MaxPooling2D((4, 2),strides=(4,2), padding='same')(output_inception_2)\n",
    "\n",
    "conv_3 = Conv2D(64, (1, 1), padding='same', activation='relu')(output_inception_2)\n",
    "last = Flatten()(conv_3)\n",
    "\n",
    "#List of independent guesses for each zone\n",
    "output_nodes = []\n",
    "for i in range(ZONES):\n",
    "    output_nodes.append(Dense(1,activation='sigmoid')(last))\n",
    "\n",
    "out = keras.layers.concatenate(output_nodes)\n",
    "print(conv_3.shape)\n",
    "multi_label_model = Model(input_img, out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4/4 [==============================] - 13s - loss: 5.6911 - binary_accuracy: 0.6419    \n",
      "Epoch 2/20\n",
      "4/4 [==============================] - 12s - loss: 4.7629 - binary_accuracy: 0.7022    \n",
      "Epoch 3/20\n",
      "4/4 [==============================] - 12s - loss: 4.5258 - binary_accuracy: 0.7169    \n",
      "Epoch 4/20\n",
      "4/4 [==============================] - 12s - loss: 5.0100 - binary_accuracy: 0.6868    \n",
      "Epoch 5/20\n",
      "4/4 [==============================] - 12s - loss: 4.3779 - binary_accuracy: 0.7257    \n",
      "Epoch 6/20\n",
      "4/4 [==============================] - 12s - loss: 3.7404 - binary_accuracy: 0.7662    \n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-2:\n",
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-1:\n",
      "Process ForkPoolWorker-4:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/anaconda3/envs/DHSenv_3.5/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/envs/DHSenv_3.5/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/envs/DHSenv_3.5/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/DHSenv_3.5/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/envs/DHSenv_3.5/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ubuntu/anaconda3/envs/DHSenv_3.5/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/DHSenv_3.5/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/DHSenv_3.5/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/ubuntu/anaconda3/envs/DHSenv_3.5/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ubuntu/anaconda3/envs/DHSenv_3.5/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/ubuntu/anaconda3/envs/DHSenv_3.5/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/ubuntu/anaconda3/envs/DHSenv_3.5/lib/python3.5/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/ubuntu/anaconda3/envs/DHSenv_3.5/lib/python3.5/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/home/ubuntu/anaconda3/envs/DHSenv_3.5/lib/python3.5/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/ubuntu/anaconda3/envs/DHSenv_3.5/lib/python3.5/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/ubuntu/anaconda3/envs/DHSenv_3.5/lib/python3.5/site-packages/keras/utils/data_utils.py\", line 371, in get_index\n",
      "    return ds[i]\n",
      "  File \"/home/ubuntu/anaconda3/envs/DHSenv_3.5/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ubuntu/anaconda3/envs/DHSenv_3.5/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"<ipython-input-2-a319a5404fdc>\", line 42, in __getitem__\n",
      "    os.remove(path)\n",
      "  File \"/home/ubuntu/anaconda3/envs/DHSenv_3.5/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/ubuntu/anaconda3/envs/DHSenv_3.5/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ubuntu/anaconda3/envs/DHSenv_3.5/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.optimizers import SGD\n",
    "from keras import metrics\n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "\n",
    "#Set up learning\n",
    "\n",
    "#Optimization\n",
    "lr=0.0005 \n",
    "beta_1=0.80\n",
    "beta_2=0.97\n",
    "\n",
    "adam_op = optimizers.Adam(lr,beta_1,beta_2)\n",
    "multi_label_model.compile(optimizer=adam_op,\n",
    "                          metrics=[metrics.binary_accuracy],\n",
    "                         loss= 'binary_crossentropy')\n",
    "\n",
    "#Tensorboard\n",
    "x = datetime.today()\n",
    "stamp = \"{}-{}-{}_{}:{}:{}_lr-{}_b1-{}_b2-{}\".format(x.year,x.month,\n",
    "                                                     x.day,x.hour,x.minute,\n",
    "                                                     x.second,lr,beta_1,beta_2)\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(stamp),histogram_freq=0,batch_size=BATCH_SIZE,\n",
    "                          write_grads=True,write_images=True)\n",
    "\n",
    "\n",
    "\n",
    "#Generators and fit\n",
    "\n",
    "#Bucket with clean data\n",
    "UPLOAD_BUCKET = 'cleandhsdata' #bucket where clean data was stored\n",
    "key_id, secret_key = hfuncs.GetAWSCredentials()\n",
    "client = hfuncs.GetAWSClient(key_id,secret_key)\n",
    "bucket = client.Bucket(UPLOAD_BUCKET)\n",
    "\n",
    "#Initialize train sequencer\n",
    "mode =\"train\"\n",
    "num_batches = sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1 #train,test,val root directories have their own keys\n",
    "train_seq = Sequencer(num_batches,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "#Initialize validation sequencer\n",
    "mode = \"val\"\n",
    "num_batches = sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1 #train,test,val root directories have their own keys\n",
    "val_seq = Sequencer(num_batches,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "\n",
    "hist = multi_label_model.fit_generator(train_seq,\n",
    "                                       steps_per_epoch=4,\n",
    "                                       epochs=20,callbacks=[tensorboard],\n",
    "                                      use_multiprocessing =True,workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (DSHenv_3.5)",
   "language": "python",
   "name": "dhsenv_3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
