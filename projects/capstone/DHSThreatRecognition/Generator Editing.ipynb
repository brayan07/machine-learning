{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import HelperFuncs as hfuncs\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.data_utils import Sequence\n",
    "import h5py\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to first try training a CNN on the individual images.\n",
    "We will be using binary cross entropy across the 17 regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "FINAL_WIDTH = 400\n",
    "FINAL_HEIGHT = 600\n",
    "CHANNELS = 1\n",
    "ZONES = 17\n",
    "\n",
    "class Sequencer(Sequence):\n",
    "    idx_dict={}\n",
    "    \n",
    "    def __init__(self,num_batches,bucket_name,mode=\"train\"):\n",
    "        self.num_batches = num_batches\n",
    "        self.bucket_name = bucket_name\n",
    "        self.mode = mode\n",
    "        self.key_id, self.secret_key = hfuncs.GetAWSCredentials()\n",
    "        self.mode = mode\n",
    "        \n",
    "        order = np.arange(self.num_batches)\n",
    "        np.random.seed(0)\n",
    "        np.random.shuffle(order)\n",
    "        k = 0\n",
    "        for k in range(len(order)):\n",
    "            self.idx_dict[k]=order[k]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "    def __getitem__(self,idx):\n",
    "        #Get Client\n",
    "        client = hfuncs.GetAWSClient(self.key_id,self.secret_key)\n",
    "        bucket = client.Bucket(self.bucket_name)\n",
    "        \n",
    "        idx = self.idx_dict[idx] #Mix up order of the batches \n",
    "        \n",
    "        path = \"temp/{}/batch_{}.hdf5\".format(self.mode,idx)\n",
    "        key = \"{}/batch_{}.hdf5\".format(self.mode,idx)\n",
    "        bucket.download_file(Key=key,Filename=path)\n",
    "        \n",
    "        f = h5py.File(path,\"r\")\n",
    "        try:\n",
    "            return f['/image'].value, f['/labels'].value\n",
    "        finally:\n",
    "            f.close()\n",
    "            os.remove(path)\n",
    "    def on_epoch_end(self):\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how an existing architecture would do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from inception_resnet_v1 import InceptionResNetV1\n",
    "from keras.layers import Input,Flatten,Dense,Concatenate\n",
    "from keras.models import Model\n",
    "import keras\n",
    "\n",
    "\n",
    "input_img = Input(shape=(FINAL_WIDTH,FINAL_HEIGHT,CHANNELS))\n",
    "incep = InceptionResNetV1(include_top=False,\n",
    "                          weights=None,\n",
    "                          input_tensor=input_img,\n",
    "                         pooling='max')\n",
    "last = incep.output\n",
    "#last = Flatten()(last)\n",
    "\n",
    "#List of independent guesses for each zone\n",
    "output_nodes = []\n",
    "for i in range(ZONES):\n",
    "    output_nodes.append(Dense(1,activation='sigmoid')(last))\n",
    "\n",
    "out = keras.layers.concatenate(output_nodes)\n",
    "\n",
    "multi_label_model = Model(input_img, out)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from keras.layers import Input,Flatten\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "\n",
    "input_img = Input(shape=(FINAL_WIDTH,FINAL_HEIGHT,CHANNELS))\n",
    "resnet = InceptionResNetV2(include_top=False,weights='imagenet',input_tensor=input_img,\n",
    "                           input_shape =(FINAL_WIDTH,FINAL_HEIGHT,CHANNELS))\n",
    "\n",
    "last = Flatten(resnet)\n",
    "#List of independent guesses for each zone\n",
    "output_nodes = []\n",
    "for i in range(ZONES):\n",
    "    output_nodes.append(Dense(1,activation='sigmoid')(last))\n",
    "\n",
    "out = keras.layers.concatenate(output_nodes)\n",
    "resnet_multi_label_model = Model(input_img, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'multi_label_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8b132f3360bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0msgd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnesterov\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m multi_label_model.compile(optimizer=sgd,\n\u001b[0m\u001b[1;32m     16\u001b[0m                           \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_accuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                          loss= 'binary_crossentropy')\n",
      "\u001b[0;31mNameError\u001b[0m: name 'multi_label_model' is not defined"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.optimizers import SGD\n",
    "from keras import metrics\n",
    "\n",
    "#Set up learning\n",
    "\n",
    "#Optimization\n",
    "lr=0.0005 \n",
    "momentum=0.5\n",
    "nes = True\n",
    "description = \"SGD_globalMaxPooling_noDropout\"\n",
    "\n",
    "sgd = SGD(lr,momentum=momentum,nesterov=nes)\n",
    "multi_label_model.compile(optimizer=sgd,\n",
    "                          metrics=[metrics.binary_accuracy],\n",
    "                         loss= 'binary_crossentropy')\n",
    "\n",
    "#Tensorboard\n",
    "x = datetime.today()\n",
    "stamp = \"{}-{}-{}_{}:{}:{}_lr-{}_b1-{}_b2-{}_{}\".format(x.year,x.month,\n",
    "                                                     x.day,x.hour,x.minute,\n",
    "                                                     x.second,lr,momentum,nes,description)\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(stamp),histogram_freq=0,batch_size=BATCH_SIZE,\n",
    "                          write_grads=True,write_images=True)\n",
    "\n",
    "\n",
    "\n",
    "#Generators and fit\n",
    "\n",
    "#Bucket with clean data\n",
    "UPLOAD_BUCKET = 'cleandhsdata' #bucket where clean data was stored\n",
    "key_id, secret_key = hfuncs.GetAWSCredentials()\n",
    "client = hfuncs.GetAWSClient(key_id,secret_key)\n",
    "bucket = client.Bucket(UPLOAD_BUCKET)\n",
    "\n",
    "#Initialize train sequencer\n",
    "mode =\"train\"\n",
    "num_batches = sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1 #train,test,val root directories have their own keys\n",
    "train_seq = Sequencer(num_batches,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "#Initialize validation sequencer\n",
    "mode = \"val\"\n",
    "num_batches = sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1 #train,test,val root directories have their own keys\n",
    "val_seq = Sequencer(num_batches,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "\n",
    "hist = multi_label_model.fit_generator(train_seq,\n",
    "                                       steps_per_epoch=20,\n",
    "                                       epochs=20,\n",
    "                                       validation_data = val_seq,\n",
    "                                       validation_steps = 140,\n",
    "                                       callbacks=[tensorboard],\n",
    "                                      use_multiprocessing =True,workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from inception_resnet_v1 import InceptionResNetV1\n",
    "from keras.layers import Input,Flatten,Dense,Concatenate,Dropout\n",
    "from keras.models import Model\n",
    "import keras\n",
    "from datetime import datetime\n",
    "from keras.callbacks import TensorBoard,EarlyStopping\n",
    "from keras.optimizers import SGD\n",
    "from keras import metrics\n",
    "\n",
    "def trainCNNRes(alpha,lr,momentum,pooling,dropout_rate,description=\"\"):\n",
    "    \n",
    "    #ResNetv1\n",
    "    input_img = Input(shape=(FINAL_WIDTH,FINAL_HEIGHT,CHANNELS))\n",
    "    incep = InceptionResNetV1(include_top=False,\n",
    "                              weights=None,\n",
    "                              input_tensor=input_img,\n",
    "                             pooling=pooling,\n",
    "                             alpha=alpha)\n",
    "    #Apply dropout specified with specified rate\n",
    "    last = incep.output\n",
    "    last = Dropout(dropout_rate,seed=0)(last)\n",
    "    \n",
    "    #List of independent guesses for each zone\n",
    "    output_nodes = []\n",
    "    for i in range(ZONES):\n",
    "        output_nodes.append(Dense(1,activation='sigmoid')(last))\n",
    "\n",
    "    out = keras.layers.concatenate(output_nodes)\n",
    "    \n",
    "    #FInish model\n",
    "    multi_label_model = Model(input_img, out)\n",
    "\n",
    "    \n",
    "    #optimizer\n",
    "    nes=True\n",
    "    sgd = SGD(lr,momentum=momentum,nesterov=nes)\n",
    "    del incep\n",
    "    multi_label_model.compile(optimizer=sgd,\n",
    "                              metrics=[metrics.binary_accuracy],\n",
    "                             loss= 'binary_crossentropy')\n",
    "\n",
    "    #Tensorboard\n",
    "    x = datetime.today()\n",
    "    stamp = \"{}-{}-{}_{}:{}:{}_lr-{}_mom-{}_alpha-{}_pooling-{}_dropout-{}\".format(x.year,x.month,\n",
    "                                                         x.day,x.hour,x.minute,\n",
    "                                                         x.second,lr,momentum,alpha,pooling,dropout_rate)\n",
    "    tensorboard = TensorBoard(log_dir=\"logs/{}\".format(stamp),histogram_freq=0,batch_size=BATCH_SIZE,\n",
    "                              write_grads=False,write_images=True)\n",
    "\n",
    "    #Generators and fit\n",
    "\n",
    "    #Bucket with clean data\n",
    "    UPLOAD_BUCKET = 'cleandhsdata' #bucket where clean data was stored\n",
    "    key_id, secret_key = hfuncs.GetAWSCredentials()\n",
    "    client = hfuncs.GetAWSClient(key_id,secret_key)\n",
    "    bucket = client.Bucket(UPLOAD_BUCKET)\n",
    "\n",
    "    #Initialize train sequencer\n",
    "    mode =\"train\"\n",
    "    num_batches = sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1 #train,test,val root directories have their own keys\n",
    "    train_seq = Sequencer(num_batches,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "    #Initialize validation sequencer\n",
    "    mode = \"val\"\n",
    "    num_batches = sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1 #train,test,val root directories have their own keys\n",
    "    val_seq = Sequencer(num_batches,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "    #Early stopping callback\n",
    "    estop = EarlyStopping(monitor='val_loss',min_delta=0.002,patience=3)\n",
    "    \n",
    "    hist = multi_label_model.fit_generator(train_seq,\n",
    "                                           steps_per_epoch=2100,\n",
    "                                           epochs=100,\n",
    "                                           validation_data = val_seq,\n",
    "                                           validation_steps = 600,\n",
    "                                           callbacks=[tensorboard,estop],\n",
    "                                          use_multiprocessing =True,workers=2)\n",
    "    return hist,multi_label_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from inception_resnet_v1 import InceptionResNetV1\n",
    "from keras.layers import Input,Flatten,Dense,Concatenate,Dropout\n",
    "from keras.models import Model\n",
    "import keras\n",
    "from datetime import datetime\n",
    "from keras.callbacks import TensorBoard,EarlyStopping\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras import metrics\n",
    "\n",
    "def trainCNNResAdam(alpha,lr,beta1,beta2,pooling,dropout_rate,description=\"\"):\n",
    "    \n",
    "    #ResNetv1\n",
    "    input_img = Input(shape=(FINAL_WIDTH,FINAL_HEIGHT,CHANNELS))\n",
    "    incep = InceptionResNetV1(include_top=False,\n",
    "                              weights=None,\n",
    "                              input_tensor=input_img,\n",
    "                             pooling=pooling,\n",
    "                             alpha=alpha)\n",
    "    #Apply dropout specified with specified rate\n",
    "    last = incep.output\n",
    "    last = Dropout(dropout_rate,seed=0)(last)\n",
    "    \n",
    "    #List of independent guesses for each zone\n",
    "    output_nodes = []\n",
    "    for i in range(ZONES):\n",
    "        output_nodes.append(Dense(1,activation='sigmoid')(last))\n",
    "\n",
    "    out = keras.layers.concatenate(output_nodes)\n",
    "    \n",
    "    #FInish model\n",
    "    multi_label_model = Model(input_img, out)\n",
    "\n",
    "    \n",
    "    #optimizer\n",
    "    adam = Adam(lr,beta_1=beta1,beta_2=beta2)\n",
    "    multi_label_model.compile(optimizer=adam,\n",
    "                              metrics=[metrics.binary_accuracy],\n",
    "                             loss= 'binary_crossentropy')\n",
    "\n",
    "    #Tensorboard\n",
    "    x = datetime.today()\n",
    "    stamp = \"{}-{}-{}_{}:{}:{}_lr-{}_beta1-{}_beta2-{}_alpha-{}_pooling-{}_dropout-{}\".format(x.year,x.month,\n",
    "                                                         x.day,x.hour,x.minute,\n",
    "                                                         x.second,lr,beta1,beta2,alpha,pooling,dropout_rate)\n",
    "    tensorboard = TensorBoard(log_dir=\"logs/{}\".format(stamp),histogram_freq=0,batch_size=BATCH_SIZE,\n",
    "                              write_grads=False,write_images=True)\n",
    "\n",
    "    #Generators and fit\n",
    "\n",
    "    #Bucket with clean data\n",
    "    UPLOAD_BUCKET = 'cleandhsdata' #bucket where clean data was stored\n",
    "    key_id, secret_key = hfuncs.GetAWSCredentials()\n",
    "    client = hfuncs.GetAWSClient(key_id,secret_key)\n",
    "    bucket = client.Bucket(UPLOAD_BUCKET)\n",
    "\n",
    "    #Initialize train sequencer\n",
    "    mode =\"train\"\n",
    "    num_batches = sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1 #train,test,val root directories have their own keys\n",
    "    train_seq = Sequencer(num_batches,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "    #Initialize validation sequencer\n",
    "    mode = \"val\"\n",
    "    num_batches = sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1 #train,test,val root directories have their own keys\n",
    "    val_seq = Sequencer(num_batches,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "    #Early stopping callback\n",
    "    estop = EarlyStopping(monitor='val_loss',min_delta=0.002,patience=2)\n",
    "    \n",
    "    hist = multi_label_model.fit_generator(train_seq,\n",
    "                                           steps_per_epoch=2100,\n",
    "                                           epochs=100,\n",
    "                                           validation_data = val_seq,\n",
    "                                           validation_steps = 600,\n",
    "                                           callbacks=[tensorboard,estop],\n",
    "                                          use_multiprocessing =True,workers=2)\n",
    "    return hist,multi_label_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypermarameter Oprimization (Later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from scipy import stats\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "import pickle\n",
    "\n",
    "lower=-1\n",
    "upper=-4\n",
    "mu =-2.5\n",
    "sigma = 0.5\n",
    "\n",
    "log_lr = stats.truncnorm((lower-mu)/sigma,(upper-mu)/sigma,loc=mu,scale=sigma)\n",
    "\n",
    "lower=0\n",
    "upper=1\n",
    "mu =0.5\n",
    "sigma =0.10\n",
    "\n",
    "momentum= stats.truncnorm((lower-mu)/sigma,(upper-mu)/sigma,loc=mu,scale=sigma)\n",
    "\n",
    "alpha=[2]\n",
    "\n",
    "pooling = ['max','average']\n",
    "\n",
    "lower=0\n",
    "upper=0.5\n",
    "mu =0\n",
    "sigma =0.20\n",
    "\n",
    "dropout = stats.truncnorm((lower-mu)/sigma,(upper-mu)/sigma,loc=mu,scale=sigma)\n",
    "\n",
    "param_grid = {'log_lr':log_lr,'momentum':momentum,\n",
    "             'alpha':alpha,'pooling':pooling,\n",
    "             'dropout':dropout}\n",
    "param_list = list(ParameterSampler(param_grid,n_iter=25,random_state=0))\n",
    "\n",
    "rounded_list=[]\n",
    "for d in param_list:\n",
    "    dropout = np.around(d['dropout'],decimals=2)\n",
    "    momentum = np.around(d['momentum'],decimals=2)\n",
    "    lr = np.around(10**d['log_lr'],decimals=4)\n",
    "    pooling = d['pooling']\n",
    "    alpha = d['alpha']\n",
    "    rounded_list.append({'lr':lr,\n",
    "                         'momentum':momentum,\n",
    "                         'alpha':alpha,\n",
    "                         'pooling':pooling,\n",
    "                         'dropout':dropout})\n",
    "with open(\"param_list.pickle\",\"wb\") as f:\n",
    "    pickle.dump(rounded_list,f)\n",
    "\n",
    "#25 Models\n",
    "hist_list = []\n",
    "for d in rounded_list:\n",
    "    try:\n",
    "        hist_list.append(trainCNNRes(alpha=d['alpha'],\n",
    "                                     lr=d['lr'],\n",
    "                                     momentum=d['momentum'],\n",
    "                                     pooling=d['pooling'],\n",
    "                                     dropout_rate=d['dropout'],\n",
    "                                     description=\"\"))\n",
    "    except:\n",
    "        print(\"Something went wrong\")\n",
    "        hist_list.append(\"Failure\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2100/2100 [==============================] - 2156s - loss: 0.8138 - binary_accuracy: 0.8415 - val_loss: 0.5572 - val_binary_accuracy: 0.8994\n",
      "Epoch 2/100\n",
      "2100/2100 [==============================] - 2155s - loss: 0.6491 - binary_accuracy: 0.8499 - val_loss: 0.4965 - val_binary_accuracy: 0.8994\n",
      "Epoch 3/100\n",
      "2100/2100 [==============================] - 2164s - loss: 0.5770 - binary_accuracy: 0.8554 - val_loss: 0.4586 - val_binary_accuracy: 0.8994\n",
      "Epoch 4/100\n",
      "2100/2100 [==============================] - 2163s - loss: 0.5270 - binary_accuracy: 0.8608 - val_loss: 0.4290 - val_binary_accuracy: 0.8994\n",
      "Epoch 5/100\n",
      "2100/2100 [==============================] - 2216s - loss: 0.4965 - binary_accuracy: 0.8648 - val_loss: 0.4074 - val_binary_accuracy: 0.8994\n",
      "Epoch 6/100\n",
      "2100/2100 [==============================] - 2164s - loss: 0.4703 - binary_accuracy: 0.8694 - val_loss: 0.3938 - val_binary_accuracy: 0.8994\n",
      "Epoch 7/100\n",
      "2100/2100 [==============================] - 2168s - loss: 0.4528 - binary_accuracy: 0.8723 - val_loss: 0.3814 - val_binary_accuracy: 0.8994\n",
      "Epoch 8/100\n",
      " 165/2100 [=>............................] - ETA: 1769s - loss: 0.4430 - binary_accuracy: 0.8726"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "[SSL: DECRYPTION_FAILED_OR_BAD_RECORD_MAC] decryption failed or bad record mac (_ssl.c:1977)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/lib/python3.5/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/local/lib/python3.5/dist-packages/keras/utils/data_utils.py\", line 371, in get_index\n    return ds[i]\n  File \"<ipython-input-2-1a756b861685>\", line 35, in __getitem__\n    bucket.download_file(Key=key,Filename=path)\n  File \"/usr/local/lib/python3.5/dist-packages/boto3/s3/inject.py\", line 164, in bucket_download_file\n    ExtraArgs=ExtraArgs, Callback=Callback, Config=Config)\n  File \"/usr/local/lib/python3.5/dist-packages/boto3/s3/inject.py\", line 126, in download_file\n    extra_args=ExtraArgs, callback=Callback)\n  File \"/usr/local/lib/python3.5/dist-packages/boto3/s3/transfer.py\", line 299, in download_file\n    future.result()\n  File \"/usr/local/lib/python3.5/dist-packages/s3transfer/futures.py\", line 73, in result\n    return self._coordinator.result()\n  File \"/usr/local/lib/python3.5/dist-packages/s3transfer/futures.py\", line 233, in result\n    raise self._exception\n  File \"/usr/local/lib/python3.5/dist-packages/s3transfer/tasks.py\", line 126, in __call__\n    return self._execute_main(kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/s3transfer/tasks.py\", line 150, in _execute_main\n    return_value = self._main(**kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/s3transfer/download.py\", line 518, in _main\n    for chunk in chunks:\n  File \"/usr/local/lib/python3.5/dist-packages/s3transfer/download.py\", line 646, in __next__\n    chunk = self._body.read(self._chunksize)\n  File \"/usr/local/lib/python3.5/dist-packages/s3transfer/utils.py\", line 491, in read\n    value = self._stream.read(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/botocore/response.py\", line 74, in read\n    chunk = self._raw_stream.read(amt)\n  File \"/usr/local/lib/python3.5/dist-packages/botocore/vendored/requests/packages/urllib3/response.py\", line 243, in read\n    data = self._fp.read(amt)\n  File \"/usr/lib/python3.5/http/client.py\", line 448, in read\n    n = self.readinto(b)\n  File \"/usr/lib/python3.5/http/client.py\", line 488, in readinto\n    n = self.fp.readinto(b)\n  File \"/usr/lib/python3.5/socket.py\", line 575, in readinto\n    return self._sock.recv_into(b)\n  File \"/usr/lib/python3.5/ssl.py\", line 929, in recv_into\n    return self.read(nbytes, buffer)\n  File \"/usr/lib/python3.5/ssl.py\", line 791, in read\n    return self._sslobj.read(len, buffer)\n  File \"/usr/lib/python3.5/ssl.py\", line 575, in read\n    v = self._sslobj.read(len, buffer)\nssl.SSLError: [SSL: DECRYPTION_FAILED_OR_BAD_RECORD_MAC] decryption failed or bad record mac (_ssl.c:1977)\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mSSLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSSLError\u001b[0m: [SSL: DECRYPTION_FAILED_OR_BAD_RECORD_MAC] decryption failed or bad record mac (_ssl.c:1977)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f1ac86569d25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainCNNRes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0005\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpooling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdropout_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-544f4edf111c>\u001b[0m in \u001b[0;36mtrainCNNRes\u001b[0;34m(alpha, lr, momentum, pooling, dropout_rate, description)\u001b[0m\n\u001b[1;32m     75\u001b[0m                                            \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                                            \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mestop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                                           use_multiprocessing =True,workers=2)\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmulti_label_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2009\u001b[0m                 \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2011\u001b[0;31m                     \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2013\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: [SSL: DECRYPTION_FAILED_OR_BAD_RECORD_MAC] decryption failed or bad record mac (_ssl.c:1977)"
     ]
    }
   ],
   "source": [
    "hist,model = trainCNNRes(alpha=1,lr=0.0005,momentum=0.5,pooling='max',dropout_rate=0.50,description=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "#Save trained model\n",
    "model.save(\"Final_CNNRes2.h5\")\n",
    "del hist, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hist,model2 = trainCNNResAdam(alpha=1,lr=0.0005,beta1=0.9,beta2=0.99,pooling='max',dropout_rate=0.50,description=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Save trained model\n",
    "mode2.save(\"Final_CNNRes3.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Input,Flatten,Dense,Concatenate,Dropout\n",
    "\n",
    "#Number of angles per scan \n",
    "ANGLES = 32\n",
    "\n",
    "#Hidden dimensions\n",
    "LSTM_OUTPUT_DIM = 1000\n",
    "\n",
    "#Build new sequential model,Removing dense layers from model\n",
    "input_scan = Input(shape=(ANGLES,FINAL_WIDTH,FINAL_HEIGHT,CHANNELS))\n",
    "sequenced_model = TimeDistributed(model.get_layer(name='global_max_pooling2d_3'))(input_scan)\n",
    "\n",
    "#One lstm layer for now\n",
    "lstm = LSTM(LSTM_OUTPUT_DIM)(sequenced_model)\n",
    "\n",
    "#Finally, 17 dense layers connected to the output\n",
    "output_nodes = []\n",
    "for i in range(ZONES):\n",
    "    output_nodes.append(Dense(1,activation='sigmoid')(lstm))\n",
    "\n",
    "out = keras.layers.concatenate(output_nodes)\n",
    "\n",
    "#complete model\n",
    "multi_label_model = Model(input_scan, out)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
