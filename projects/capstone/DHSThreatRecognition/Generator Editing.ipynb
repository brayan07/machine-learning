{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import HelperFuncs as hfuncs\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.data_utils import Sequence\n",
    "import h5py\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to first try training a CNN on the individual images.\n",
    "We will be using binary cross entropy across the 17 regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "FINAL_WIDTH = 400\n",
    "FINAL_HEIGHT = 600\n",
    "CHANNELS = 1\n",
    "ZONES = 17\n",
    "\n",
    "class Sequencer(Sequence):\n",
    "    idx_dict={}\n",
    "    \n",
    "    def __init__(self,num_batches,bucket_name,mode=\"train\"):\n",
    "        self.num_batches = num_batches\n",
    "        self.bucket_name = bucket_name\n",
    "        self.mode = mode\n",
    "        self.key_id, self.secret_key = hfuncs.GetAWSCredentials()\n",
    "        self.mode = mode\n",
    "        \n",
    "        order = np.arange(self.num_batches)\n",
    "        np.random.seed(0)\n",
    "        np.random.shuffle(order)\n",
    "        k = 0\n",
    "        for k in range(len(order)):\n",
    "            self.idx_dict[k]=order[k]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "    def __getitem__(self,idx):\n",
    "        #Get Client\n",
    "        client = hfuncs.GetAWSClient(self.key_id,self.secret_key)\n",
    "        bucket = client.Bucket(self.bucket_name)\n",
    "        \n",
    "        idx = self.idx_dict[idx] #Mix up order of the batches \n",
    "        \n",
    "        path = \"temp/{}/batch_{}.hdf5\".format(self.mode,idx)\n",
    "        key = \"{}/batch_{}.hdf5\".format(self.mode,idx)\n",
    "        bucket.download_file(Key=key,Filename=path)\n",
    "        \n",
    "        f = h5py.File(path,\"r\")\n",
    "        try:\n",
    "            return f['/image'].value, f['/labels'].value\n",
    "        finally:\n",
    "            f.close()\n",
    "            os.remove(path)\n",
    "    def on_epoch_end(self):\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from twilio.rest import Client\n",
    "import configparser\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "class SMSNotifier(Callback):\n",
    "    def on_epoch_end(self,epoch,logs=None):\n",
    "        #Execute every other epoch\n",
    "        if epoch % 2 == 0:\n",
    "            #Get config credentials\n",
    "            config = configparser.ConfigParser()\n",
    "            config.read('twilio.conf')\n",
    "            account_sid = config['DEFAULT']['AccountID']\n",
    "            auth_token = config['DEFAULT']['AuthToken']\n",
    "            #Get client\n",
    "            client = Client(account_sid, auth_token)\n",
    "            #Create message\n",
    "            if logs is not None:\n",
    "                message = \"Epoch {} complete. Loss: {} Val_Loss: {} \".format(epoch,\n",
    "                                                                             logs.get('loss'),\n",
    "                                                                             logs.get('val_loss'))\n",
    "            else:\n",
    "                message = \"Epoch {} complete. No loss data available.\".format(epoch)\n",
    "            #Sendmessage\n",
    "            message = client.messages.create(\n",
    "                to=\"+16178884129\", \n",
    "                from_=\"+18572142288\",\n",
    "                body=message)\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom loss to take care of class imbalance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "def weighted_binary_crossentropy(y_true, y_pred):\n",
    "    x = 0.10 #Approximate percentage of positives in each of the 17 zones\n",
    "    POS_ADJ = 0.5/x\n",
    "    NEG_ADJ = 0.5/(1-x)\n",
    "    n_values = BATCH_SIZE\n",
    "    elems = (tf.unstack(y_true,num=n_values,axis=0)) \n",
    "    adj = tf.map_fn(lambda x:tuple([tf.cond(tf.equal(x[i],1.),lambda:POS_ADJ,lambda: NEG_ADJ) for i in range(n_values)]),\n",
    "                    elems, \n",
    "                    dtype = tuple([tf.float32 for i in range(n_values)]) )\n",
    "    adj = tf.stack(adj,axis=0)\n",
    "    return K.mean(tf.multiply(adj,K.binary_crossentropy(y_true,y_pred)),axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how an existing architecture would do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from inception_resnet_v1 import InceptionResNetV1\n",
    "from keras.layers import Input,Flatten,Dense,Concatenate,Dropout\n",
    "from keras.models import Model\n",
    "import keras\n",
    "from datetime import datetime\n",
    "from keras.callbacks import TensorBoard,EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\n",
    "from keras.optimizers import SGD\n",
    "from keras import metrics\n",
    "import os\n",
    "\n",
    "def trainCNNRes(alpha,lr,momentum,pooling,dropout_rate,description=\"\"):\n",
    "    \n",
    "    #ResNetv1\n",
    "    input_img = Input(shape=(FINAL_WIDTH,FINAL_HEIGHT,CHANNELS))\n",
    "    incep = InceptionResNetV1(include_top=False,\n",
    "                              weights=None,\n",
    "                              input_tensor=input_img,\n",
    "                             pooling=pooling,\n",
    "                             alpha=alpha)\n",
    "    #Apply dropout specified with specified rate\n",
    "    last = incep.output\n",
    "    last = Dropout(dropout_rate,seed=0)(last)\n",
    "    \n",
    "    #List of independent guesses for each zone\n",
    "    output_nodes = []\n",
    "    for i in range(ZONES):\n",
    "        output_nodes.append(Dense(1,activation='sigmoid')(last))\n",
    "\n",
    "    out = keras.layers.concatenate(output_nodes)\n",
    "    \n",
    "    #FInish model\n",
    "    multi_label_model = Model(input_img, out)\n",
    "\n",
    "    \n",
    "    #optimizer\n",
    "    nes=True\n",
    "    sgd = SGD(lr,momentum=momentum,nesterov=nes)\n",
    "    del incep\n",
    "    multi_label_model.compile(optimizer=sgd,\n",
    "                              metrics=[metrics.binary_accuracy,metrics.binary_crossentropy],\n",
    "                             loss= weighted_binary_crossentropy)\n",
    "    ##Set up call-backs\n",
    "    \n",
    "    #Tensorboard\n",
    "    x = datetime.today()\n",
    "    stamp = \"{}-{}-{}_{}:{}:{}_lr-{}_mom-{}_alpha-{}_pooling-{}_dropout-{}\".format(x.year,x.month,\n",
    "                                                         x.day,x.hour,x.minute,\n",
    "                                                         x.second,lr,momentum,alpha,pooling,dropout_rate)\n",
    "    tensorboard = TensorBoard(log_dir=\"logs/{}\".format(stamp),histogram_freq=0,batch_size=BATCH_SIZE,\n",
    "                              write_grads=False,write_images=True)\n",
    "    \n",
    "    #Early stopping callback\n",
    "    estop = EarlyStopping(monitor='val_loss',min_delta=0.001,patience=4)\n",
    "    \n",
    "    #Model checkpoint\n",
    "    check_point_dir = 'check_points/'\n",
    "    if not os.path.isdir(check_point_dir):\n",
    "        os.makedirs(check_point_dir)    \n",
    "    chkpt = ModelCheckpoint(os.path.join(check_point_dir,\"{}_{}_{}_{}_\".format(x.year,x.month,x.day,x.hour) + \"singleSGD_{epoch:02d}-{val_loss:.2f}.hdf5\"),\n",
    "                           monitor='val_loss',\n",
    "                           verbose=1,\n",
    "                           save_best_only=True)\n",
    "    #Reduce learning rate on plateau\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                 factor=0.2,\n",
    "                                 patience=1,\n",
    "                                 min_lr=0.00005,\n",
    "                                 cooldown = 5)\n",
    "    #Notifications\n",
    "    notify = SMSNotifier()\n",
    "    \n",
    "    ##Generators and fit\n",
    "    \n",
    "    #Bucket with clean data\n",
    "    UPLOAD_BUCKET = 'cleandhsdata' #bucket where clean data was stored\n",
    "    key_id, secret_key = hfuncs.GetAWSCredentials()\n",
    "    client = hfuncs.GetAWSClient(key_id,secret_key)\n",
    "    bucket = client.Bucket(UPLOAD_BUCKET)\n",
    "\n",
    "    #Initialize train sequencer\n",
    "    mode =\"train\"\n",
    "    num_batches = sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1 #train,test,val root directories have their own keys\n",
    "    train_seq = Sequencer(num_batches,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "    #Initialize validation sequencer\n",
    "    mode = \"val\"\n",
    "    num_batches = sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1 #train,test,val root directories have their own keys\n",
    "    val_seq = Sequencer(num_batches,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "    \n",
    "    \n",
    "    hist = multi_label_model.fit_generator(train_seq,\n",
    "                                           steps_per_epoch=3,\n",
    "                                           epochs=100,\n",
    "                                           validation_data = val_seq,\n",
    "                                           validation_steps = 2,\n",
    "                                           callbacks=[tensorboard,estop,chkpt,reduce_lr,notify],\n",
    "                                          use_multiprocessing =False,workers=1)\n",
    "    return hist,multi_label_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypermarameter Oprimization (Later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/3 [===================>..........] - ETA: 9s - loss: 4.9705 - binary_accuracy: 0.4926 - binary_crossentropy: 5.0999 Epoch 00000: val_loss improved from inf to 5.99257, saving model to check_points/2017_9_25_3_singleSGD_00-5.99.hdf5\n",
      "3/3 [==============================] - 49s - loss: 5.5716 - binary_accuracy: 0.4882 - binary_crossentropy: 5.9487 - val_loss: 5.9926 - val_binary_accuracy: 0.5912 - val_binary_crossentropy: 6.4710\n",
      "Epoch 2/100\n",
      "2/3 [===================>..........] - ETA: 8s - loss: 8.2399 - binary_accuracy: 0.5544 - binary_crossentropy: 6.9199 Epoch 00001: val_loss did not improve\n",
      "3/3 [==============================] - 32s - loss: 7.6999 - binary_accuracy: 0.5539 - binary_crossentropy: 6.8245 - val_loss: 6.2582 - val_binary_accuracy: 0.5912 - val_binary_crossentropy: 6.5241\n",
      "Epoch 3/100\n",
      "2/3 [===================>..........] - ETA: 8s - loss: 7.9969 - binary_accuracy: 0.5559 - binary_crossentropy: 6.8117 Epoch 00002: val_loss did not improve\n",
      "3/3 [==============================] - 32s - loss: 7.4111 - binary_accuracy: 0.5667 - binary_crossentropy: 6.5802 - val_loss: 6.7792 - val_binary_accuracy: 0.5324 - val_binary_crossentropy: 7.4619\n",
      "Epoch 4/100\n",
      "2/3 [===================>..........] - ETA: 8s - loss: 8.0279 - binary_accuracy: 0.5103 - binary_crossentropy: 7.6839 Epoch 00003: val_loss did not improve\n",
      "3/3 [==============================] - 32s - loss: 7.2830 - binary_accuracy: 0.5294 - binary_crossentropy: 7.3502 - val_loss: 6.7792 - val_binary_accuracy: 0.5324 - val_binary_crossentropy: 7.4619\n",
      "Epoch 5/100\n",
      "2/3 [===================>..........] - ETA: 8s - loss: 7.3469 - binary_accuracy: 0.5250 - binary_crossentropy: 7.2267 Epoch 00004: val_loss did not improve\n",
      "3/3 [==============================] - 32s - loss: 6.8858 - binary_accuracy: 0.5176 - binary_crossentropy: 7.1907 - val_loss: 6.7792 - val_binary_accuracy: 0.5324 - val_binary_crossentropy: 7.4619\n",
      "Epoch 6/100\n",
      "2/3 [===================>..........] - ETA: 8s - loss: 7.7459 - binary_accuracy: 0.4735 - binary_crossentropy: 7.5775 Epoch 00005: val_loss did not improve\n",
      "3/3 [==============================] - 32s - loss: 7.5720 - binary_accuracy: 0.4745 - binary_crossentropy: 7.3395 - val_loss: 6.7792 - val_binary_accuracy: 0.5324 - val_binary_crossentropy: 7.4619\n"
     ]
    }
   ],
   "source": [
    "hist,model = trainCNNRes(alpha=1,lr=0.05,momentum=0.5,pooling='max',dropout_rate=0.25,description=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did it\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'iwew' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-a27c8fbdf453>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;36m10\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0miwew\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Did it\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'iwew' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (DSHenv_3.5)",
   "language": "python",
   "name": "dhsenv_3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
