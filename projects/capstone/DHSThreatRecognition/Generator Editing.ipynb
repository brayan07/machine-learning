{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import HelperFuncs as hfuncs\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.data_utils import Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to first try training a CNN on the individual images.\n",
    "We will be using binary cross entropy across the 17 regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "FINAL_WIDTH = 400\n",
    "FINAL_HEIGHT = 600\n",
    "CHANNELS = 1\n",
    "ZONES = 17\n",
    "BUCKET_NAME = 'miscdatastorage'\n",
    "DATA_DIR = 'DHSData/'\n",
    "TEMP_DIR = 'temp'\n",
    "LABELS_DIR = r'stage1_labels.csv'\n",
    "EXTENSION = '.a3daps'\n",
    "np.random.seed(0)\n",
    "\n",
    "#Define a generator function\n",
    "class myGenerator:\n",
    "    #AWS and Directory information \n",
    "    bucketName = BUCKET_NAME\n",
    "    dataDir = DATA_DIR\n",
    "    temp_dir = TEMP_DIR\n",
    "    labels_dir = LABELS_DIR\n",
    "    #Connect to AWS\n",
    "    key_id, secret_key = hfuncs.GetAWSCredentials()\n",
    "    client = hfuncs.GetAWSClient(key_id,secret_key)\n",
    "    bucket = client.Bucket(bucketName)\n",
    "    extension = EXTENSION\n",
    "    #labels and keys\n",
    "    labels_dict = hfuncs.GetLabelsDict(labels_dir)\n",
    "    key_ary = None\n",
    "    #Batch information\n",
    "    n_samples = 0\n",
    "    batch_size = 0\n",
    "    #Requester\n",
    "    batch_requester = None\n",
    "    #Initialize required parameters\n",
    "    def __init__(self,keys,n_samples,batch_size=BATCH_SIZE):\n",
    "        self.key_ary = keys\n",
    "        self.n_samples = n_samples\n",
    "        self.batch_size = batch_size\n",
    "        #Initialize AWS Batch Requester\n",
    "        self.batchrequester = hfuncs.BatchRequester(self.bucket,self.key_ary,self.labels_dict,self.dataDir,self.temp_dir,self.extension)\n",
    "    def GenerateSamples(self):\n",
    "        '''Returns generator that retireves n_sample scans at a time,\n",
    "        mixes each scan-slice image into a meta-batch, and returns mini-batches of \n",
    "        BATCH_SIZE'''\n",
    "        #While there is data left, yield batch\n",
    "        while self.batchrequester.DoItemsRemain():\n",
    "            #Request data\n",
    "            print(\"Retrieving data..\")\n",
    "            X,y = self.batchrequester.NextBatch(self.n_samples)\n",
    "            n_angles = X.shape[3] #num angles (64)\n",
    "\n",
    "            #Create efficient mapping for mixing and indexing batch data\n",
    "            indexing_dict = {}\n",
    "            order = np.arange(X.shape[0]*n_angles)\n",
    "            np.random.shuffle(order)\n",
    "            k = 0\n",
    "            for s in range(X.shape[0]):\n",
    "                for a in range(n_angles):\n",
    "                    indexing_dict[order[k]]=[s,a]\n",
    "                    k+=1\n",
    "\n",
    "            print(\"Data retrieved and indexing computed.\")\n",
    "\n",
    "            #Initialize output arrays\n",
    "            print(\"Initializing arrays...\")\n",
    "            X_train = np.zeros((X.shape[0]*n_angles,FINAL_WIDTH,FINAL_HEIGHT,CHANNELS))\n",
    "            y_train = np.zeros((X.shape[0]*n_angles,ZONES))\n",
    "            print(\"Arrays initialized\")\n",
    "\n",
    "            #Set counter to 0, channel to 1\n",
    "            chan = 0 #No need to iterate here\n",
    "            i = 0\n",
    "            #Clean each image and store it in output\n",
    "            while i < X.shape[0] * n_angles:\n",
    "                j = i\n",
    "                while j < i+BATCH_SIZE:\n",
    "                    s,a = indexing_dict[j]                    \n",
    "                    X_train[j,:,:,chan] = hfuncs.CropCleanResize(X[s,:,:,a],FINAL_WIDTH,FINAL_HEIGHT)\n",
    "                    y_train[j,:] = y[s,:]\n",
    "                    j += 1\n",
    "\n",
    "                yield X_train[i:i+BATCH_SIZE,:,:,:],y_train[i:i+BATCH_SIZE]\n",
    "                i += BATCH_SIZE\n",
    "class Sequencer(Sequence):\n",
    "    generatorInstance = None\n",
    "    \n",
    "    def __init__(self,keys,n_samples,batch_size=BATCH_SIZE):\n",
    "        self.keys = keys\n",
    "        self.generatorInstance = myGenerator(keys,n_samples)\n",
    "        self.batch_size = batch_size\n",
    "        self.n_samples = n_samples\n",
    "        #Request data\n",
    "        print(\"Retrieving data..\")\n",
    "        self.X,self.y = self.generatorInstance.batchrequester.NextBatch(self.n_samples)\n",
    "        self.n_angles = self.X.shape[3] #num angles (64)\n",
    "\n",
    "        #Create efficient mapping for mixing and indexing batch data\n",
    "        self.indexing_dict = {}\n",
    "        order = np.arange(self.X.shape[0]*self.n_angles)\n",
    "        np.random.shuffle(order)\n",
    "        k = 0\n",
    "        for s in range(self.X.shape[0]):\n",
    "            for a in range(self.n_angles):\n",
    "                self.indexing_dict[order[k]]=[s,a]\n",
    "                k+=1\n",
    "\n",
    "        print(\"Data retrieved and indexing computed.\")\n",
    "\n",
    "        #Initialize output arrays\n",
    "        print(\"Initializing arrays...\")\n",
    "        self.X_train = np.zeros((self.X.shape[0]*self.n_angles,FINAL_WIDTH,FINAL_HEIGHT,CHANNELS))\n",
    "        self.y_train = np.zeros((self.X.shape[0]*self.n_angles,ZONES))\n",
    "        print(\"Arrays initialized\")\n",
    "        \n",
    "        #Channel to 1\n",
    "        self.chan = 0 #No need to iterate here\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.X.shape[0]*self.n_angles)//self.batch_size\n",
    "    def __getitem__(self,idx):\n",
    "        j = idx*self.batch_size\n",
    "        while j < (idx+1)*self.batch_size:\n",
    "            s,a = self.indexing_dict[j]\n",
    "            self.X_train[j,:,:,self.chan] = hfuncs.CropCleanResize(self.X[s,:,:,a],FINAL_WIDTH,FINAL_HEIGHT)\n",
    "            self.y_train[j,:] = self.y[s,:]\n",
    "            j += 1\n",
    "        return self.X_train[idx*self.batch_size:(idx+1)*self.batch_size,:,:,:],self.y_train[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "        \n",
    "\n",
    "def CleanKeyAry(key_ary,labels_dict,dataDir,extension):\n",
    "    '''Taken from the BatchRequester class'''\n",
    "    key_ary_new=[]\n",
    "    for key in key_ary:\n",
    "        img_id = key.strip().replace(dataDir,'').replace(extension,'')\n",
    "        if img_id in labels_dict.keys():\n",
    "            key_ary_new.append(key)\n",
    "        else:\n",
    "            continue\n",
    "    return key_ary_new\n",
    "\n",
    "def getTrainTestValData(labels_dir=LABELS_DIR,extension=EXTENSION,dataDir=DATA_DIR,bucketName=BUCKET_NAME):\n",
    "    '''Retrieves all samples that have corresponding labels \n",
    "    and splits data into a train, test, val set. '''\n",
    "    #Labels        \n",
    "    labels_dict = hfuncs.GetLabelsDict(labels_dir)\n",
    "    \n",
    "    #AWS Bucket\n",
    "    key_id, secret_key = hfuncs.GetAWSCredentials()\n",
    "    client = hfuncs.GetAWSClient(key_id,secret_key)\n",
    "    bucket = client.Bucket(bucketName)\n",
    "    \n",
    "    #Get shuffled keys and separate into train,test,and validation\n",
    "    key_ary = hfuncs.GetShuffledKeys(bucket)\n",
    "    key_ary = CleanKeyAry(key_ary,labels_dict,dataDir,extension)\n",
    "    K_train,K_test = train_test_split(key_ary,test_size=0.20,random_state=0)\n",
    "    K_train,K_val = train_test_split(K_train,test_size=0.25,random_state=0) #0.80*0.25 = 0.20 validation \n",
    "    \n",
    "    return K_train,K_test,K_val     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving data..\n",
      "Data retrieved and indexing computed.\n",
      "Initializing arrays...\n",
      "Arrays initialized\n"
     ]
    }
   ],
   "source": [
    "K_train,K_test,K_val = getTrainTestValData()\n",
    "sequencer=ValidationSequencer(K_train,5,batch_size=10)\n",
    "X,Y = sequencer.__getitem__(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D , AveragePooling2D,Flatten\n",
    "from keras.models import Model\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import keras\n",
    "\n",
    "#Build Basic model\n",
    "input_img = Input(shape=(FINAL_WIDTH,FINAL_HEIGHT,CHANNELS))\n",
    "input_norm = BatchNormalization(axis=3)(input_img)\n",
    "\n",
    "pooling_1 = MaxPooling2D((2,2),padding='same')(input_img)\n",
    "\n",
    "tower_1 = Conv2D(64, (1, 1), padding='same', activation='relu')(pooling_1)\n",
    "tower_1 = Conv2D(64, (3, 3), padding='same', activation='relu')(tower_1)\n",
    "\n",
    "tower_2 = Conv2D(64, (1, 1), padding='same', activation='relu')(pooling_1)\n",
    "tower_2 = Conv2D(64, (5, 5), padding='same', activation='relu')(tower_2)\n",
    "\n",
    "tower_3 = MaxPooling2D((2, 2), strides=(1, 1), padding='same')(pooling_1)\n",
    "tower_3 = Conv2D(64, (1, 1), padding='same', activation='relu')(tower_3)\n",
    "\n",
    "output_inception = keras.layers.concatenate([tower_1, tower_2, tower_3], axis=1)\n",
    "\n",
    "pooling_2 = MaxPooling2D((3,2),padding='same')(output_inception)\n",
    "#pooling_2 = BatchNormalization(axis=3)(pooling_2)\n",
    "\n",
    "tower_1_2 = Conv2D(128, (1, 1), padding='same', activation='relu')(pooling_2)\n",
    "tower_1_2 = Conv2D(128, (3, 3), padding='same', activation='relu')(tower_1_2)\n",
    "\n",
    "tower_2_2 = Conv2D(128, (1, 1), padding='same', activation='relu')(pooling_2)\n",
    "tower_2_2 = Conv2D(128, (5, 5), padding='same', activation='relu')(tower_2_2)\n",
    "\n",
    "tower_3_2 = MaxPooling2D((2, 2), strides=(1, 1), padding='same')(pooling_2)\n",
    "tower_3_2 = Conv2D(128, (1, 1), padding='same', activation='relu')(tower_3_2)\n",
    "\n",
    "output_inception_2 = keras.layers.concatenate([tower_1_2, tower_2_2, tower_3_2], axis=1)\n",
    "\n",
    "#output_inception_2 = Dropout(0.10)(output_inception_2)\n",
    "output_inception_2 = MaxPooling2D((2, 1),strides=(2,1), padding='same')(output_inception_2)\n",
    "\n",
    "conv_3 = Conv2D(256, (1, 1), padding='same', activation='relu')(output_inception_2)\n",
    "last = Flatten()(conv_3)\n",
    "\n",
    "#List of independent guesses for each zone\n",
    "output_nodes = []\n",
    "for i in range(ZONES):\n",
    "    output_nodes.append(Dense(1,activation='sigmoid')(last))\n",
    "\n",
    "out = keras.layers.concatenate(output_nodes)\n",
    "\n",
    "multi_label_model = Model(input_img, out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Retrieving data..\n",
      "Data retrieved\n",
      "Initializing arrays...\n",
      "Arrays initialized\n",
      "Processing image 0\n",
      "(400, 600)\n",
      "Processing image 1\n",
      "(400, 600)\n",
      "Processing image 2\n",
      "(400, 600)\n",
      "Processing image 3\n",
      "(400, 600)\n",
      "Processing image 4\n",
      "(400, 600)\n",
      "Processing image 5\n",
      "(400, 600)\n",
      "Processing image 6\n",
      "(400, 600)\n",
      "Processing image 7\n",
      "(400, 600)\n",
      "Processing image 8\n",
      "(400, 600)\n",
      "Processing image 9\n",
      "(400, 600)\n",
      "Processing image 10\n",
      "(400, 600)\n",
      "Processing image 11\n",
      "(400, 600)\n",
      "Processing image 12\n",
      "(400, 600)\n",
      "Processing image 13\n",
      "(400, 600)\n",
      "Processing image 14\n",
      "(400, 600)\n",
      "Processing image 15\n",
      "(400, 600)\n",
      "Processing image 16\n",
      "(400, 600)\n",
      "Processing image 17\n",
      "(400, 600)\n",
      "Processing image 18\n",
      "(400, 600)\n",
      "Processing image 19\n",
      "(400, 600)\n",
      "Processing image 0\n",
      "(400, 600)\n",
      "Processing image 1\n",
      "(400, 600)\n",
      "Processing image 2\n",
      "(400, 600)\n",
      "Processing image 3\n",
      "(400, 600)\n",
      "Processing image 4\n",
      "(400, 600)\n",
      "Processing image 5\n",
      "(400, 600)\n",
      "Processing image 6\n",
      "(400, 600)\n",
      "Processing image 7\n",
      "(400, 600)\n",
      "Processing image 8\n",
      "(400, 600)\n",
      "Processing image 9\n",
      "(400, 600)\n",
      "Processing image 10\n",
      "(400, 600)\n",
      "Processing image 11\n",
      "(400, 600)\n",
      "Processing image 12\n",
      "(400, 600)\n",
      "Processing image 13\n",
      "(400, 600)\n",
      "Processing image 14\n",
      "(400, 600)\n",
      "Processing image 15\n",
      "(400, 600)\n",
      "Processing image 16\n",
      "(400, 600)\n",
      "Processing image 17\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.optimizers import SGD\n",
    "from keras import metrics\n",
    "from keras import optimizers\n",
    "\n",
    "#Now we create generators for training,testing,and validation\n",
    "K_train,K_test,K_val = getTrainTestValData()\n",
    "\n",
    "#Set up learning\n",
    "\n",
    "#Optimization\n",
    "lr=0.001 \n",
    "beta_1=0.9\n",
    "beta_2=0.999\n",
    "\n",
    "adam_op = optimizers.Adam(lr,beta_1,beta_2)\n",
    "multi_label_model.compile(optimizer=optimizers.Adam,\n",
    "                          metrics=[metrics.binary_accuracy],\n",
    "                         loss= 'binary_crossentropy')\n",
    "\n",
    "#Tensorboard\n",
    "x = datetime.today()\n",
    "stamp = \"{}-{}-{}_{}:{}:{}_lr-{}_b1-{}_b2-{}\".format(x.year,x.month,\n",
    "                                                     x.day,x.hour,x.minute,\n",
    "                                                     x.second,lr,beta_1,beta_2)\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(stamp),histogram_freq=0,batch_size=BATCH_SIZE,\n",
    "                          write_grads=True,write_images=True)\n",
    "\n",
    "\n",
    "\n",
    "#Generators and fit\n",
    "train_gen = Sequencer(K_train,n_samples=len(K_train))\n",
    "val_seq = Sequencer(K_val,n_samples = len(K_val))\n",
    "max_val_steps = val_seq.__len__()\n",
    "hist = multi_label_model.fit_generator(train_gen,validation_data=val_seq,validation_steps=max_val_steps,\n",
    "                                       steps_per_epoch=20,\n",
    "                                       epochs=20,callbacks=[tensorboard],\n",
    "                                      use_multiprocessing =True,workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.set_state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (DSHenv_3.5)",
   "language": "python",
   "name": "dhsenv_3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
