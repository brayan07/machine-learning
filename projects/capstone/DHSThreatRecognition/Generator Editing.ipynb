{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import HelperFuncs as hfuncs\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.data_utils import Sequence\n",
    "import h5py\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to first try training a CNN on the individual images.\n",
    "We will be using binary cross entropy across the 17 regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "FINAL_WIDTH = 400\n",
    "FINAL_HEIGHT = 600\n",
    "CHANNELS = 1\n",
    "ZONES = 17\n",
    "\n",
    "class Sequencer(Sequence):\n",
    "    idx_dict={}\n",
    "    \n",
    "    def __init__(self,num_batches,bucket_name,mode=\"train\"):\n",
    "        self.num_batches = num_batches\n",
    "        self.bucket_name = bucket_name\n",
    "        self.mode = mode\n",
    "        self.key_id, self.secret_key = hfuncs.GetAWSCredentials()\n",
    "        self.mode = mode\n",
    "        \n",
    "        order = np.arange(self.num_batches)\n",
    "        np.random.seed(0)\n",
    "        np.random.shuffle(order)\n",
    "        k = 0\n",
    "        for k in range(len(order)):\n",
    "            self.idx_dict[k]=order[k]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "    def __getitem__(self,idx):\n",
    "        #Get Client\n",
    "        client = hfuncs.GetAWSClient(self.key_id,self.secret_key)\n",
    "        bucket = client.Bucket(self.bucket_name)\n",
    "        \n",
    "        idx = self.idx_dict[idx] #Mix up order of the batches \n",
    "        \n",
    "        path = \"temp/{}/batch_{}.hdf5\".format(self.mode,idx)\n",
    "        key = \"{}/batch_{}.hdf5\".format(self.mode,idx)\n",
    "        bucket.download_file(Key=key,Filename=path)\n",
    "        \n",
    "        f = h5py.File(path,\"r\")\n",
    "        try:\n",
    "            return f['/image'].value, f['/labels'].value\n",
    "        finally:\n",
    "            f.close()\n",
    "            os.remove(path)\n",
    "    def on_epoch_end(self):\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how an existing architecture would do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inception_resnet_v1 import InceptionResNetV1\n",
    "from keras.layers import Input,Flatten,Dense,Concatenate,Dropout\n",
    "from keras.models import Model\n",
    "import keras\n",
    "from datetime import datetime\n",
    "from keras.callbacks import TensorBoard,EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\n",
    "from keras.optimizers import SGD\n",
    "from keras import metrics\n",
    "import os\n",
    "\n",
    "def trainCNNRes(alpha,lr,momentum,pooling,dropout_rate,description=\"\"):\n",
    "    \n",
    "    #ResNetv1\n",
    "    input_img = Input(shape=(FINAL_WIDTH,FINAL_HEIGHT,CHANNELS))\n",
    "    incep = InceptionResNetV1(include_top=False,\n",
    "                              weights=None,\n",
    "                              input_tensor=input_img,\n",
    "                             pooling=pooling,\n",
    "                             alpha=alpha)\n",
    "    #Apply dropout specified with specified rate\n",
    "    last = incep.output\n",
    "    last = Dropout(dropout_rate,seed=0)(last)\n",
    "    \n",
    "    #List of independent guesses for each zone\n",
    "    output_nodes = []\n",
    "    for i in range(ZONES):\n",
    "        output_nodes.append(Dense(1,activation='sigmoid')(last))\n",
    "\n",
    "    out = keras.layers.concatenate(output_nodes)\n",
    "    \n",
    "    #FInish model\n",
    "    multi_label_model = Model(input_img, out)\n",
    "\n",
    "    \n",
    "    #optimizer\n",
    "    nes=True\n",
    "    sgd = SGD(lr,momentum=momentum,nesterov=nes)\n",
    "    del incep\n",
    "    multi_label_model.compile(optimizer=sgd,\n",
    "                              metrics=[metrics.binary_accuracy],\n",
    "                             loss= 'binary_crossentropy')\n",
    "    ##Set up call-backs\n",
    "    \n",
    "    #Tensorboard\n",
    "    x = datetime.today()\n",
    "    stamp = \"{}-{}-{}_{}:{}:{}_lr-{}_mom-{}_alpha-{}_pooling-{}_dropout-{}\".format(x.year,x.month,\n",
    "                                                         x.day,x.hour,x.minute,\n",
    "                                                         x.second,lr,momentum,alpha,pooling,dropout_rate)\n",
    "    tensorboard = TensorBoard(log_dir=\"logs/{}\".format(stamp),histogram_freq=0,batch_size=BATCH_SIZE,\n",
    "                              write_grads=False,write_images=True)\n",
    "    \n",
    "    #Early stopping callback\n",
    "    estop = EarlyStopping(monitor='val_loss',min_delta=0.001,patience=4)\n",
    "    \n",
    "    #Model checkpoint\n",
    "    check_point_dir = 'check_points/'\n",
    "    if not os.path.isdir(check_point_dir):\n",
    "        os.makedirs(check_point_dir)    \n",
    "    chkpt = ModelCheckpoint(os.path.join(check_point_dir,\"singleSGD_{epoch:02d}-{val_loss:.2f}.hdf5\"),\n",
    "                           monitor='val_loss',\n",
    "                           verbose=1,\n",
    "                           save_best_only=True)\n",
    "    #Reduce learning rate on plateau\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                 factor=0.2,\n",
    "                                 patience=2,\n",
    "                                 min_lr=0.00005,\n",
    "                                 cooldown = 5)\n",
    "    \n",
    "    ##Generators and fit\n",
    "    \n",
    "    #Bucket with clean data\n",
    "    UPLOAD_BUCKET = 'cleandhsdata' #bucket where clean data was stored\n",
    "    key_id, secret_key = hfuncs.GetAWSCredentials()\n",
    "    client = hfuncs.GetAWSClient(key_id,secret_key)\n",
    "    bucket = client.Bucket(UPLOAD_BUCKET)\n",
    "\n",
    "    #Initialize train sequencer\n",
    "    mode =\"train\"\n",
    "    num_batches = sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1 #train,test,val root directories have their own keys\n",
    "    train_seq = Sequencer(num_batches,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "    #Initialize validation sequencer\n",
    "    mode = \"val\"\n",
    "    num_batches = sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1 #train,test,val root directories have their own keys\n",
    "    val_seq = Sequencer(num_batches,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "    \n",
    "    \n",
    "    hist = multi_label_model.fit_generator(train_seq,\n",
    "                                           steps_per_epoch=3,\n",
    "                                           epochs=100,\n",
    "                                           validation_data = val_seq,\n",
    "                                           validation_steps = 2,\n",
    "                                           callbacks=[tensorboard,estop,chkpt,reduce_lr],\n",
    "                                          use_multiprocessing =False,workers=1)\n",
    "    return hist,multi_label_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypermarameter Oprimization (Later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/3 [===================>..........] - ETA: 10s - loss: 1.3825 - binary_accuracy: 0.7279Epoch 00000: val_loss improved from inf to 0.42909, saving model to check_points/singleSGD_00-0.43.hdf5\n",
      "3/3 [==============================] - 83s - loss: 1.2296 - binary_accuracy: 0.7422 - val_loss: 0.4291 - val_binary_accuracy: 0.8603\n",
      "Epoch 2/100\n",
      "2/3 [===================>..........] - ETA: 8s - loss: 0.6304 - binary_accuracy: 0.8662 Epoch 00001: val_loss improved from 0.42909 to 0.34613, saving model to check_points/singleSGD_01-0.35.hdf5\n",
      "3/3 [==============================] - 33s - loss: 0.7144 - binary_accuracy: 0.8569 - val_loss: 0.3461 - val_binary_accuracy: 0.8794\n",
      "Epoch 3/100\n",
      "2/3 [===================>..........] - ETA: 8s - loss: 0.8516 - binary_accuracy: 0.8485 Epoch 00002: val_loss improved from 0.34613 to 0.33462, saving model to check_points/singleSGD_02-0.33.hdf5\n",
      "3/3 [==============================] - 34s - loss: 0.8256 - binary_accuracy: 0.8471 - val_loss: 0.3346 - val_binary_accuracy: 0.8794\n",
      "Epoch 4/100\n",
      "2/3 [===================>..........] - ETA: 8s - loss: 0.8109 - binary_accuracy: 0.8412 Epoch 00003: val_loss improved from 0.33462 to 0.32595, saving model to check_points/singleSGD_03-0.33.hdf5\n",
      "3/3 [==============================] - 34s - loss: 0.7945 - binary_accuracy: 0.8461 - val_loss: 0.3260 - val_binary_accuracy: 0.8794\n",
      "Epoch 5/100\n",
      "2/3 [===================>..........] - ETA: 8s - loss: 0.8252 - binary_accuracy: 0.8426 Epoch 00004: val_loss improved from 0.32595 to 0.32374, saving model to check_points/singleSGD_04-0.32.hdf5\n",
      "3/3 [==============================] - 33s - loss: 0.7765 - binary_accuracy: 0.8500 - val_loss: 0.3237 - val_binary_accuracy: 0.8912\n",
      "Epoch 6/100\n",
      "2/3 [===================>..........] - ETA: 8s - loss: 0.6764 - binary_accuracy: 0.8559 "
     ]
    }
   ],
   "source": [
    "hist,model = trainCNNRes(alpha=1,lr=0.002,momentum=0.5,pooling='max',dropout_rate=0.25,description=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (DSHenv_3.5)",
   "language": "python",
   "name": "dhsenv_3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
