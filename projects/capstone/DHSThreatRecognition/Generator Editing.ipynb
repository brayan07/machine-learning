{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import HelperFuncs as hfuncs\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.data_utils import Sequence\n",
    "import h5py\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to first try training a CNN on the individual images.\n",
    "We will be using binary cross entropy across the 17 regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "FINAL_WIDTH = 400\n",
    "FINAL_HEIGHT = 600\n",
    "CHANNELS = 1\n",
    "ZONES = 17\n",
    "\n",
    "class Sequencer(Sequence):\n",
    "    idx_dict={}\n",
    "    \n",
    "    def __init__(self,num_batches,bucket_name,mode=\"train\"):\n",
    "        self.num_batches = num_batches\n",
    "        self.bucket_name = bucket_name\n",
    "        self.mode = mode\n",
    "        self.key_id, self.secret_key = hfuncs.GetAWSCredentials()\n",
    "        self.mode = mode\n",
    "        \n",
    "        order = np.arange(self.num_batches)\n",
    "        np.random.seed(0)\n",
    "        np.random.shuffle(order)\n",
    "        k = 0\n",
    "        for k in range(len(order)):\n",
    "            self.idx_dict[k]=order[k]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "    def __getitem__(self,idx):\n",
    "        #Get Client\n",
    "        client = hfuncs.GetAWSClient(self.key_id,self.secret_key)\n",
    "        bucket = client.Bucket(self.bucket_name)\n",
    "        \n",
    "        idx = self.idx_dict[idx] #Mix up order of the batches \n",
    "        \n",
    "        path = \"temp/{}/batch_{}.hdf5\".format(self.mode,idx)\n",
    "        key = \"{}/batch_{}.hdf5\".format(self.mode,idx)\n",
    "        bucket.download_file(Key=key,Filename=path)\n",
    "        \n",
    "        f = h5py.File(path,\"r\")\n",
    "        try:\n",
    "            return f['/image'].value, f['/labels'].value\n",
    "        finally:\n",
    "            f.close()\n",
    "            os.remove(path)        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how an existing architecture would do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from inception_resnet_v1 import InceptionResNetV1\n",
    "from keras.layers import Input,Flatten,Dense,Concatenate\n",
    "from keras.models import Model\n",
    "import keras\n",
    "\n",
    "\n",
    "input_img = Input(shape=(FINAL_WIDTH,FINAL_HEIGHT,CHANNELS))\n",
    "incep = InceptionResNetV1(include_top=False,\n",
    "                          weights=None,\n",
    "                          input_tensor=input_img,\n",
    "                         pooling='max')\n",
    "last = incep.output\n",
    "#last = Flatten()(last)\n",
    "\n",
    "#List of independent guesses for each zone\n",
    "output_nodes = []\n",
    "for i in range(ZONES):\n",
    "    output_nodes.append(Dense(1,activation='sigmoid')(last))\n",
    "\n",
    "out = keras.layers.concatenate(output_nodes)\n",
    "\n",
    "multi_label_model = Model(input_img, out)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from keras.layers import Input,Flatten\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "\n",
    "input_img = Input(shape=(FINAL_WIDTH,FINAL_HEIGHT,CHANNELS))\n",
    "resnet = InceptionResNetV2(include_top=False,weights='imagenet',input_tensor=input_img,\n",
    "                           input_shape =(FINAL_WIDTH,FINAL_HEIGHT,CHANNELS))\n",
    "\n",
    "last = Flatten(resnet)\n",
    "#List of independent guesses for each zone\n",
    "output_nodes = []\n",
    "for i in range(ZONES):\n",
    "    output_nodes.append(Dense(1,activation='sigmoid')(last))\n",
    "\n",
    "out = keras.layers.concatenate(output_nodes)\n",
    "resnet_multi_label_model = Model(input_img, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "20/20 [==============================] - 77s - loss: 0.6646 - binary_accuracy: 0.8016 - val_loss: 0.5904 - val_binary_accuracy: 0.7991\n",
      "Epoch 2/20\n",
      "20/20 [==============================] - 66s - loss: 0.3926 - binary_accuracy: 0.8959 - val_loss: 0.6007 - val_binary_accuracy: 0.6946\n",
      "Epoch 3/20\n",
      "20/20 [==============================] - 123s - loss: 0.4140 - binary_accuracy: 0.8924 - val_loss: 0.6176 - val_binary_accuracy: 0.6391\n",
      "Epoch 4/20\n",
      "20/20 [==============================] - 65s - loss: 0.3947 - binary_accuracy: 0.8903 - val_loss: 0.6328 - val_binary_accuracy: 0.6363\n",
      "Epoch 5/20\n",
      "20/20 [==============================] - 65s - loss: 0.3784 - binary_accuracy: 0.8976 - val_loss: 0.6227 - val_binary_accuracy: 0.6632\n",
      "Epoch 6/20\n",
      "20/20 [==============================] - 67s - loss: 0.3836 - binary_accuracy: 0.8946 - val_loss: 0.5991 - val_binary_accuracy: 0.6526\n",
      "Epoch 7/20\n",
      "20/20 [==============================] - 66s - loss: 0.3717 - binary_accuracy: 0.8947 - val_loss: 0.5663 - val_binary_accuracy: 0.6859\n",
      "Epoch 8/20\n",
      "20/20 [==============================] - 67s - loss: 0.4012 - binary_accuracy: 0.8846 - val_loss: 0.5526 - val_binary_accuracy: 0.7062\n",
      "Epoch 9/20\n",
      "20/20 [==============================] - 65s - loss: 0.3694 - binary_accuracy: 0.8959 - val_loss: 0.5337 - val_binary_accuracy: 0.7320\n",
      "Epoch 10/20\n",
      "20/20 [==============================] - 65s - loss: 0.3507 - binary_accuracy: 0.9031 - val_loss: 0.5005 - val_binary_accuracy: 0.7783\n",
      "Epoch 11/20\n",
      "20/20 [==============================] - 64s - loss: 0.3623 - binary_accuracy: 0.9046 - val_loss: 0.4954 - val_binary_accuracy: 0.7847\n",
      "Epoch 12/20\n",
      "20/20 [==============================] - 68s - loss: 0.3748 - binary_accuracy: 0.8987 - val_loss: 0.4816 - val_binary_accuracy: 0.8002\n",
      "Epoch 13/20\n",
      "20/20 [==============================] - 64s - loss: 0.3549 - binary_accuracy: 0.9056 - val_loss: 0.4573 - val_binary_accuracy: 0.8187\n",
      "Epoch 14/20\n",
      "20/20 [==============================] - 64s - loss: 0.3710 - binary_accuracy: 0.8975 - val_loss: 0.4394 - val_binary_accuracy: 0.8342\n",
      "Epoch 15/20\n",
      "20/20 [==============================] - 65s - loss: 0.3648 - binary_accuracy: 0.9000 - val_loss: 0.4413 - val_binary_accuracy: 0.8310\n",
      "Epoch 16/20\n",
      "20/20 [==============================] - 65s - loss: 0.3745 - binary_accuracy: 0.8946 - val_loss: 0.4229 - val_binary_accuracy: 0.8515\n",
      "Epoch 17/20\n",
      "20/20 [==============================] - 64s - loss: 0.3413 - binary_accuracy: 0.9069 - val_loss: 0.4081 - val_binary_accuracy: 0.8689\n",
      "Epoch 18/20\n",
      "20/20 [==============================] - 64s - loss: 0.3564 - binary_accuracy: 0.9018 - val_loss: 0.4004 - val_binary_accuracy: 0.8718\n",
      "Epoch 19/20\n",
      "20/20 [==============================] - 64s - loss: 0.3586 - binary_accuracy: 0.8990 - val_loss: 0.4019 - val_binary_accuracy: 0.8651\n",
      "Epoch 20/20\n",
      "20/20 [==============================] - 65s - loss: 0.3720 - binary_accuracy: 0.8946 - val_loss: 0.3930 - val_binary_accuracy: 0.8785\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.optimizers import SGD\n",
    "from keras import metrics\n",
    "\n",
    "#Set up learning\n",
    "\n",
    "#Optimization\n",
    "lr=0.0005 \n",
    "momentum=0.5\n",
    "nes = True\n",
    "description = \"SGD_globalMaxPooling_noDropout\"\n",
    "\n",
    "sgd = SGD(lr,momentum=momentum,nesterov=nes)\n",
    "multi_label_model.compile(optimizer=sgd,\n",
    "                          metrics=[metrics.binary_accuracy],\n",
    "                         loss= 'binary_crossentropy')\n",
    "\n",
    "#Tensorboard\n",
    "x = datetime.today()\n",
    "stamp = \"{}-{}-{}_{}:{}:{}_lr-{}_b1-{}_b2-{}_{}\".format(x.year,x.month,\n",
    "                                                     x.day,x.hour,x.minute,\n",
    "                                                     x.second,lr,momentum,nes,description)\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(stamp),histogram_freq=0,batch_size=BATCH_SIZE,\n",
    "                          write_grads=True,write_images=True)\n",
    "\n",
    "\n",
    "\n",
    "#Generators and fit\n",
    "\n",
    "#Bucket with clean data\n",
    "UPLOAD_BUCKET = 'cleandhsdata' #bucket where clean data was stored\n",
    "key_id, secret_key = hfuncs.GetAWSCredentials()\n",
    "client = hfuncs.GetAWSClient(key_id,secret_key)\n",
    "bucket = client.Bucket(UPLOAD_BUCKET)\n",
    "\n",
    "#Initialize train sequencer\n",
    "mode =\"train\"\n",
    "num_batches = sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1 #train,test,val root directories have their own keys\n",
    "train_seq = Sequencer(num_batches,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "#Initialize validation sequencer\n",
    "mode = \"val\"\n",
    "num_batches = sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1 #train,test,val root directories have their own keys\n",
    "val_seq = Sequencer(num_batches,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "\n",
    "hist = multi_label_model.fit_generator(train_seq,\n",
    "                                       steps_per_epoch=20,\n",
    "                                       epochs=20,\n",
    "                                       validation_data = val_seq,\n",
    "                                       validation_steps = 140,\n",
    "                                       callbacks=[tensorboard],\n",
    "                                      use_multiprocessing =True,workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from inception_resnet_v1 import InceptionResNetV1\n",
    "from keras.layers import Input,Flatten,Dense,Concatenate,Dropout\n",
    "from keras.models import Model\n",
    "import keras\n",
    "from datetime import datetime\n",
    "from keras.callbacks import TensorBoard,EarlyStopping\n",
    "from keras.optimizers import SGD\n",
    "from keras import metrics\n",
    "\n",
    "def trainCNNRes(alpha,lr,momentum,pooling,dropout_rate,description=\"\"):\n",
    "    \n",
    "    #ResNetv1\n",
    "    input_img = Input(shape=(FINAL_WIDTH,FINAL_HEIGHT,CHANNELS))\n",
    "    incep = InceptionResNetV1(include_top=False,\n",
    "                              weights=None,\n",
    "                              input_tensor=input_img,\n",
    "                             pooling=pooling,\n",
    "                             alpha=alpha)\n",
    "    #Apply dropout specified with specified rate\n",
    "    last = incep.output\n",
    "    last = Dropout(dropout_rate,seed=0)(last)\n",
    "    \n",
    "    #List of independent guesses for each zone\n",
    "    output_nodes = []\n",
    "    for i in range(ZONES):\n",
    "        output_nodes.append(Dense(1,activation='sigmoid')(last))\n",
    "\n",
    "    out = keras.layers.concatenate(output_nodes)\n",
    "    \n",
    "    #FInish model\n",
    "    multi_label_model = Model(input_img, out)\n",
    "\n",
    "    \n",
    "    #optimizer\n",
    "    nes=True\n",
    "    sgd = SGD(lr,momentum=momentum,nesterov=nes)\n",
    "    multi_label_model.compile(optimizer=sgd,\n",
    "                              metrics=[metrics.binary_accuracy],\n",
    "                             loss= 'binary_crossentropy')\n",
    "\n",
    "    #Tensorboard\n",
    "    x = datetime.today()\n",
    "    stamp = \"{}-{}-{}_{}:{}:{}_lr-{}_mom-{}_alpha-{}_pooling-{}_dropout-{}\".format(x.year,x.month,\n",
    "                                                         x.day,x.hour,x.minute,\n",
    "                                                         x.second,lr,momentum,alpha,pooling,dropout_rate)\n",
    "    tensorboard = TensorBoard(log_dir=\"logs/{}\".format(stamp),histogram_freq=0,batch_size=BATCH_SIZE,\n",
    "                              write_grads=False,write_images=True)\n",
    "\n",
    "    #Generators and fit\n",
    "\n",
    "    #Bucket with clean data\n",
    "    UPLOAD_BUCKET = 'cleandhsdata' #bucket where clean data was stored\n",
    "    key_id, secret_key = hfuncs.GetAWSCredentials()\n",
    "    client = hfuncs.GetAWSClient(key_id,secret_key)\n",
    "    bucket = client.Bucket(UPLOAD_BUCKET)\n",
    "\n",
    "    #Initialize train sequencer\n",
    "    mode =\"train\"\n",
    "    num_batches = sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1 #train,test,val root directories have their own keys\n",
    "    train_seq = Sequencer(num_batches,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "    #Initialize validation sequencer\n",
    "    mode = \"val\"\n",
    "    num_batches = sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1 #train,test,val root directories have their own keys\n",
    "    val_seq = Sequencer(num_batches,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "    #Early stopping callback\n",
    "    estop = EarlyStopping(monitor='loss',min_delta=0.005,patience=3)\n",
    "    \n",
    "    hist = multi_label_model.fit_generator(train_seq,\n",
    "                                           steps_per_epoch=40,\n",
    "                                           epochs=10,\n",
    "                                           validation_data = val_seq,\n",
    "                                           validation_steps = 140,\n",
    "                                           callbacks=[tensorboard,estop],\n",
    "                                          use_multiprocessing =True,workers=4)\n",
    "    return hist,multi_label_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from inception_resnet_v1 import InceptionResNetV1\n",
    "from keras.layers import Input,Flatten,Dense,Concatenate,Dropout\n",
    "from keras.models import Model\n",
    "import keras\n",
    "from datetime import datetime\n",
    "from keras.callbacks import TensorBoard,EarlyStopping\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras import metrics\n",
    "\n",
    "def trainCNNResAdam(alpha,lr,beta1,beta2,pooling,dropout_rate,description=\"\"):\n",
    "    \n",
    "    #ResNetv1\n",
    "    input_img = Input(shape=(FINAL_WIDTH,FINAL_HEIGHT,CHANNELS))\n",
    "    incep = InceptionResNetV1(include_top=False,\n",
    "                              weights=None,\n",
    "                              input_tensor=input_img,\n",
    "                             pooling=pooling,\n",
    "                             alpha=alpha)\n",
    "    #Apply dropout specified with specified rate\n",
    "    last = incep.output\n",
    "    last = Dropout(dropout_rate,seed=0)(last)\n",
    "    \n",
    "    #List of independent guesses for each zone\n",
    "    output_nodes = []\n",
    "    for i in range(ZONES):\n",
    "        output_nodes.append(Dense(1,activation='sigmoid')(last))\n",
    "\n",
    "    out = keras.layers.concatenate(output_nodes)\n",
    "    \n",
    "    #FInish model\n",
    "    multi_label_model = Model(input_img, out)\n",
    "\n",
    "    \n",
    "    #optimizer\n",
    "    adam = Adam(lr,beta_1=beta1,beta_2=beta2)\n",
    "    multi_label_model.compile(optimizer=adam,\n",
    "                              metrics=[metrics.binary_accuracy],\n",
    "                             loss= 'binary_crossentropy')\n",
    "\n",
    "    #Tensorboard\n",
    "    x = datetime.today()\n",
    "    stamp = \"{}-{}-{}_{}:{}:{}_lr-{}_beta1-{}_beta2-{}_alpha-{}_pooling-{}_dropout-{}\".format(x.year,x.month,\n",
    "                                                         x.day,x.hour,x.minute,\n",
    "                                                         x.second,lr,beta1,beta2,alpha,pooling,dropout_rate)\n",
    "    tensorboard = TensorBoard(log_dir=\"logs/{}\".format(stamp),histogram_freq=0,batch_size=BATCH_SIZE,\n",
    "                              write_grads=False,write_images=True)\n",
    "\n",
    "    #Generators and fit\n",
    "\n",
    "    #Bucket with clean data\n",
    "    UPLOAD_BUCKET = 'cleandhsdata' #bucket where clean data was stored\n",
    "    key_id, secret_key = hfuncs.GetAWSCredentials()\n",
    "    client = hfuncs.GetAWSClient(key_id,secret_key)\n",
    "    bucket = client.Bucket(UPLOAD_BUCKET)\n",
    "\n",
    "    #Initialize train sequencer\n",
    "    mode =\"train\"\n",
    "    num_batches = sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1 #train,test,val root directories have their own keys\n",
    "    train_seq = Sequencer(num_batches,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "    #Initialize validation sequencer\n",
    "    mode = \"val\"\n",
    "    num_batches = sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1 #train,test,val root directories have their own keys\n",
    "    val_seq = Sequencer(num_batches,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "    #Early stopping callback\n",
    "    estop = EarlyStopping(monitor='loss',min_delta=0.005,patience=3)\n",
    "    \n",
    "    hist = multi_label_model.fit_generator(train_seq,\n",
    "                                           steps_per_epoch=100,\n",
    "                                           epochs=21,\n",
    "                                           validation_data = val_seq,\n",
    "                                           validation_steps = 140,\n",
    "                                           callbacks=[tensorboard,estop],\n",
    "                                          use_multiprocessing =True,workers=4)\n",
    "    return hist,multi_label_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypermarameter Oprimization (Later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from scipy import stats\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "import pickle\n",
    "\n",
    "lower=-1\n",
    "upper=-4\n",
    "mu =-2.5\n",
    "sigma = 0.5\n",
    "\n",
    "log_lr = stats.truncnorm((lower-mu)/sigma,(upper-mu)/sigma,loc=mu,scale=sigma)\n",
    "\n",
    "lower=0\n",
    "upper=1\n",
    "mu =0.5\n",
    "sigma =0.10\n",
    "\n",
    "momentum= stats.truncnorm((lower-mu)/sigma,(upper-mu)/sigma,loc=mu,scale=sigma)\n",
    "\n",
    "alpha=[2]\n",
    "\n",
    "pooling = ['max','average']\n",
    "\n",
    "lower=0\n",
    "upper=0.5\n",
    "mu =0\n",
    "sigma =0.20\n",
    "\n",
    "dropout = stats.truncnorm((lower-mu)/sigma,(upper-mu)/sigma,loc=mu,scale=sigma)\n",
    "\n",
    "param_grid = {'log_lr':log_lr,'momentum':momentum,\n",
    "             'alpha':alpha,'pooling':pooling,\n",
    "             'dropout':dropout}\n",
    "param_list = list(ParameterSampler(param_grid,n_iter=25,random_state=0))\n",
    "\n",
    "rounded_list=[]\n",
    "for d in param_list:\n",
    "    dropout = np.around(d['dropout'],decimals=2)\n",
    "    momentum = np.around(d['momentum'],decimals=2)\n",
    "    lr = np.around(10**d['log_lr'],decimals=4)\n",
    "    pooling = d['pooling']\n",
    "    alpha = d['alpha']\n",
    "    rounded_list.append({'lr':lr,\n",
    "                         'momentum':momentum,\n",
    "                         'alpha':alpha,\n",
    "                         'pooling':pooling,\n",
    "                         'dropout':dropout})\n",
    "with open(\"param_list.pickle\",\"wb\") as f:\n",
    "    pickle.dump(rounded_list,f)\n",
    "\n",
    "#25 Models\n",
    "hist_list = []\n",
    "for d in rounded_list:\n",
    "    try:\n",
    "        hist_list.append(trainCNNRes(alpha=d['alpha'],\n",
    "                                     lr=d['lr'],\n",
    "                                     momentum=d['momentum'],\n",
    "                                     pooling=d['pooling'],\n",
    "                                     dropout_rate=d['dropout'],\n",
    "                                     description=\"\"))\n",
    "    except:\n",
    "        print(\"Something went wrong\")\n",
    "        hist_list.append(\"Failure\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "40/40 [==============================] - 91s - loss: 1.2085 - binary_accuracy: 0.7030 - val_loss: 0.5163 - val_binary_accuracy: 0.8268\n",
      "Epoch 2/10\n",
      "40/40 [==============================] - 84s - loss: 0.7333 - binary_accuracy: 0.7988 - val_loss: 0.4895 - val_binary_accuracy: 0.8546\n",
      "Epoch 3/10\n",
      "40/40 [==============================] - 84s - loss: 0.6391 - binary_accuracy: 0.8389 - val_loss: 0.4499 - val_binary_accuracy: 0.8863\n",
      "Epoch 4/10\n",
      "40/40 [==============================] - 86s - loss: 0.6201 - binary_accuracy: 0.8460 - val_loss: 0.4067 - val_binary_accuracy: 0.8947\n",
      "Epoch 5/10\n",
      "40/40 [==============================] - 82s - loss: 0.5995 - binary_accuracy: 0.8522 - val_loss: 0.3717 - val_binary_accuracy: 0.8967\n",
      "Epoch 6/10\n",
      "40/40 [==============================] - 82s - loss: 0.6309 - binary_accuracy: 0.8483 - val_loss: 0.3550 - val_binary_accuracy: 0.8978\n",
      "Epoch 7/10\n",
      "40/40 [==============================] - 82s - loss: 0.5865 - binary_accuracy: 0.8576 - val_loss: 0.3593 - val_binary_accuracy: 0.8981\n",
      "Epoch 8/10\n",
      "40/40 [==============================] - 82s - loss: 0.5833 - binary_accuracy: 0.8558 - val_loss: 0.3787 - val_binary_accuracy: 0.8983\n",
      "Epoch 9/10\n",
      "40/40 [==============================] - 81s - loss: 0.5866 - binary_accuracy: 0.8543 - val_loss: 0.4005 - val_binary_accuracy: 0.8983\n",
      "Epoch 10/10\n",
      "40/40 [==============================] - 83s - loss: 0.5985 - binary_accuracy: 0.8506 - val_loss: 0.4211 - val_binary_accuracy: 0.8984\n"
     ]
    }
   ],
   "source": [
    "hist,model = trainCNNRes(alpha=1,lr=0.0001,momentum=0.5,pooling='max',dropout_rate=0.20,description=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/21\n",
      "100/100 [==============================] - 148s - loss: 0.6179 - binary_accuracy: 0.8548 - val_loss: 0.4634 - val_binary_accuracy: 0.8359\n",
      "Epoch 2/21\n",
      "100/100 [==============================] - 164s - loss: 0.4973 - binary_accuracy: 0.8647 - val_loss: 0.3930 - val_binary_accuracy: 0.8790\n",
      "Epoch 3/21\n",
      "100/100 [==============================] - 142s - loss: 0.4563 - binary_accuracy: 0.8708 - val_loss: 0.3865 - val_binary_accuracy: 0.8984\n",
      "Epoch 4/21\n",
      "100/100 [==============================] - 142s - loss: 0.4104 - binary_accuracy: 0.8840 - val_loss: 0.3959 - val_binary_accuracy: 0.8984\n",
      "Epoch 5/21\n",
      "100/100 [==============================] - 142s - loss: 0.3990 - binary_accuracy: 0.8856 - val_loss: 0.3936 - val_binary_accuracy: 0.8982\n",
      "Epoch 6/21\n",
      "100/100 [==============================] - 142s - loss: 0.3819 - binary_accuracy: 0.8880 - val_loss: 0.3800 - val_binary_accuracy: 0.8984\n",
      "Epoch 7/21\n",
      "100/100 [==============================] - 141s - loss: 0.3612 - binary_accuracy: 0.8951 - val_loss: 0.3608 - val_binary_accuracy: 0.8984\n",
      "Epoch 8/21\n",
      "100/100 [==============================] - 141s - loss: 0.3501 - binary_accuracy: 0.8994 - val_loss: 0.3561 - val_binary_accuracy: 0.8984\n",
      "Epoch 9/21\n",
      "100/100 [==============================] - 141s - loss: 0.3381 - binary_accuracy: 0.9030 - val_loss: 0.3425 - val_binary_accuracy: 0.8984\n",
      "Epoch 10/21\n",
      "100/100 [==============================] - 141s - loss: 0.3322 - binary_accuracy: 0.9047 - val_loss: 0.3454 - val_binary_accuracy: 0.8984\n",
      "Epoch 11/21\n",
      "100/100 [==============================] - 141s - loss: 0.3304 - binary_accuracy: 0.9030 - val_loss: 0.3422 - val_binary_accuracy: 0.8984\n",
      "Epoch 12/21\n",
      "100/100 [==============================] - 141s - loss: 0.3316 - binary_accuracy: 0.9024 - val_loss: 0.3471 - val_binary_accuracy: 0.8984\n",
      "Epoch 13/21\n",
      "100/100 [==============================] - 142s - loss: 0.3208 - binary_accuracy: 0.9050 - val_loss: 0.3372 - val_binary_accuracy: 0.8984\n",
      "Epoch 14/21\n",
      "100/100 [==============================] - 141s - loss: 0.3230 - binary_accuracy: 0.9024 - val_loss: 0.3384 - val_binary_accuracy: 0.8984\n",
      "Epoch 15/21\n",
      "100/100 [==============================] - 141s - loss: 0.3140 - binary_accuracy: 0.9062 - val_loss: 0.3458 - val_binary_accuracy: 0.8984\n",
      "Epoch 16/21\n",
      "100/100 [==============================] - 141s - loss: 0.3146 - binary_accuracy: 0.9017 - val_loss: 0.3319 - val_binary_accuracy: 0.8984\n",
      "Epoch 17/21\n",
      "100/100 [==============================] - 141s - loss: 0.3079 - binary_accuracy: 0.9058 - val_loss: 0.3371 - val_binary_accuracy: 0.8984\n",
      "Epoch 18/21\n",
      "100/100 [==============================] - 141s - loss: 0.3052 - binary_accuracy: 0.9066 - val_loss: 0.3321 - val_binary_accuracy: 0.8982\n",
      "Epoch 19/21\n",
      "100/100 [==============================] - 141s - loss: 0.2984 - binary_accuracy: 0.9059 - val_loss: 0.3354 - val_binary_accuracy: 0.8984\n",
      "Epoch 20/21\n",
      "100/100 [==============================] - 142s - loss: 0.2982 - binary_accuracy: 0.9049 - val_loss: 0.3324 - val_binary_accuracy: 0.8982\n",
      "Epoch 21/21\n",
      "100/100 [==============================] - 141s - loss: 0.2945 - binary_accuracy: 0.9069 - val_loss: 0.3314 - val_binary_accuracy: 0.8984\n"
     ]
    }
   ],
   "source": [
    "del hist,model\n",
    "hist,model = trainCNNResAdam(alpha=1,lr=0.0001,beta1=0.9,beta2=0.99,pooling='max',dropout_rate=0.20,description=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "#Save trained model\n",
    "model.save(\"Final_CNNRes.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_format': 'channels_last', 'name': 'global_max_pooling2d_3', 'trainable': True}\n",
      "{'data_format': 'channels_last', 'name': 'global_max_pooling2d_3', 'trainable': True}\n"
     ]
    }
   ],
   "source": [
    "print(model.layers[-20].get_config())\n",
    "print(model.get_layer(name='global_max_pooling2d_3').get_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Input,Flatten,Dense,Concatenate,Dropout\n",
    "\n",
    "#Number of angles per scan \n",
    "ANGLES = 32\n",
    "\n",
    "#Hidden dimensions\n",
    "LSTM_OUTPUT_DIM = 1000\n",
    "\n",
    "#Build new sequential model,Removing dense layers from model\n",
    "input_scan = Input(shape=(ANGLES,FINAL_WIDTH,FINAL_HEIGHT,CHANNELS))\n",
    "sequenced_model = TimeDistributed(model.get_layer(name='global_max_pooling2d_3'))(input_scan)\n",
    "\n",
    "#One lstm layer for now\n",
    "lstm = LSTM(LSTM_OUTPUT_DIM)(sequenced_model)\n",
    "\n",
    "#Finally, 17 dense layers connected to the output\n",
    "output_nodes = []\n",
    "for i in range(ZONES):\n",
    "    output_nodes.append(Dense(1,activation='sigmoid')(lstm))\n",
    "\n",
    "out = keras.layers.concatenate(output_nodes)\n",
    "\n",
    "#complete model\n",
    "multi_label_model = Model(input_scan, out)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
