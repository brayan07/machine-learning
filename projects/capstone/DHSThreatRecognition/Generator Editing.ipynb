{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import HelperFuncs as hfuncs\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.data_utils import Sequence\n",
    "import h5py\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to first try training a CNN on the individual images.\n",
    "We will be using binary cross entropy across the 17 regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "FINAL_WIDTH = 400\n",
    "FINAL_HEIGHT = 600\n",
    "CHANNELS = 1\n",
    "ZONES = 17\n",
    "\n",
    "class Sequencer(Sequence):\n",
    "    idx_dict={}\n",
    "    \n",
    "    def __init__(self,num_batches,bucket_name,mode=\"train\"):\n",
    "        self.num_batches = num_batches\n",
    "        self.bucket_name = bucket_name\n",
    "        self.mode = mode\n",
    "        self.key_id, self.secret_key = hfuncs.GetAWSCredentials()\n",
    "        self.mode = mode\n",
    "        \n",
    "        order = np.arange(self.num_batches)\n",
    "        np.random.seed(0)\n",
    "        np.random.shuffle(order)\n",
    "        k = 0\n",
    "        for k in range(len(order)):\n",
    "            self.idx_dict[k]=order[k]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "    def __getitem__(self,idx):\n",
    "        #Get Client\n",
    "        client = hfuncs.GetAWSClient(self.key_id,self.secret_key)\n",
    "        bucket = client.Bucket(self.bucket_name)\n",
    "        \n",
    "        idx = self.idx_dict[idx] #Mix up order of the batches \n",
    "        \n",
    "        path = \"temp/{}/batch_{}.hdf5\".format(self.mode,idx)\n",
    "        key = \"{}/batch_{}.hdf5\".format(self.mode,idx)\n",
    "        bucket.download_file(Key=key,Filename=path)\n",
    "        \n",
    "        f = h5py.File(path,\"r\")\n",
    "        try:\n",
    "            return f['/image'].value, f['/labels'].value\n",
    "        finally:\n",
    "            f.close()\n",
    "            os.remove(path)\n",
    "    def on_epoch_end(self):\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from twilio.rest import Client\n",
    "import configparser\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "class SMSNotifier(Callback):\n",
    "    def on_epoch_end(self,epoch,logs=None):\n",
    "        #Execute every other epoch\n",
    "        if epoch % 2 == 0:\n",
    "            #Get config credentials\n",
    "            config = configparser.ConfigParser()\n",
    "            config.read('twilio.conf')\n",
    "            account_sid = config['DEFAULT']['AccountID']\n",
    "            auth_token = config['DEFAULT']['AuthToken']\n",
    "            #Get client\n",
    "            client = Client(account_sid, auth_token)\n",
    "            #Create message\n",
    "            if logs is not None:\n",
    "                message = \"Epoch {} complete. Loss: {} Val_Loss: {} \".format(epoch,\n",
    "                                                                             logs.get('loss'),\n",
    "                                                                             logs.get('val_loss'))\n",
    "            else:\n",
    "                message = \"Epoch {} complete. No loss data available.\".format(epoch)\n",
    "            #Sendmessage\n",
    "            message = client.messages.create(\n",
    "                to=\"+16178884129\", \n",
    "                from_=\"+18572142288\",\n",
    "                body=message)\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom loss to take care of class imbalance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "def weighted_binary_crossentropy(y_true, y_pred):\n",
    "    x = 0.10 #Approximate percentage of positives in each of the 17 zones\n",
    "    POS_ADJ = 0.5/x\n",
    "    NEG_ADJ = 0.5/(1-x)\n",
    "    n_values = BATCH_SIZE\n",
    "    elems = (tf.unstack(y_true,num=n_values,axis=0)) \n",
    "    adj = tf.map_fn(lambda x:tuple([tf.cond(tf.equal(x[i],1.),lambda:POS_ADJ,lambda: NEG_ADJ) for i in range(n_values)]),\n",
    "                    elems, \n",
    "                    dtype = tuple([tf.float32 for i in range(n_values)]) )\n",
    "    adj = tf.stack(adj,axis=0)\n",
    "    return K.mean(tf.multiply(adj,K.binary_crossentropy(y_true,y_pred)),axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how an existing architecture would do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from inception_resnet_v1 import InceptionResNetV1\n",
    "from keras.layers import Input,Flatten,Dense,Concatenate,Dropout\n",
    "from keras.models import Model\n",
    "import keras\n",
    "from datetime import datetime\n",
    "from keras.callbacks import TensorBoard,EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras import metrics\n",
    "import os\n",
    "\n",
    "def trainCNNRes(alpha,lr,beta1,beta2,pooling,dropout_rate,description=\"\"):\n",
    "    \n",
    "    #ResNetv1\n",
    "    input_img = Input(shape=(FINAL_WIDTH,FINAL_HEIGHT,CHANNELS))\n",
    "    incep = InceptionResNetV1(include_top=False,\n",
    "                              weights=None,\n",
    "                              input_tensor=input_img,\n",
    "                             pooling=pooling,\n",
    "                             alpha=alpha)\n",
    "    #Apply dropout specified with specified rate\n",
    "    last = incep.output\n",
    "    last = Dropout(dropout_rate,seed=0)(last)\n",
    "    \n",
    "    #List of independent guesses for each zone\n",
    "    output_nodes = []\n",
    "    for i in range(ZONES):\n",
    "        output_nodes.append(Dense(1,activation='sigmoid')(last))\n",
    "\n",
    "    out = keras.layers.concatenate(output_nodes)\n",
    "    \n",
    "    #FInish model\n",
    "    multi_label_model = Model(input_img, out)\n",
    "\n",
    "    \n",
    "    #optimizer\n",
    "    #nes=True\n",
    "    #sgd = SGD(lr,momentum=momentum,nesterov=nes)\n",
    "    adam = Adam(lr,beta1,beta2)\n",
    "    del incep\n",
    "    multi_label_model.compile(optimizer=adam,\n",
    "                              metrics=[metrics.binary_accuracy,metrics.binary_crossentropy],\n",
    "                             loss= weighted_binary_crossentropy)\n",
    "    ##Set up call-backs\n",
    "    \n",
    "    #Tensorboard\n",
    "    x = datetime.today()\n",
    "    stamp = \"{}-{}-{}_{}:{}:{}_lr-{}_beta1-{}_beta2-{}_alpha-{}_pooling-{}_dropout-{}\".format(x.year,x.month,\n",
    "                                                         x.day,x.hour,x.minute,\n",
    "                                                         x.second,lr,beta1,beta2,alpha,pooling,dropout_rate)\n",
    "    tensorboard = TensorBoard(log_dir=\"logs/{}\".format(stamp),histogram_freq=0,batch_size=BATCH_SIZE,\n",
    "                              write_grads=False,write_images=True)\n",
    "    \n",
    "    #Early stopping callback\n",
    "    estop = EarlyStopping(monitor='val_loss',min_delta=0.001,patience=5)\n",
    "    \n",
    "    #Model checkpoint\n",
    "    check_point_dir = 'check_points/'\n",
    "    if not os.path.isdir(check_point_dir):\n",
    "        os.makedirs(check_point_dir)    \n",
    "    chkpt = ModelCheckpoint(os.path.join(check_point_dir,\"{}_{}_{}_{}_\".format(x.year,x.month,x.day,x.hour) + \"singleSGD_{epoch:02d}-{val_loss:.2f}.hdf5\"),\n",
    "                           monitor='val_loss',\n",
    "                           verbose=1,\n",
    "                           save_best_only=True)\n",
    "    #Reduce learning rate on plateau\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                 factor=0.2,\n",
    "                                 patience=1,\n",
    "                                 min_lr=0.00005,\n",
    "                                 cooldown = 5)\n",
    "    #Notifications\n",
    "    notify = SMSNotifier()\n",
    "    \n",
    "    ##Generators and fit\n",
    "    \n",
    "    #Bucket with clean data\n",
    "    UPLOAD_BUCKET = 'cleandhsdata' #bucket where clean data was stored\n",
    "    key_id, secret_key = hfuncs.GetAWSCredentials()\n",
    "    client = hfuncs.GetAWSClient(key_id,secret_key)\n",
    "    bucket = client.Bucket(UPLOAD_BUCKET)\n",
    "\n",
    "    #Initialize train sequencer\n",
    "    mode =\"train\"\n",
    "    num_batches = sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1 #train,test,val root directories have their own keys\n",
    "    train_seq = Sequencer(num_batches,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "    #Initialize validation sequencer\n",
    "    mode = \"val\"\n",
    "    num_batches = sum([1 if \"{}/\".format(mode) in k.key else 0 for k in bucket.objects.all()])-1 #train,test,val root directories have their own keys\n",
    "    val_seq = Sequencer(num_batches,UPLOAD_BUCKET,mode=mode)\n",
    "\n",
    "    \n",
    "    \n",
    "    hist = multi_label_model.fit_generator(train_seq,\n",
    "                                           steps_per_epoch=2120,\n",
    "                                           epochs=300,\n",
    "                                           validation_data = val_seq,\n",
    "                                           validation_steps = 700,\n",
    "                                           callbacks=[tensorboard,estop,chkpt,reduce_lr,notify],\n",
    "                                          use_multiprocessing = True,workers=6)\n",
    "    return hist,multi_label_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypermarameter Oprimization (Later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "2119/2120 [============================>.] - ETA: 0s - loss: 0.6943 - binary_accuracy: 0.6054 - binary_crossentropy: 0.6877Epoch 00000: val_loss improved from inf to 0.75035, saving model to check_points/2017_9_25_4_singleSGD_00-0.75.hdf5\n",
      "2120/2120 [==============================] - 2340s - loss: 0.6943 - binary_accuracy: 0.6055 - binary_crossentropy: 0.6876 - val_loss: 0.7504 - val_binary_accuracy: 0.6796 - val_binary_crossentropy: 0.6318\n",
      "Epoch 2/300\n",
      "2119/2120 [============================>.] - ETA: 0s - loss: 0.4703 - binary_accuracy: 0.7660 - binary_crossentropy: 0.4741Epoch 00001: val_loss did not improve\n",
      "2120/2120 [==============================] - 2337s - loss: 0.4702 - binary_accuracy: 0.7661 - binary_crossentropy: 0.4740 - val_loss: 1.1326 - val_binary_accuracy: 0.7486 - val_binary_crossentropy: 0.6018\n",
      "Epoch 3/300\n",
      "2119/2120 [============================>.] - ETA: 0s - loss: 0.2308 - binary_accuracy: 0.8993 - binary_crossentropy: 0.2396Epoch 00002: val_loss did not improve\n",
      "2120/2120 [==============================] - 2298s - loss: 0.2308 - binary_accuracy: 0.8993 - binary_crossentropy: 0.2396 - val_loss: 1.6757 - val_binary_accuracy: 0.7928 - val_binary_crossentropy: 0.6572\n",
      "Epoch 4/300\n",
      "2119/2120 [============================>.] - ETA: 0s - loss: 0.0604 - binary_accuracy: 0.9759 - binary_crossentropy: 0.0654Epoch 00003: val_loss did not improve\n",
      "2120/2120 [==============================] - 2322s - loss: 0.0604 - binary_accuracy: 0.9759 - binary_crossentropy: 0.0654 - val_loss: 2.1321 - val_binary_accuracy: 0.8593 - val_binary_crossentropy: 0.5877\n",
      "Epoch 5/300\n",
      "2119/2120 [============================>.] - ETA: 0s - loss: 0.0272 - binary_accuracy: 0.9898 - binary_crossentropy: 0.0297Epoch 00004: val_loss did not improve\n",
      "2120/2120 [==============================] - 2287s - loss: 0.0272 - binary_accuracy: 0.9898 - binary_crossentropy: 0.0297 - val_loss: 2.7079 - val_binary_accuracy: 0.8632 - val_binary_crossentropy: 0.7073\n",
      "Epoch 6/300\n",
      "2119/2120 [============================>.] - ETA: 0s - loss: 0.0151 - binary_accuracy: 0.9946 - binary_crossentropy: 0.0167Epoch 00005: val_loss did not improve\n",
      "2120/2120 [==============================] - 2297s - loss: 0.0151 - binary_accuracy: 0.9946 - binary_crossentropy: 0.0167 - val_loss: 2.9837 - val_binary_accuracy: 0.8682 - val_binary_crossentropy: 0.7655\n",
      "Epoch 7/300\n",
      "2119/2120 [============================>.] - ETA: 0s - loss: 0.0102 - binary_accuracy: 0.9964 - binary_crossentropy: 0.0112Epoch 00006: val_loss did not improve\n",
      "2120/2120 [==============================] - 2335s - loss: 0.0102 - binary_accuracy: 0.9964 - binary_crossentropy: 0.0112 - val_loss: 3.1457 - val_binary_accuracy: 0.8726 - val_binary_crossentropy: 0.7918\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    hist,model = trainCNNRes(alpha=1,lr=0.001,beta1=0.9,beta2=0.999,pooling='max',dropout_rate=0.25,description=\"\")\n",
    "finally:\n",
    "    os.system(\"aws ec2 stop-instances --instance-ids i-0172c75d2de9bad71\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65280\n"
     ]
    }
   ],
   "source": [
    "print(os.system(\"aws ec2 stop-instances --instance-ids i-0172c75d2de9bad71\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
